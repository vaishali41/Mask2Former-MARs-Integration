[10/16 20:54:19] detectron2 INFO: Rank of current process: 0. World size: 1
[10/16 20:54:20] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/16 20:54:20] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/16 20:54:20] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 2                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 8   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/16 20:54:20] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 8
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/16 20:54:20] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/16 20:55:00] detectron2 INFO: Rank of current process: 0. World size: 1
[10/16 20:55:01] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/16 20:55:01] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/16 20:55:01] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 2                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 8   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/16 20:55:01] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 8
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/16 20:55:01] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/16 20:55:08] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.34 seconds.
[10/16 20:55:09] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/16 20:55:12] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/16 21:18:49] detectron2 INFO: Rank of current process: 0. World size: 1
[10/16 21:18:50] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/16 21:18:50] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/16 21:18:50] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 2                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 8   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/16 21:18:50] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 8
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/16 21:18:50] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/16 21:18:57] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.25 seconds.
[10/16 21:18:57] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/16 21:19:01] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/16 21:19:02] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/16 21:19:02] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/16 21:19:02] d2.data.build INFO: Using training sampler TrainingSampler
[10/16 21:19:02] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/16 21:19:02] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/16 21:19:02] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/16 21:19:02] d2.data.build INFO: Making batched data loader with batch_size=2
[10/16 21:19:02] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/16 21:19:02] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/16 21:19:02] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/16 21:19:02] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/16 21:19:03] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/16 21:19:03] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/16 21:19:03] d2.engine.train_loop INFO: Starting training from iteration 0
[10/16 21:21:22] detectron2 INFO: Rank of current process: 0. World size: 1
[10/16 21:21:23] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/16 21:21:23] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/16 21:21:23] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 16   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/16 21:21:23] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 16
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/16 21:21:23] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/16 21:21:30] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.40 seconds.
[10/16 21:21:31] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/16 21:21:34] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/16 21:21:36] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/16 21:21:36] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/16 21:21:36] d2.data.build INFO: Using training sampler TrainingSampler
[10/16 21:21:36] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/16 21:21:36] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/16 21:21:36] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/16 21:21:36] d2.data.build INFO: Making batched data loader with batch_size=1
[10/16 21:21:36] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/16 21:21:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/16 21:21:36] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/16 21:21:36] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/16 21:21:36] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/16 21:21:36] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/16 21:21:36] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 00:02:03] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 00:02:03] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 00:02:03] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 00:02:03] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 4                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 00:02:03] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 00:02:03] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 00:02:11] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.35 seconds.
[10/17 00:02:11] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 00:02:15] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 00:02:16] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 00:02:16] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 00:02:16] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 00:02:16] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 00:02:16] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 00:02:16] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 00:02:16] d2.data.build INFO: Making batched data loader with batch_size=4
[10/17 00:02:16] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 00:02:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 00:02:16] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 00:02:17] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 00:02:17] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 00:02:17] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 00:02:17] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 00:13:06] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 00:13:07] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 00:13:07] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 00:13:07] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 4                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 00:13:07] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 00:13:07] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 00:13:14] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.28 seconds.
[10/17 00:13:15] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 00:13:18] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 00:13:20] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 00:13:20] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 00:13:20] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 00:13:20] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 00:13:20] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 00:13:20] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 00:13:20] d2.data.build INFO: Making batched data loader with batch_size=4
[10/17 00:13:20] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 00:13:20] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 00:13:20] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 00:13:20] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 00:13:20] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 00:13:20] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 00:13:20] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 05:09:34] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 05:09:35] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 05:09:35] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 05:09:35] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 05:09:35] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 05:09:35] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 05:09:42] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.36 seconds.
[10/17 05:09:43] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 05:09:47] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 05:09:48] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 05:09:48] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 05:09:48] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 05:09:48] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 05:09:48] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 05:09:48] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 05:09:48] d2.data.build INFO: Making batched data loader with batch_size=1
[10/17 05:09:48] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 05:09:48] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 05:09:48] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 05:09:48] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 05:09:48] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 05:09:48] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 05:09:48] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 06:26:22] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 06:26:23] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 06:26:23] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 06:26:23] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 06:26:23] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 06:26:23] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 06:26:30] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.28 seconds.
[10/17 06:26:31] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 06:26:34] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 06:26:36] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 06:26:36] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 06:26:36] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 06:26:36] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 06:26:36] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 06:26:36] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 06:26:36] d2.data.build INFO: Making batched data loader with batch_size=1
[10/17 06:26:36] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 06:26:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 06:26:36] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 06:26:36] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 06:26:36] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 06:26:36] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 06:26:36] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 07:06:20] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 07:06:21] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 07:06:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 07:06:21] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 07:06:21] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 07:06:21] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 07:06:28] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.28 seconds.
[10/17 07:06:29] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 07:06:32] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 07:06:33] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 07:06:34] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 07:06:34] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 07:06:34] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 07:06:34] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 07:06:34] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 07:06:34] d2.data.build INFO: Making batched data loader with batch_size=1
[10/17 07:06:34] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:06:34] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:06:34] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 07:06:34] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 07:06:34] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 07:06:34] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 07:06:34] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 07:08:33] d2.utils.events INFO:  eta: 1 day, 1:50:02  iter: 19  total_loss: 151.7  loss_ce: 7.379  loss_mask: 2.827  loss_dice: 4.861  loss_ce_0: 9.076  loss_mask_0: 2.164  loss_dice_0: 4.228  loss_ce_1: 7.204  loss_mask_1: 1.806  loss_dice_1: 4.499  loss_ce_2: 8.153  loss_mask_2: 1.815  loss_dice_2: 4.71  loss_ce_3: 7.809  loss_mask_3: 2.756  loss_dice_3: 4.85  loss_ce_4: 8.12  loss_mask_4: 2.938  loss_dice_4: 4.646  loss_ce_5: 8.644  loss_mask_5: 2.847  loss_dice_5: 4.707  loss_ce_6: 8.478  loss_mask_6: 2.815  loss_dice_6: 4.783  loss_ce_7: 7.877  loss_mask_7: 2.659  loss_dice_7: 4.834  loss_ce_8: 8.297  loss_mask_8: 2.892  loss_dice_8: 4.753  loss_mars: 0.3116    time: 6.0621  last_time: 5.8654  data_time: 0.0091  last_data_time: 0.0021   lr: 7.6924e-06  max_mem: 0M
[10/17 07:10:29] d2.utils.events INFO:  eta: 1 day, 0:50:01  iter: 39  total_loss: 126.7  loss_ce: 5.149  loss_mask: 1.738  loss_dice: 4.753  loss_ce_0: 9.04  loss_mask_0: 1.09  loss_dice_0: 4.481  loss_ce_1: 6.21  loss_mask_1: 1.056  loss_dice_1: 4.55  loss_ce_2: 6.247  loss_mask_2: 1.526  loss_dice_2: 4.593  loss_ce_3: 5.388  loss_mask_3: 1.242  loss_dice_3: 4.674  loss_ce_4: 4.938  loss_mask_4: 2.069  loss_dice_4: 4.728  loss_ce_5: 5.555  loss_mask_5: 1.953  loss_dice_5: 4.722  loss_ce_6: 5.193  loss_mask_6: 1.764  loss_dice_6: 4.774  loss_ce_7: 4.725  loss_mask_7: 1.706  loss_dice_7: 4.73  loss_ce_8: 4.904  loss_mask_8: 1.745  loss_dice_8: 4.703  loss_mars: 0.3336    time: 5.9370  last_time: 5.3924  data_time: 0.0023  last_data_time: 0.0018   lr: 1.5684e-05  max_mem: 0M
[10/17 07:12:26] d2.utils.events INFO:  eta: 1 day, 0:55:32  iter: 59  total_loss: 113.3  loss_ce: 4.954  loss_mask: 1.181  loss_dice: 4.758  loss_ce_0: 8.791  loss_mask_0: 0.9745  loss_dice_0: 4.557  loss_ce_1: 5.178  loss_mask_1: 1.174  loss_dice_1: 4.574  loss_ce_2: 4.541  loss_mask_2: 1.21  loss_dice_2: 4.566  loss_ce_3: 4.748  loss_mask_3: 1.205  loss_dice_3: 4.652  loss_ce_4: 4.559  loss_mask_4: 1.208  loss_dice_4: 4.656  loss_ce_5: 4.867  loss_mask_5: 1.197  loss_dice_5: 4.71  loss_ce_6: 4.651  loss_mask_6: 1.033  loss_dice_6: 4.774  loss_ce_7: 4.752  loss_mask_7: 1.256  loss_dice_7: 4.745  loss_ce_8: 5.206  loss_mask_8: 1.287  loss_dice_8: 4.72  loss_mars: 0.3319    time: 5.9004  last_time: 5.9131  data_time: 0.0032  last_data_time: 0.0030   lr: 2.3676e-05  max_mem: 0M
[10/17 07:14:17] d2.utils.events INFO:  eta: 1 day, 0:16:20  iter: 79  total_loss: 104.3  loss_ce: 3.675  loss_mask: 1.798  loss_dice: 4.614  loss_ce_0: 8.955  loss_mask_0: 1.471  loss_dice_0: 4.326  loss_ce_1: 3.93  loss_mask_1: 1.544  loss_dice_1: 4.372  loss_ce_2: 3.608  loss_mask_2: 1.493  loss_dice_2: 4.374  loss_ce_3: 3.529  loss_mask_3: 1.653  loss_dice_3: 4.429  loss_ce_4: 3.483  loss_mask_4: 1.782  loss_dice_4: 4.508  loss_ce_5: 3.736  loss_mask_5: 1.833  loss_dice_5: 4.517  loss_ce_6: 3.71  loss_mask_6: 1.718  loss_dice_6: 4.551  loss_ce_7: 3.722  loss_mask_7: 1.735  loss_dice_7: 4.589  loss_ce_8: 3.709  loss_mask_8: 1.89  loss_dice_8: 4.552  loss_mars: 0.3321    time: 5.8186  last_time: 8.0012  data_time: 0.0029  last_data_time: 0.0087   lr: 3.1668e-05  max_mem: 0M
[10/17 07:16:13] d2.utils.events INFO:  eta: 1 day, 0:14:27  iter: 99  total_loss: 107.6  loss_ce: 3.805  loss_mask: 1.903  loss_dice: 4.823  loss_ce_0: 8.89  loss_mask_0: 1.36  loss_dice_0: 4.34  loss_ce_1: 4.278  loss_mask_1: 1.409  loss_dice_1: 4.507  loss_ce_2: 3.622  loss_mask_2: 1.448  loss_dice_2: 4.632  loss_ce_3: 3.629  loss_mask_3: 1.717  loss_dice_3: 4.796  loss_ce_4: 3.483  loss_mask_4: 1.678  loss_dice_4: 4.786  loss_ce_5: 3.425  loss_mask_5: 1.991  loss_dice_5: 4.772  loss_ce_6: 3.483  loss_mask_6: 1.831  loss_dice_6: 4.782  loss_ce_7: 3.603  loss_mask_7: 1.931  loss_dice_7: 4.828  loss_ce_8: 3.646  loss_mask_8: 1.955  loss_dice_8: 4.827  loss_mars: 0.2241    time: 5.8118  last_time: 5.7292  data_time: 0.0028  last_data_time: 0.0017   lr: 3.966e-05  max_mem: 0M
[10/17 07:18:11] d2.utils.events INFO:  eta: 1 day, 0:39:30  iter: 119  total_loss: 106  loss_ce: 4.15  loss_mask: 1.546  loss_dice: 4.628  loss_ce_0: 8.915  loss_mask_0: 0.8844  loss_dice_0: 4.468  loss_ce_1: 4.409  loss_mask_1: 0.9291  loss_dice_1: 4.524  loss_ce_2: 4.453  loss_mask_2: 1.202  loss_dice_2: 4.478  loss_ce_3: 4.072  loss_mask_3: 1.33  loss_dice_3: 4.566  loss_ce_4: 4.273  loss_mask_4: 1.365  loss_dice_4: 4.625  loss_ce_5: 4.237  loss_mask_5: 1.458  loss_dice_5: 4.626  loss_ce_6: 4.207  loss_mask_6: 1.401  loss_dice_6: 4.648  loss_ce_7: 4.028  loss_mask_7: 1.444  loss_dice_7: 4.652  loss_ce_8: 4.176  loss_mask_8: 1.472  loss_dice_8: 4.605  loss_mars: 0.4114    time: 5.8280  last_time: 5.9723  data_time: 0.0033  last_data_time: 0.0036   lr: 4.7652e-05  max_mem: 0M
[10/17 07:20:03] d2.utils.events INFO:  eta: 1 day, 0:35:45  iter: 139  total_loss: 106.5  loss_ce: 3.659  loss_mask: 2.724  loss_dice: 4.659  loss_ce_0: 8.949  loss_mask_0: 1.765  loss_dice_0: 3.896  loss_ce_1: 3.661  loss_mask_1: 1.684  loss_dice_1: 3.871  loss_ce_2: 3.601  loss_mask_2: 2.071  loss_dice_2: 4.458  loss_ce_3: 3.632  loss_mask_3: 2.205  loss_dice_3: 4.621  loss_ce_4: 3.581  loss_mask_4: 2.476  loss_dice_4: 4.594  loss_ce_5: 3.343  loss_mask_5: 2.426  loss_dice_5: 4.643  loss_ce_6: 3.441  loss_mask_6: 2.819  loss_dice_6: 4.648  loss_ce_7: 3.367  loss_mask_7: 2.574  loss_dice_7: 4.619  loss_ce_8: 3.38  loss_mask_8: 2.607  loss_dice_8: 4.67  loss_mars: 0.2058    time: 5.7951  last_time: 5.8300  data_time: 0.0028  last_data_time: 0.0030   lr: 5.5644e-05  max_mem: 0M
[10/17 07:21:55] d2.utils.events INFO:  eta: 1 day, 0:32:10  iter: 159  total_loss: 108.7  loss_ce: 4.02  loss_mask: 1.308  loss_dice: 4.77  loss_ce_0: 8.782  loss_mask_0: 1.061  loss_dice_0: 4.638  loss_ce_1: 4.093  loss_mask_1: 1.104  loss_dice_1: 4.635  loss_ce_2: 4.163  loss_mask_2: 1.178  loss_dice_2: 4.673  loss_ce_3: 4.207  loss_mask_3: 1.353  loss_dice_3: 4.688  loss_ce_4: 4.027  loss_mask_4: 1.304  loss_dice_4: 4.735  loss_ce_5: 4.012  loss_mask_5: 1.572  loss_dice_5: 4.709  loss_ce_6: 3.912  loss_mask_6: 1.356  loss_dice_6: 4.737  loss_ce_7: 3.954  loss_mask_7: 1.29  loss_dice_7: 4.817  loss_ce_8: 4.08  loss_mask_8: 1.346  loss_dice_8: 4.737  loss_mars: 0.1302    time: 5.7678  last_time: 6.0827  data_time: 0.0039  last_data_time: 0.0081   lr: 6.3636e-05  max_mem: 0M
[10/17 07:23:57] d2.utils.events INFO:  eta: 1 day, 0:46:41  iter: 179  total_loss: 108.8  loss_ce: 3.109  loss_mask: 2.731  loss_dice: 4.735  loss_ce_0: 8.806  loss_mask_0: 1.08  loss_dice_0: 4.463  loss_ce_1: 3.154  loss_mask_1: 1.052  loss_dice_1: 4.513  loss_ce_2: 3.078  loss_mask_2: 1.128  loss_dice_2: 4.539  loss_ce_3: 3.174  loss_mask_3: 1.714  loss_dice_3: 4.592  loss_ce_4: 2.985  loss_mask_4: 2.205  loss_dice_4: 4.825  loss_ce_5: 3.302  loss_mask_5: 2.538  loss_dice_5: 4.762  loss_ce_6: 3.188  loss_mask_6: 2.542  loss_dice_6: 4.732  loss_ce_7: 3.16  loss_mask_7: 2.596  loss_dice_7: 4.691  loss_ce_8: 3.183  loss_mask_8: 2.459  loss_dice_8: 4.708  loss_mars: 0.2943    time: 5.8077  last_time: 6.4845  data_time: 0.0026  last_data_time: 0.0083   lr: 7.1628e-05  max_mem: 0M
[10/17 07:25:23] d2.engine.hooks INFO: Overall training speed: 184 iterations in 0:18:38 (6.0806 s / it)
[10/17 07:25:23] d2.engine.hooks INFO: Total training time: 0:18:39 (0:00:00 on hooks)
[10/17 07:25:23] d2.utils.events INFO:  eta: 1 day, 1:07:14  iter: 186  total_loss: 101.8  loss_ce: 3.071  loss_mask: 1.852  loss_dice: 4.774  loss_ce_0: 8.724  loss_mask_0: 0.9058  loss_dice_0: 4.629  loss_ce_1: 3.007  loss_mask_1: 0.8663  loss_dice_1: 4.623  loss_ce_2: 2.9  loss_mask_2: 0.8238  loss_dice_2: 4.656  loss_ce_3: 3.089  loss_mask_3: 1.29  loss_dice_3: 4.722  loss_ce_4: 2.996  loss_mask_4: 1.607  loss_dice_4: 4.825  loss_ce_5: 3.252  loss_mask_5: 1.617  loss_dice_5: 4.79  loss_ce_6: 3.137  loss_mask_6: 1.718  loss_dice_6: 4.76  loss_ce_7: 3.006  loss_mask_7: 1.564  loss_dice_7: 4.771  loss_ce_8: 3.062  loss_mask_8: 1.641  loss_dice_8: 4.786  loss_mars: 0.2118    time: 6.0479  last_time: 6.2092  data_time: 0.0029  last_data_time: 0.0021   lr: 7.4026e-05  max_mem: 0M
[10/17 07:26:37] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 07:26:37] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 07:26:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 07:26:37] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 4                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 07:26:37] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 07:26:37] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 07:26:45] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.37 seconds.
[10/17 07:26:45] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 07:26:49] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 07:26:50] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 07:26:50] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 07:26:50] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 07:26:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 07:26:50] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 07:26:50] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 07:26:50] d2.data.build INFO: Making batched data loader with batch_size=4
[10/17 07:26:50] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:26:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:26:50] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 07:26:50] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 07:26:50] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 07:26:50] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 07:26:50] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 07:27:40] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 07:27:41] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 07:27:41] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 07:27:41] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 2                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 07:27:41] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 07:27:41] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 07:27:48] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.36 seconds.
[10/17 07:27:48] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 07:27:52] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 07:27:53] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 07:27:53] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 07:27:53] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 07:27:53] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 07:27:53] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 07:27:54] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 07:27:54] d2.data.build INFO: Making batched data loader with batch_size=2
[10/17 07:27:54] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:27:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:27:54] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 07:27:54] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 07:27:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 07:27:54] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 07:27:54] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 07:28:07] d2.engine.train_loop ERROR: Exception in writing metrics: 
Traceback (most recent call last):
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 370, in _write_metrics
    SimpleTrainer.write_metrics(loss_dict, data_time, iter, prefix)
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 388, in write_metrics
    metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 388, in <dictcomp>
    metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}
AttributeError: 'float' object has no attribute 'detach'
[10/17 07:28:07] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 518, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 370, in _write_metrics
    SimpleTrainer.write_metrics(loss_dict, data_time, iter, prefix)
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 388, in write_metrics
    metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 388, in <dictcomp>
    metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}
AttributeError: 'float' object has no attribute 'detach'
[10/17 07:28:07] d2.engine.hooks INFO: Total training time: 0:00:13 (0:00:00 on hooks)
[10/17 07:28:07] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 0M
[10/17 07:28:42] detectron2 INFO: Rank of current process: 0. World size: 1
[10/17 07:28:43] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/17 07:28:43] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/17 07:28:43] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/17 07:28:43] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/17 07:28:43] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/17 07:28:50] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.38 seconds.
[10/17 07:28:51] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/17 07:28:55] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/17 07:28:56] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/17 07:28:56] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/17 07:28:56] d2.data.build INFO: Using training sampler TrainingSampler
[10/17 07:28:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 07:28:56] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/17 07:28:56] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/17 07:28:56] d2.data.build INFO: Making batched data loader with batch_size=1
[10/17 07:28:56] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:28:56] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/vaishali/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[10/17 07:28:56] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/17 07:28:56] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone - Total num: 53
[10/17 07:28:56] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/17 07:28:56] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/17 07:28:56] d2.engine.train_loop INFO: Starting training from iteration 0
[10/17 07:30:55] d2.utils.events INFO:  eta: 1 day, 1:53:59  iter: 19  total_loss: 151.7  loss_ce: 7.379  loss_mask: 2.827  loss_dice: 4.861  loss_ce_0: 9.076  loss_mask_0: 2.164  loss_dice_0: 4.228  loss_ce_1: 7.204  loss_mask_1: 1.806  loss_dice_1: 4.499  loss_ce_2: 8.153  loss_mask_2: 1.815  loss_dice_2: 4.71  loss_ce_3: 7.809  loss_mask_3: 2.756  loss_dice_3: 4.85  loss_ce_4: 8.12  loss_mask_4: 2.938  loss_dice_4: 4.646  loss_ce_5: 8.644  loss_mask_5: 2.847  loss_dice_5: 4.707  loss_ce_6: 8.478  loss_mask_6: 2.815  loss_dice_6: 4.783  loss_ce_7: 7.877  loss_mask_7: 2.659  loss_dice_7: 4.834  loss_ce_8: 8.297  loss_mask_8: 2.892  loss_dice_8: 4.753  loss_mars: 0.3116    time: 6.0890  last_time: 5.9836  data_time: 0.0100  last_data_time: 0.0025   lr: 7.6924e-06  max_mem: 0M
[10/17 07:32:51] d2.utils.events INFO:  eta: 1 day, 1:37:13  iter: 39  total_loss: 126.7  loss_ce: 5.149  loss_mask: 1.738  loss_dice: 4.753  loss_ce_0: 9.04  loss_mask_0: 1.09  loss_dice_0: 4.481  loss_ce_1: 6.21  loss_mask_1: 1.056  loss_dice_1: 4.55  loss_ce_2: 6.247  loss_mask_2: 1.526  loss_dice_2: 4.593  loss_ce_3: 5.388  loss_mask_3: 1.242  loss_dice_3: 4.674  loss_ce_4: 4.938  loss_mask_4: 2.069  loss_dice_4: 4.728  loss_ce_5: 5.555  loss_mask_5: 1.953  loss_dice_5: 4.722  loss_ce_6: 5.193  loss_mask_6: 1.764  loss_dice_6: 4.774  loss_ce_7: 4.725  loss_mask_7: 1.706  loss_dice_7: 4.73  loss_ce_8: 4.904  loss_mask_8: 1.745  loss_dice_8: 4.703  loss_mars: 0.3336    time: 5.9295  last_time: 5.2746  data_time: 0.0025  last_data_time: 0.0018   lr: 1.5684e-05  max_mem: 0M
[10/17 07:34:42] d2.utils.events INFO:  eta: 1 day, 1:00:36  iter: 59  total_loss: 113.3  loss_ce: 4.954  loss_mask: 1.181  loss_dice: 4.758  loss_ce_0: 8.791  loss_mask_0: 0.9745  loss_dice_0: 4.557  loss_ce_1: 5.178  loss_mask_1: 1.174  loss_dice_1: 4.574  loss_ce_2: 4.541  loss_mask_2: 1.21  loss_dice_2: 4.566  loss_ce_3: 4.748  loss_mask_3: 1.205  loss_dice_3: 4.652  loss_ce_4: 4.559  loss_mask_4: 1.208  loss_dice_4: 4.656  loss_ce_5: 4.867  loss_mask_5: 1.197  loss_dice_5: 4.71  loss_ce_6: 4.651  loss_mask_6: 1.033  loss_dice_6: 4.774  loss_ce_7: 4.752  loss_mask_7: 1.256  loss_dice_7: 4.745  loss_ce_8: 5.206  loss_mask_8: 1.287  loss_dice_8: 4.72  loss_mars: 0.3319    time: 5.8062  last_time: 5.9767  data_time: 0.0031  last_data_time: 0.0034   lr: 2.3676e-05  max_mem: 0M
[10/17 07:36:36] d2.utils.events INFO:  eta: 1 day, 0:37:54  iter: 79  total_loss: 104.3  loss_ce: 3.675  loss_mask: 1.798  loss_dice: 4.614  loss_ce_0: 8.955  loss_mask_0: 1.471  loss_dice_0: 4.326  loss_ce_1: 3.93  loss_mask_1: 1.544  loss_dice_1: 4.372  loss_ce_2: 3.608  loss_mask_2: 1.493  loss_dice_2: 4.374  loss_ce_3: 3.529  loss_mask_3: 1.653  loss_dice_3: 4.429  loss_ce_4: 3.483  loss_mask_4: 1.782  loss_dice_4: 4.508  loss_ce_5: 3.736  loss_mask_5: 1.833  loss_dice_5: 4.517  loss_ce_6: 3.71  loss_mask_6: 1.718  loss_dice_6: 4.551  loss_ce_7: 3.722  loss_mask_7: 1.735  loss_dice_7: 4.589  loss_ce_8: 3.709  loss_mask_8: 1.89  loss_dice_8: 4.552  loss_mars: 0.3321    time: 5.7728  last_time: 6.0476  data_time: 0.0029  last_data_time: 0.0085   lr: 3.1668e-05  max_mem: 0M
[10/17 07:38:42] d2.utils.events INFO:  eta: 1 day, 1:00:00  iter: 99  total_loss: 107.6  loss_ce: 3.805  loss_mask: 1.903  loss_dice: 4.823  loss_ce_0: 8.89  loss_mask_0: 1.36  loss_dice_0: 4.34  loss_ce_1: 4.278  loss_mask_1: 1.409  loss_dice_1: 4.507  loss_ce_2: 3.622  loss_mask_2: 1.448  loss_dice_2: 4.632  loss_ce_3: 3.629  loss_mask_3: 1.717  loss_dice_3: 4.796  loss_ce_4: 3.483  loss_mask_4: 1.678  loss_dice_4: 4.786  loss_ce_5: 3.425  loss_mask_5: 1.991  loss_dice_5: 4.772  loss_ce_6: 3.483  loss_mask_6: 1.831  loss_dice_6: 4.782  loss_ce_7: 3.603  loss_mask_7: 1.931  loss_dice_7: 4.828  loss_ce_8: 3.646  loss_mask_8: 1.955  loss_dice_8: 4.827  loss_mars: 0.2241    time: 5.8824  last_time: 5.7381  data_time: 0.0029  last_data_time: 0.0017   lr: 3.966e-05  max_mem: 0M
[10/17 07:40:44] d2.utils.events INFO:  eta: 1 day, 1:22:21  iter: 119  total_loss: 106  loss_ce: 4.15  loss_mask: 1.546  loss_dice: 4.628  loss_ce_0: 8.915  loss_mask_0: 0.8844  loss_dice_0: 4.468  loss_ce_1: 4.409  loss_mask_1: 0.9291  loss_dice_1: 4.524  loss_ce_2: 4.453  loss_mask_2: 1.202  loss_dice_2: 4.478  loss_ce_3: 4.072  loss_mask_3: 1.33  loss_dice_3: 4.566  loss_ce_4: 4.273  loss_mask_4: 1.365  loss_dice_4: 4.625  loss_ce_5: 4.237  loss_mask_5: 1.458  loss_dice_5: 4.626  loss_ce_6: 4.207  loss_mask_6: 1.401  loss_dice_6: 4.648  loss_ce_7: 4.028  loss_mask_7: 1.444  loss_dice_7: 4.652  loss_ce_8: 4.176  loss_mask_8: 1.472  loss_dice_8: 4.605  loss_mars: 0.4114    time: 5.9179  last_time: 6.2009  data_time: 0.0030  last_data_time: 0.0017   lr: 4.7652e-05  max_mem: 0M
[10/17 07:42:57] d2.utils.events INFO:  eta: 1 day, 1:34:42  iter: 139  total_loss: 106.5  loss_ce: 3.659  loss_mask: 2.724  loss_dice: 4.659  loss_ce_0: 8.949  loss_mask_0: 1.765  loss_dice_0: 3.896  loss_ce_1: 3.661  loss_mask_1: 1.684  loss_dice_1: 3.871  loss_ce_2: 3.601  loss_mask_2: 2.071  loss_dice_2: 4.458  loss_ce_3: 3.632  loss_mask_3: 2.205  loss_dice_3: 4.621  loss_ce_4: 3.581  loss_mask_4: 2.476  loss_dice_4: 4.594  loss_ce_5: 3.343  loss_mask_5: 2.426  loss_dice_5: 4.643  loss_ce_6: 3.441  loss_mask_6: 2.819  loss_dice_6: 4.648  loss_ce_7: 3.367  loss_mask_7: 2.574  loss_dice_7: 4.619  loss_ce_8: 3.38  loss_mask_8: 2.607  loss_dice_8: 4.67  loss_mars: 0.2058    time: 6.0233  last_time: 6.3567  data_time: 0.0034  last_data_time: 0.0031   lr: 5.5644e-05  max_mem: 0M
[10/17 07:46:16] d2.utils.events INFO:  eta: 1 day, 1:46:08  iter: 159  total_loss: 108.7  loss_ce: 4.02  loss_mask: 1.308  loss_dice: 4.77  loss_ce_0: 8.782  loss_mask_0: 1.061  loss_dice_0: 4.638  loss_ce_1: 4.093  loss_mask_1: 1.104  loss_dice_1: 4.635  loss_ce_2: 4.163  loss_mask_2: 1.178  loss_dice_2: 4.673  loss_ce_3: 4.207  loss_mask_3: 1.353  loss_dice_3: 4.688  loss_ce_4: 4.027  loss_mask_4: 1.304  loss_dice_4: 4.735  loss_ce_5: 4.012  loss_mask_5: 1.572  loss_dice_5: 4.709  loss_ce_6: 3.912  loss_mask_6: 1.356  loss_dice_6: 4.737  loss_ce_7: 3.954  loss_mask_7: 1.29  loss_dice_7: 4.817  loss_ce_8: 4.08  loss_mask_8: 1.346  loss_dice_8: 4.737  loss_mars: 0.1302    time: 6.5216  last_time: 9.5487  data_time: 0.0028  last_data_time: 0.0020   lr: 6.3636e-05  max_mem: 0M
[10/17 07:48:36] d2.utils.events INFO:  eta: 1 day, 1:44:08  iter: 179  total_loss: 108.8  loss_ce: 3.109  loss_mask: 2.731  loss_dice: 4.735  loss_ce_0: 8.806  loss_mask_0: 1.08  loss_dice_0: 4.463  loss_ce_1: 3.154  loss_mask_1: 1.052  loss_dice_1: 4.513  loss_ce_2: 3.078  loss_mask_2: 1.128  loss_dice_2: 4.539  loss_ce_3: 3.174  loss_mask_3: 1.714  loss_dice_3: 4.592  loss_ce_4: 2.985  loss_mask_4: 2.205  loss_dice_4: 4.825  loss_ce_5: 3.302  loss_mask_5: 2.538  loss_dice_5: 4.762  loss_ce_6: 3.188  loss_mask_6: 2.542  loss_dice_6: 4.732  loss_ce_7: 3.16  loss_mask_7: 2.596  loss_dice_7: 4.691  loss_ce_8: 3.183  loss_mask_8: 2.459  loss_dice_8: 4.708  loss_mars: 0.2943    time: 6.5719  last_time: 5.9448  data_time: 0.0034  last_data_time: 0.0023   lr: 7.1628e-05  max_mem: 0M
[10/17 07:50:34] d2.utils.events INFO:  eta: 1 day, 1:40:25  iter: 199  total_loss: 99.46  loss_ce: 3.054  loss_mask: 1.818  loss_dice: 4.722  loss_ce_0: 8.648  loss_mask_0: 0.9514  loss_dice_0: 4.559  loss_ce_1: 3.007  loss_mask_1: 0.9538  loss_dice_1: 4.568  loss_ce_2: 2.931  loss_mask_2: 0.9665  loss_dice_2: 4.579  loss_ce_3: 3.084  loss_mask_3: 1.252  loss_dice_3: 4.648  loss_ce_4: 3.042  loss_mask_4: 1.773  loss_dice_4: 4.672  loss_ce_5: 3.167  loss_mask_5: 1.836  loss_dice_5: 4.698  loss_ce_6: 3.175  loss_mask_6: 1.896  loss_dice_6: 4.752  loss_ce_7: 2.98  loss_mask_7: 1.707  loss_dice_7: 4.734  loss_ce_8: 2.986  loss_mask_8: 1.723  loss_dice_8: 4.668  loss_mars: 0.2577    time: 6.5065  last_time: 6.5900  data_time: 0.0034  last_data_time: 0.0070   lr: 7.962e-05  max_mem: 0M
[10/17 07:52:30] d2.utils.events INFO:  eta: 1 day, 1:37:36  iter: 219  total_loss: 98.07  loss_ce: 2.653  loss_mask: 2.357  loss_dice: 4.566  loss_ce_0: 8.693  loss_mask_0: 1.433  loss_dice_0: 4.081  loss_ce_1: 2.731  loss_mask_1: 1.41  loss_dice_1: 4.12  loss_ce_2: 2.805  loss_mask_2: 1.437  loss_dice_2: 4.143  loss_ce_3: 2.602  loss_mask_3: 1.521  loss_dice_3: 4.26  loss_ce_4: 2.645  loss_mask_4: 2.063  loss_dice_4: 4.464  loss_ce_5: 2.748  loss_mask_5: 2.258  loss_dice_5: 4.594  loss_ce_6: 2.701  loss_mask_6: 2.237  loss_dice_6: 4.563  loss_ce_7: 2.577  loss_mask_7: 2.198  loss_dice_7: 4.532  loss_ce_8: 2.616  loss_mask_8: 2.198  loss_dice_8: 4.54  loss_mars: 0.3087    time: 6.4398  last_time: 6.0746  data_time: 0.0026  last_data_time: 0.0035   lr: 8.7612e-05  max_mem: 0M
[10/17 07:54:31] d2.utils.events INFO:  eta: 1 day, 1:36:22  iter: 239  total_loss: 100.8  loss_ce: 4.046  loss_mask: 0.8684  loss_dice: 4.832  loss_ce_0: 8.371  loss_mask_0: 0.6031  loss_dice_0: 4.616  loss_ce_1: 3.81  loss_mask_1: 0.6372  loss_dice_1: 4.65  loss_ce_2: 3.862  loss_mask_2: 0.6166  loss_dice_2: 4.679  loss_ce_3: 4.001  loss_mask_3: 0.6418  loss_dice_3: 4.657  loss_ce_4: 3.939  loss_mask_4: 0.7651  loss_dice_4: 4.773  loss_ce_5: 3.77  loss_mask_5: 0.8484  loss_dice_5: 4.82  loss_ce_6: 3.911  loss_mask_6: 0.8801  loss_dice_6: 4.834  loss_ce_7: 3.966  loss_mask_7: 0.8921  loss_dice_7: 4.833  loss_ce_8: 3.993  loss_mask_8: 0.8895  loss_dice_8: 4.825  loss_mars: 0.1169    time: 6.4088  last_time: 5.5171  data_time: 0.0032  last_data_time: 0.0022   lr: 9.5604e-05  max_mem: 0M
[10/17 07:56:31] d2.utils.events INFO:  eta: 1 day, 1:32:45  iter: 259  total_loss: 95.92  loss_ce: 3.042  loss_mask: 2.159  loss_dice: 4.456  loss_ce_0: 8.559  loss_mask_0: 1.41  loss_dice_0: 4.154  loss_ce_1: 2.801  loss_mask_1: 1.433  loss_dice_1: 4.242  loss_ce_2: 2.922  loss_mask_2: 1.466  loss_dice_2: 4.189  loss_ce_3: 2.787  loss_mask_3: 1.704  loss_dice_3: 4.384  loss_ce_4: 2.975  loss_mask_4: 1.659  loss_dice_4: 4.252  loss_ce_5: 2.973  loss_mask_5: 2.205  loss_dice_5: 4.318  loss_ce_6: 3.077  loss_mask_6: 1.989  loss_dice_6: 4.548  loss_ce_7: 3.096  loss_mask_7: 1.976  loss_dice_7: 4.544  loss_ce_8: 3.157  loss_mask_8: 2.196  loss_dice_8: 4.491  loss_mars: 0.4538    time: 6.3762  last_time: 5.0656  data_time: 0.0030  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/17 07:58:30] d2.utils.events INFO:  eta: 1 day, 1:29:25  iter: 279  total_loss: 97.13  loss_ce: 2.814  loss_mask: 1.359  loss_dice: 4.814  loss_ce_0: 8.354  loss_mask_0: 0.9218  loss_dice_0: 4.445  loss_ce_1: 2.783  loss_mask_1: 1.066  loss_dice_1: 4.523  loss_ce_2: 3.041  loss_mask_2: 0.964  loss_dice_2: 4.518  loss_ce_3: 3.193  loss_mask_3: 0.979  loss_dice_3: 4.629  loss_ce_4: 3.108  loss_mask_4: 0.9979  loss_dice_4: 4.773  loss_ce_5: 3.284  loss_mask_5: 0.9872  loss_dice_5: 4.811  loss_ce_6: 3.168  loss_mask_6: 1.157  loss_dice_6: 4.75  loss_ce_7: 2.856  loss_mask_7: 1.195  loss_dice_7: 4.772  loss_ce_8: 2.997  loss_mask_8: 1.229  loss_dice_8: 4.79  loss_mars: 0.38    time: 6.3458  last_time: 5.5697  data_time: 0.0031  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/17 08:00:22] d2.utils.events INFO:  eta: 1 day, 1:22:26  iter: 299  total_loss: 109.8  loss_ce: 3.9  loss_mask: 2.195  loss_dice: 4.884  loss_ce_0: 8.411  loss_mask_0: 1.067  loss_dice_0: 4.293  loss_ce_1: 3.85  loss_mask_1: 1.055  loss_dice_1: 4.263  loss_ce_2: 3.878  loss_mask_2: 1.099  loss_dice_2: 4.325  loss_ce_3: 4.086  loss_mask_3: 1.216  loss_dice_3: 4.343  loss_ce_4: 3.89  loss_mask_4: 1.09  loss_dice_4: 4.631  loss_ce_5: 3.978  loss_mask_5: 1.772  loss_dice_5: 4.862  loss_ce_6: 4.064  loss_mask_6: 2.039  loss_dice_6: 4.824  loss_ce_7: 3.963  loss_mask_7: 2.214  loss_dice_7: 4.848  loss_ce_8: 4.07  loss_mask_8: 2.258  loss_dice_8: 4.89  loss_mars: 0.2497    time: 6.2960  last_time: 5.5120  data_time: 0.0028  last_data_time: 0.0084   lr: 0.0001  max_mem: 0M
[10/17 08:02:18] d2.utils.events INFO:  eta: 1 day, 1:15:46  iter: 319  total_loss: 109  loss_ce: 2.939  loss_mask: 1.161  loss_dice: 4.735  loss_ce_0: 8.22  loss_mask_0: 0.772  loss_dice_0: 4.347  loss_ce_1: 2.902  loss_mask_1: 0.9035  loss_dice_1: 4.38  loss_ce_2: 2.864  loss_mask_2: 0.7304  loss_dice_2: 4.551  loss_ce_3: 2.816  loss_mask_3: 0.7896  loss_dice_3: 4.516  loss_ce_4: 2.9  loss_mask_4: 0.7787  loss_dice_4: 4.68  loss_ce_5: 3.003  loss_mask_5: 1.048  loss_dice_5: 4.683  loss_ce_6: 2.955  loss_mask_6: 1.163  loss_dice_6: 4.7  loss_ce_7: 3.075  loss_mask_7: 1.238  loss_dice_7: 4.706  loss_ce_8: 3.007  loss_mask_8: 1.182  loss_dice_8: 4.721  loss_mars: 0.3248    time: 6.2629  last_time: 4.7991  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 08:04:12] d2.utils.events INFO:  eta: 1 day, 1:13:08  iter: 339  total_loss: 116.7  loss_ce: 5.463  loss_mask: 1.57  loss_dice: 4.714  loss_ce_0: 8.109  loss_mask_0: 1.012  loss_dice_0: 4.518  loss_ce_1: 4.974  loss_mask_1: 1.016  loss_dice_1: 4.528  loss_ce_2: 4.876  loss_mask_2: 1.096  loss_dice_2: 4.501  loss_ce_3: 5.007  loss_mask_3: 1.06  loss_dice_3: 4.532  loss_ce_4: 5.201  loss_mask_4: 1.609  loss_dice_4: 4.765  loss_ce_5: 4.915  loss_mask_5: 1.861  loss_dice_5: 4.848  loss_ce_6: 5.167  loss_mask_6: 1.809  loss_dice_6: 4.837  loss_ce_7: 5.126  loss_mask_7: 1.641  loss_dice_7: 4.777  loss_ce_8: 5.271  loss_mask_8: 1.523  loss_dice_8: 4.73  loss_mars: 0.3599    time: 6.2304  last_time: 6.0836  data_time: 0.0034  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 08:06:10] d2.utils.events INFO:  eta: 1 day, 1:11:09  iter: 359  total_loss: 104.9  loss_ce: 3.818  loss_mask: 1.581  loss_dice: 4.649  loss_ce_0: 8.086  loss_mask_0: 0.9997  loss_dice_0: 4.487  loss_ce_1: 3.641  loss_mask_1: 1.025  loss_dice_1: 4.487  loss_ce_2: 3.888  loss_mask_2: 1.049  loss_dice_2: 4.476  loss_ce_3: 3.838  loss_mask_3: 1.089  loss_dice_3: 4.509  loss_ce_4: 3.944  loss_mask_4: 1.041  loss_dice_4: 4.476  loss_ce_5: 3.841  loss_mask_5: 1.278  loss_dice_5: 4.519  loss_ce_6: 3.961  loss_mask_6: 1.498  loss_dice_6: 4.663  loss_ce_7: 3.825  loss_mask_7: 1.566  loss_dice_7: 4.64  loss_ce_8: 3.773  loss_mask_8: 1.59  loss_dice_8: 4.655  loss_mars: 0.3849    time: 6.2112  last_time: 5.4182  data_time: 0.0029  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 08:08:11] d2.utils.events INFO:  eta: 1 day, 1:09:42  iter: 379  total_loss: 101.7  loss_ce: 4.015  loss_mask: 1.668  loss_dice: 4.57  loss_ce_0: 8.048  loss_mask_0: 1.065  loss_dice_0: 4.202  loss_ce_1: 3.372  loss_mask_1: 1.048  loss_dice_1: 4.236  loss_ce_2: 3.748  loss_mask_2: 1.052  loss_dice_2: 4.27  loss_ce_3: 3.775  loss_mask_3: 1.233  loss_dice_3: 4.264  loss_ce_4: 3.889  loss_mask_4: 1.306  loss_dice_4: 4.328  loss_ce_5: 3.824  loss_mask_5: 1.406  loss_dice_5: 4.527  loss_ce_6: 3.896  loss_mask_6: 1.463  loss_dice_6: 4.603  loss_ce_7: 3.897  loss_mask_7: 1.515  loss_dice_7: 4.624  loss_ce_8: 3.971  loss_mask_8: 1.536  loss_dice_8: 4.62  loss_mars: 0.5683    time: 6.2037  last_time: 6.6027  data_time: 0.0032  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 08:10:05] d2.utils.events INFO:  eta: 1 day, 1:05:37  iter: 399  total_loss: 101.8  loss_ce: 3.516  loss_mask: 2.23  loss_dice: 4.407  loss_ce_0: 7.997  loss_mask_0: 1.43  loss_dice_0: 4.282  loss_ce_1: 3.615  loss_mask_1: 1.504  loss_dice_1: 4.143  loss_ce_2: 3.487  loss_mask_2: 1.819  loss_dice_2: 4.183  loss_ce_3: 3.799  loss_mask_3: 1.871  loss_dice_3: 4.286  loss_ce_4: 3.689  loss_mask_4: 1.614  loss_dice_4: 4.181  loss_ce_5: 3.834  loss_mask_5: 1.743  loss_dice_5: 4.231  loss_ce_6: 3.559  loss_mask_6: 2.212  loss_dice_6: 4.318  loss_ce_7: 3.484  loss_mask_7: 2.067  loss_dice_7: 4.356  loss_ce_8: 3.512  loss_mask_8: 2.328  loss_dice_8: 4.331  loss_mars: 0.4062    time: 6.1779  last_time: 5.3663  data_time: 0.0037  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 08:12:04] d2.utils.events INFO:  eta: 1 day, 1:03:38  iter: 419  total_loss: 97.97  loss_ce: 3.301  loss_mask: 2.226  loss_dice: 4.668  loss_ce_0: 7.85  loss_mask_0: 1.079  loss_dice_0: 4.127  loss_ce_1: 3.378  loss_mask_1: 1.198  loss_dice_1: 4.261  loss_ce_2: 3.293  loss_mask_2: 1.226  loss_dice_2: 4.088  loss_ce_3: 3.578  loss_mask_3: 1.089  loss_dice_3: 4.116  loss_ce_4: 3.305  loss_mask_4: 1.051  loss_dice_4: 4.313  loss_ce_5: 3.451  loss_mask_5: 1.12  loss_dice_5: 4.315  loss_ce_6: 3.435  loss_mask_6: 1.445  loss_dice_6: 4.555  loss_ce_7: 3.328  loss_mask_7: 2.276  loss_dice_7: 4.648  loss_ce_8: 3.248  loss_mask_8: 1.928  loss_dice_8: 4.678  loss_mars: 0.4056    time: 6.1668  last_time: 6.2398  data_time: 0.0036  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/17 08:13:56] d2.utils.events INFO:  eta: 1 day, 1:00:42  iter: 439  total_loss: 105.2  loss_ce: 4.062  loss_mask: 1.732  loss_dice: 4.576  loss_ce_0: 7.735  loss_mask_0: 1.282  loss_dice_0: 4.382  loss_ce_1: 3.557  loss_mask_1: 1.177  loss_dice_1: 4.39  loss_ce_2: 3.33  loss_mask_2: 1.232  loss_dice_2: 4.488  loss_ce_3: 3.54  loss_mask_3: 1.269  loss_dice_3: 4.417  loss_ce_4: 3.8  loss_mask_4: 1.435  loss_dice_4: 4.507  loss_ce_5: 3.968  loss_mask_5: 1.471  loss_dice_5: 4.468  loss_ce_6: 4.175  loss_mask_6: 1.302  loss_dice_6: 4.559  loss_ce_7: 4.089  loss_mask_7: 1.419  loss_dice_7: 4.677  loss_ce_8: 4.057  loss_mask_8: 1.607  loss_dice_8: 4.611  loss_mars: 0.4445    time: 6.1417  last_time: 8.7006  data_time: 0.0024  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/17 08:15:47] d2.utils.events INFO:  eta: 1 day, 0:56:21  iter: 459  total_loss: 99.36  loss_ce: 3.972  loss_mask: 1.338  loss_dice: 4.714  loss_ce_0: 7.532  loss_mask_0: 0.831  loss_dice_0: 4.447  loss_ce_1: 3.655  loss_mask_1: 0.9193  loss_dice_1: 4.342  loss_ce_2: 3.404  loss_mask_2: 1.092  loss_dice_2: 4.569  loss_ce_3: 3.653  loss_mask_3: 1.089  loss_dice_3: 4.446  loss_ce_4: 3.548  loss_mask_4: 1.114  loss_dice_4: 4.375  loss_ce_5: 3.681  loss_mask_5: 0.9819  loss_dice_5: 4.428  loss_ce_6: 3.933  loss_mask_6: 0.888  loss_dice_6: 4.644  loss_ce_7: 3.77  loss_mask_7: 0.9166  loss_dice_7: 4.607  loss_ce_8: 3.963  loss_mask_8: 1.273  loss_dice_8: 4.627  loss_mars: 0.1591    time: 6.1161  last_time: 6.1690  data_time: 0.0029  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 08:17:42] d2.utils.events INFO:  eta: 1 day, 0:54:17  iter: 479  total_loss: 96.2  loss_ce: 3.439  loss_mask: 1.757  loss_dice: 4.492  loss_ce_0: 7.534  loss_mask_0: 1.382  loss_dice_0: 4.237  loss_ce_1: 3.239  loss_mask_1: 1.295  loss_dice_1: 4.178  loss_ce_2: 3.118  loss_mask_2: 1.38  loss_dice_2: 4.231  loss_ce_3: 3.095  loss_mask_3: 1.255  loss_dice_3: 4.396  loss_ce_4: 3.047  loss_mask_4: 1.367  loss_dice_4: 4.472  loss_ce_5: 3.506  loss_mask_5: 1.197  loss_dice_5: 4.412  loss_ce_6: 3.574  loss_mask_6: 1.223  loss_dice_6: 4.481  loss_ce_7: 3.357  loss_mask_7: 1.216  loss_dice_7: 4.45  loss_ce_8: 3.33  loss_mask_8: 1.58  loss_dice_8: 4.559  loss_mars: 0.2912    time: 6.1006  last_time: 5.2649  data_time: 0.0033  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 08:19:42] d2.utils.events INFO:  eta: 1 day, 0:54:46  iter: 499  total_loss: 103.4  loss_ce: 4.234  loss_mask: 1.071  loss_dice: 4.754  loss_ce_0: 7.493  loss_mask_0: 0.8067  loss_dice_0: 3.998  loss_ce_1: 3.555  loss_mask_1: 0.841  loss_dice_1: 3.851  loss_ce_2: 3.691  loss_mask_2: 0.9925  loss_dice_2: 3.931  loss_ce_3: 3.791  loss_mask_3: 0.8358  loss_dice_3: 3.983  loss_ce_4: 3.892  loss_mask_4: 0.882  loss_dice_4: 3.976  loss_ce_5: 4.159  loss_mask_5: 0.851  loss_dice_5: 4.095  loss_ce_6: 4.231  loss_mask_6: 0.8303  loss_dice_6: 4.258  loss_ce_7: 4.203  loss_mask_7: 0.8257  loss_dice_7: 4.59  loss_ce_8: 4.109  loss_mask_8: 0.8801  loss_dice_8: 4.768  loss_mars: 0.3522    time: 6.0969  last_time: 6.5493  data_time: 0.0037  last_data_time: 0.0034   lr: 0.0001  max_mem: 0M
[10/17 08:21:38] d2.utils.events INFO:  eta: 1 day, 0:50:25  iter: 519  total_loss: 101.5  loss_ce: 3.461  loss_mask: 1.899  loss_dice: 4.545  loss_ce_0: 7.449  loss_mask_0: 1.726  loss_dice_0: 3.992  loss_ce_1: 3.156  loss_mask_1: 1.8  loss_dice_1: 3.946  loss_ce_2: 2.983  loss_mask_2: 1.751  loss_dice_2: 4.108  loss_ce_3: 3.071  loss_mask_3: 1.802  loss_dice_3: 4.191  loss_ce_4: 3.115  loss_mask_4: 1.793  loss_dice_4: 4.27  loss_ce_5: 3.246  loss_mask_5: 1.677  loss_dice_5: 4.222  loss_ce_6: 3.453  loss_mask_6: 1.794  loss_dice_6: 4.294  loss_ce_7: 3.478  loss_mask_7: 1.758  loss_dice_7: 4.393  loss_ce_8: 3.599  loss_mask_8: 1.882  loss_dice_8: 4.478  loss_mars: 0.3803    time: 6.0843  last_time: 5.3071  data_time: 0.0031  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 08:23:31] d2.utils.events INFO:  eta: 1 day, 0:47:56  iter: 539  total_loss: 99.53  loss_ce: 3.686  loss_mask: 2.372  loss_dice: 4.467  loss_ce_0: 7.329  loss_mask_0: 1.336  loss_dice_0: 4.209  loss_ce_1: 2.887  loss_mask_1: 1.483  loss_dice_1: 4.212  loss_ce_2: 2.889  loss_mask_2: 1.625  loss_dice_2: 4.018  loss_ce_3: 3.012  loss_mask_3: 1.804  loss_dice_3: 4.247  loss_ce_4: 3.176  loss_mask_4: 2.021  loss_dice_4: 4.374  loss_ce_5: 3.233  loss_mask_5: 1.931  loss_dice_5: 4.362  loss_ce_6: 3.574  loss_mask_6: 1.848  loss_dice_6: 4.395  loss_ce_7: 3.776  loss_mask_7: 1.725  loss_dice_7: 4.402  loss_ce_8: 3.677  loss_mask_8: 1.817  loss_dice_8: 4.497  loss_mars: 0.4503    time: 6.0688  last_time: 5.8099  data_time: 0.0026  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 08:25:26] d2.utils.events INFO:  eta: 1 day, 0:45:58  iter: 559  total_loss: 99.19  loss_ce: 4.075  loss_mask: 1.536  loss_dice: 4.367  loss_ce_0: 7.325  loss_mask_0: 1.537  loss_dice_0: 4.136  loss_ce_1: 3.472  loss_mask_1: 1.537  loss_dice_1: 4.008  loss_ce_2: 3.53  loss_mask_2: 1.597  loss_dice_2: 4.125  loss_ce_3: 3.722  loss_mask_3: 1.606  loss_dice_3: 4.15  loss_ce_4: 3.822  loss_mask_4: 1.64  loss_dice_4: 4.199  loss_ce_5: 3.864  loss_mask_5: 1.708  loss_dice_5: 4.243  loss_ce_6: 4.102  loss_mask_6: 1.652  loss_dice_6: 4.258  loss_ce_7: 4.357  loss_mask_7: 1.476  loss_dice_7: 4.334  loss_ce_8: 4.37  loss_mask_8: 1.542  loss_dice_8: 4.284  loss_mars: 0.2121    time: 6.0577  last_time: 5.9764  data_time: 0.0035  last_data_time: 0.0083   lr: 0.0001  max_mem: 0M
[10/17 08:27:18] d2.utils.events INFO:  eta: 1 day, 0:38:21  iter: 579  total_loss: 97.44  loss_ce: 3.012  loss_mask: 2.261  loss_dice: 3.803  loss_ce_0: 6.975  loss_mask_0: 1.558  loss_dice_0: 3.785  loss_ce_1: 2.809  loss_mask_1: 1.646  loss_dice_1: 3.792  loss_ce_2: 2.824  loss_mask_2: 1.709  loss_dice_2: 3.954  loss_ce_3: 3.098  loss_mask_3: 1.736  loss_dice_3: 3.955  loss_ce_4: 3.258  loss_mask_4: 1.765  loss_dice_4: 3.922  loss_ce_5: 3.726  loss_mask_5: 1.729  loss_dice_5: 3.88  loss_ce_6: 3.09  loss_mask_6: 1.708  loss_dice_6: 3.832  loss_ce_7: 2.981  loss_mask_7: 1.759  loss_dice_7: 3.945  loss_ce_8: 2.85  loss_mask_8: 1.86  loss_dice_8: 3.968  loss_mars: 0.6115    time: 6.0413  last_time: 5.0951  data_time: 0.0027  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 08:29:09] d2.utils.events INFO:  eta: 1 day, 0:36:23  iter: 599  total_loss: 101  loss_ce: 3.005  loss_mask: 1.951  loss_dice: 4.662  loss_ce_0: 6.878  loss_mask_0: 1.528  loss_dice_0: 4.191  loss_ce_1: 2.816  loss_mask_1: 1.487  loss_dice_1: 4.422  loss_ce_2: 2.483  loss_mask_2: 1.52  loss_dice_2: 4.596  loss_ce_3: 2.471  loss_mask_3: 1.675  loss_dice_3: 4.589  loss_ce_4: 2.559  loss_mask_4: 1.524  loss_dice_4: 4.56  loss_ce_5: 2.556  loss_mask_5: 1.532  loss_dice_5: 4.44  loss_ce_6: 2.911  loss_mask_6: 1.53  loss_dice_6: 4.401  loss_ce_7: 3.383  loss_mask_7: 1.67  loss_dice_7: 4.593  loss_ce_8: 3.539  loss_mask_8: 1.689  loss_dice_8: 4.68  loss_mars: 0.4034    time: 6.0250  last_time: 4.2129  data_time: 0.0035  last_data_time: 0.0078   lr: 0.0001  max_mem: 0M
[10/17 08:31:05] d2.utils.events INFO:  eta: 1 day, 0:33:54  iter: 619  total_loss: 92.9  loss_ce: 3.366  loss_mask: 1.209  loss_dice: 4.563  loss_ce_0: 6.886  loss_mask_0: 1.097  loss_dice_0: 4.431  loss_ce_1: 2.939  loss_mask_1: 0.9483  loss_dice_1: 4.426  loss_ce_2: 2.804  loss_mask_2: 1.069  loss_dice_2: 4.403  loss_ce_3: 2.954  loss_mask_3: 1.005  loss_dice_3: 4.422  loss_ce_4: 2.953  loss_mask_4: 1.051  loss_dice_4: 4.484  loss_ce_5: 3.364  loss_mask_5: 1.063  loss_dice_5: 4.492  loss_ce_6: 3.148  loss_mask_6: 1.299  loss_dice_6: 4.48  loss_ce_7: 3.404  loss_mask_7: 1.263  loss_dice_7: 4.494  loss_ce_8: 3.588  loss_mask_8: 1.529  loss_dice_8: 4.526  loss_mars: 0.6561    time: 6.0176  last_time: 6.0004  data_time: 0.0030  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/17 08:33:05] d2.utils.events INFO:  eta: 1 day, 0:31:23  iter: 639  total_loss: 100.7  loss_ce: 4.333  loss_mask: 1.358  loss_dice: 4.549  loss_ce_0: 6.93  loss_mask_0: 1.035  loss_dice_0: 4.041  loss_ce_1: 3.643  loss_mask_1: 1.132  loss_dice_1: 4.185  loss_ce_2: 3.821  loss_mask_2: 1.177  loss_dice_2: 4.269  loss_ce_3: 3.771  loss_mask_3: 1.172  loss_dice_3: 4.223  loss_ce_4: 3.665  loss_mask_4: 1.174  loss_dice_4: 4.155  loss_ce_5: 4.376  loss_mask_5: 1.252  loss_dice_5: 4.281  loss_ce_6: 4.713  loss_mask_6: 1.13  loss_dice_6: 4.242  loss_ce_7: 4.769  loss_mask_7: 1.144  loss_dice_7: 4.084  loss_ce_8: 4.888  loss_mask_8: 1.108  loss_dice_8: 4.46  loss_mars: 0.6074    time: 6.0171  last_time: 8.3961  data_time: 0.0027  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 08:34:57] d2.utils.events INFO:  eta: 1 day, 0:26:54  iter: 659  total_loss: 102.1  loss_ce: 3.642  loss_mask: 1.639  loss_dice: 4.624  loss_ce_0: 6.728  loss_mask_0: 0.9928  loss_dice_0: 4.43  loss_ce_1: 3.169  loss_mask_1: 1.062  loss_dice_1: 4.525  loss_ce_2: 3.072  loss_mask_2: 1.147  loss_dice_2: 4.455  loss_ce_3: 3.14  loss_mask_3: 1.157  loss_dice_3: 4.527  loss_ce_4: 3.335  loss_mask_4: 1.146  loss_dice_4: 4.532  loss_ce_5: 3.491  loss_mask_5: 1.091  loss_dice_5: 4.552  loss_ce_6: 3.522  loss_mask_6: 1.27  loss_dice_6: 4.552  loss_ce_7: 3.541  loss_mask_7: 1.426  loss_dice_7: 4.639  loss_ce_8: 3.72  loss_mask_8: 1.163  loss_dice_8: 4.546  loss_mars: 0.4022    time: 6.0036  last_time: 5.2391  data_time: 0.0038  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 08:36:54] d2.utils.events INFO:  eta: 1 day, 0:25:46  iter: 679  total_loss: 94.16  loss_ce: 4.385  loss_mask: 0.7918  loss_dice: 4.721  loss_ce_0: 6.83  loss_mask_0: 0.8145  loss_dice_0: 4.479  loss_ce_1: 3.385  loss_mask_1: 0.7607  loss_dice_1: 4.501  loss_ce_2: 3.396  loss_mask_2: 0.7289  loss_dice_2: 4.267  loss_ce_3: 3.391  loss_mask_3: 0.7733  loss_dice_3: 4.411  loss_ce_4: 3.589  loss_mask_4: 0.7547  loss_dice_4: 4.46  loss_ce_5: 3.575  loss_mask_5: 0.7305  loss_dice_5: 4.501  loss_ce_6: 3.919  loss_mask_6: 0.7303  loss_dice_6: 4.365  loss_ce_7: 4.11  loss_mask_7: 0.7313  loss_dice_7: 4.547  loss_ce_8: 4.246  loss_mask_8: 0.722  loss_dice_8: 4.708  loss_mars: 0.5539    time: 6.0003  last_time: 6.4311  data_time: 0.0032  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/17 08:38:46] d2.utils.events INFO:  eta: 1 day, 0:21:39  iter: 699  total_loss: 93.35  loss_ce: 3.413  loss_mask: 1.043  loss_dice: 4.647  loss_ce_0: 6.643  loss_mask_0: 0.9914  loss_dice_0: 4.508  loss_ce_1: 2.93  loss_mask_1: 1.155  loss_dice_1: 4.422  loss_ce_2: 2.84  loss_mask_2: 1.083  loss_dice_2: 4.438  loss_ce_3: 2.91  loss_mask_3: 1.185  loss_dice_3: 4.477  loss_ce_4: 2.862  loss_mask_4: 1.121  loss_dice_4: 4.496  loss_ce_5: 2.774  loss_mask_5: 1.19  loss_dice_5: 4.55  loss_ce_6: 3.012  loss_mask_6: 1.12  loss_dice_6: 4.593  loss_ce_7: 3.073  loss_mask_7: 1.135  loss_dice_7: 4.457  loss_ce_8: 3.389  loss_mask_8: 1.098  loss_dice_8: 4.613  loss_mars: 0.5848    time: 5.9878  last_time: 3.8109  data_time: 0.0029  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 08:40:49] d2.utils.events INFO:  eta: 1 day, 0:22:55  iter: 719  total_loss: 92.4  loss_ce: 3.274  loss_mask: 0.8607  loss_dice: 4.691  loss_ce_0: 6.285  loss_mask_0: 0.9566  loss_dice_0: 4.509  loss_ce_1: 2.432  loss_mask_1: 0.9826  loss_dice_1: 4.459  loss_ce_2: 2.286  loss_mask_2: 0.9538  loss_dice_2: 4.552  loss_ce_3: 2.213  loss_mask_3: 0.9542  loss_dice_3: 4.522  loss_ce_4: 2.381  loss_mask_4: 0.9071  loss_dice_4: 4.62  loss_ce_5: 2.96  loss_mask_5: 0.9023  loss_dice_5: 4.617  loss_ce_6: 3.283  loss_mask_6: 0.8317  loss_dice_6: 4.632  loss_ce_7: 3.321  loss_mask_7: 0.8936  loss_dice_7: 4.697  loss_ce_8: 3.021  loss_mask_8: 0.821  loss_dice_8: 4.734  loss_mars: 0.3806    time: 5.9927  last_time: 5.8895  data_time: 0.0026  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/17 08:42:46] d2.utils.events INFO:  eta: 1 day, 0:20:34  iter: 739  total_loss: 89.86  loss_ce: 3.275  loss_mask: 1.098  loss_dice: 4.5  loss_ce_0: 6.239  loss_mask_0: 0.8974  loss_dice_0: 4.378  loss_ce_1: 2.768  loss_mask_1: 0.9269  loss_dice_1: 4.387  loss_ce_2: 2.712  loss_mask_2: 0.9843  loss_dice_2: 4.312  loss_ce_3: 2.844  loss_mask_3: 1.06  loss_dice_3: 4.451  loss_ce_4: 2.837  loss_mask_4: 1.052  loss_dice_4: 4.472  loss_ce_5: 2.949  loss_mask_5: 1.11  loss_dice_5: 4.53  loss_ce_6: 2.81  loss_mask_6: 1.076  loss_dice_6: 4.503  loss_ce_7: 3.122  loss_mask_7: 1.05  loss_dice_7: 4.548  loss_ce_8: 2.987  loss_mask_8: 1.083  loss_dice_8: 4.546  loss_mars: 0.6938    time: 5.9891  last_time: 5.3300  data_time: 0.0038  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/17 08:44:42] d2.utils.events INFO:  eta: 1 day, 0:18:36  iter: 759  total_loss: 92.79  loss_ce: 3.209  loss_mask: 1.266  loss_dice: 4.266  loss_ce_0: 6.23  loss_mask_0: 1.294  loss_dice_0: 4.104  loss_ce_1: 2.931  loss_mask_1: 1.251  loss_dice_1: 4.029  loss_ce_2: 2.76  loss_mask_2: 1.264  loss_dice_2: 4.121  loss_ce_3: 2.916  loss_mask_3: 1.327  loss_dice_3: 4.085  loss_ce_4: 2.912  loss_mask_4: 1.352  loss_dice_4: 4.099  loss_ce_5: 2.833  loss_mask_5: 1.252  loss_dice_5: 4.064  loss_ce_6: 2.82  loss_mask_6: 1.286  loss_dice_6: 4.08  loss_ce_7: 2.94  loss_mask_7: 1.243  loss_dice_7: 4.181  loss_ce_8: 2.921  loss_mask_8: 1.21  loss_dice_8: 4.188  loss_mars: 0.3719    time: 5.9842  last_time: 6.0741  data_time: 0.0030  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 08:46:37] d2.utils.events INFO:  eta: 1 day, 0:17:02  iter: 779  total_loss: 90.67  loss_ce: 2.967  loss_mask: 1.338  loss_dice: 4.166  loss_ce_0: 6.207  loss_mask_0: 1.379  loss_dice_0: 4.181  loss_ce_1: 2.878  loss_mask_1: 1.287  loss_dice_1: 4.079  loss_ce_2: 2.697  loss_mask_2: 1.243  loss_dice_2: 4.08  loss_ce_3: 2.696  loss_mask_3: 1.446  loss_dice_3: 4.248  loss_ce_4: 2.615  loss_mask_4: 1.54  loss_dice_4: 4.268  loss_ce_5: 2.697  loss_mask_5: 1.451  loss_dice_5: 4.26  loss_ce_6: 2.654  loss_mask_6: 1.472  loss_dice_6: 4.218  loss_ce_7: 2.682  loss_mask_7: 1.449  loss_dice_7: 4.158  loss_ce_8: 2.662  loss_mask_8: 1.474  loss_dice_8: 4.211  loss_mars: 0.298    time: 5.9772  last_time: 5.9068  data_time: 0.0024  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 08:48:36] d2.utils.events INFO:  eta: 1 day, 0:15:04  iter: 799  total_loss: 90.44  loss_ce: 3.576  loss_mask: 0.8344  loss_dice: 4.491  loss_ce_0: 6.192  loss_mask_0: 0.7861  loss_dice_0: 4.373  loss_ce_1: 3.009  loss_mask_1: 0.6892  loss_dice_1: 4.298  loss_ce_2: 2.956  loss_mask_2: 0.8035  loss_dice_2: 4.275  loss_ce_3: 3.27  loss_mask_3: 0.7734  loss_dice_3: 4.434  loss_ce_4: 3.428  loss_mask_4: 0.6792  loss_dice_4: 4.398  loss_ce_5: 3.527  loss_mask_5: 0.822  loss_dice_5: 4.443  loss_ce_6: 3.288  loss_mask_6: 0.7483  loss_dice_6: 4.433  loss_ce_7: 3.196  loss_mask_7: 0.7148  loss_dice_7: 4.478  loss_ce_8: 3.104  loss_mask_8: 0.7985  loss_dice_8: 4.348  loss_mars: 0.4561    time: 5.9771  last_time: 5.3919  data_time: 0.0041  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/17 08:50:31] d2.utils.events INFO:  eta: 1 day, 0:12:02  iter: 819  total_loss: 99.15  loss_ce: 3.925  loss_mask: 1.606  loss_dice: 4.214  loss_ce_0: 6.378  loss_mask_0: 1.319  loss_dice_0: 4.171  loss_ce_1: 3.7  loss_mask_1: 1.333  loss_dice_1: 4.221  loss_ce_2: 3.633  loss_mask_2: 1.354  loss_dice_2: 4.108  loss_ce_3: 3.93  loss_mask_3: 1.474  loss_dice_3: 4.113  loss_ce_4: 4.181  loss_mask_4: 1.418  loss_dice_4: 4.075  loss_ce_5: 4.107  loss_mask_5: 1.413  loss_dice_5: 4.146  loss_ce_6: 4.109  loss_mask_6: 1.444  loss_dice_6: 4.35  loss_ce_7: 4.094  loss_mask_7: 1.457  loss_dice_7: 4.409  loss_ce_8: 3.878  loss_mask_8: 1.491  loss_dice_8: 4.387  loss_mars: 0.4869    time: 5.9718  last_time: 5.2726  data_time: 0.0031  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/17 08:52:27] d2.utils.events INFO:  eta: 1 day, 0:09:16  iter: 839  total_loss: 93.48  loss_ce: 4.203  loss_mask: 1.313  loss_dice: 4.448  loss_ce_0: 6.229  loss_mask_0: 1.309  loss_dice_0: 4.234  loss_ce_1: 3.868  loss_mask_1: 1.411  loss_dice_1: 4.198  loss_ce_2: 3.502  loss_mask_2: 1.387  loss_dice_2: 4.334  loss_ce_3: 3.701  loss_mask_3: 1.554  loss_dice_3: 4.301  loss_ce_4: 3.593  loss_mask_4: 1.418  loss_dice_4: 4.385  loss_ce_5: 3.529  loss_mask_5: 1.251  loss_dice_5: 4.46  loss_ce_6: 3.714  loss_mask_6: 1.338  loss_dice_6: 4.412  loss_ce_7: 3.787  loss_mask_7: 1.356  loss_dice_7: 4.418  loss_ce_8: 4.012  loss_mask_8: 1.201  loss_dice_8: 4.487  loss_mars: 0.3024    time: 5.9677  last_time: 6.8429  data_time: 0.0038  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/17 08:54:19] d2.utils.events INFO:  eta: 1 day, 0:06:46  iter: 859  total_loss: 84.17  loss_ce: 2.526  loss_mask: 1.427  loss_dice: 4.222  loss_ce_0: 5.516  loss_mask_0: 1.454  loss_dice_0: 4.128  loss_ce_1: 2.573  loss_mask_1: 1.562  loss_dice_1: 4.183  loss_ce_2: 2.644  loss_mask_2: 1.643  loss_dice_2: 4.202  loss_ce_3: 2.534  loss_mask_3: 1.448  loss_dice_3: 4.248  loss_ce_4: 2.352  loss_mask_4: 1.527  loss_dice_4: 4.225  loss_ce_5: 2.351  loss_mask_5: 1.369  loss_dice_5: 4.243  loss_ce_6: 2.582  loss_mask_6: 1.331  loss_dice_6: 4.186  loss_ce_7: 2.449  loss_mask_7: 1.352  loss_dice_7: 4.269  loss_ce_8: 2.387  loss_mask_8: 1.483  loss_dice_8: 4.255  loss_mars: 0.2873    time: 5.9583  last_time: 6.0983  data_time: 0.0031  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 08:56:12] d2.utils.events INFO:  eta: 1 day, 0:03:22  iter: 879  total_loss: 87.74  loss_ce: 3.598  loss_mask: 1.107  loss_dice: 4.3  loss_ce_0: 6.001  loss_mask_0: 1.208  loss_dice_0: 4.273  loss_ce_1: 3.034  loss_mask_1: 1.226  loss_dice_1: 4.287  loss_ce_2: 2.901  loss_mask_2: 1.133  loss_dice_2: 4.405  loss_ce_3: 3.041  loss_mask_3: 1.228  loss_dice_3: 4.335  loss_ce_4: 3.135  loss_mask_4: 1.283  loss_dice_4: 4.282  loss_ce_5: 3.219  loss_mask_5: 1.273  loss_dice_5: 4.272  loss_ce_6: 3.389  loss_mask_6: 1.299  loss_dice_6: 4.265  loss_ce_7: 3.539  loss_mask_7: 1.302  loss_dice_7: 4.26  loss_ce_8: 3.486  loss_mask_8: 1.288  loss_dice_8: 4.298  loss_mars: 0.5139    time: 5.9511  last_time: 5.5213  data_time: 0.0032  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 08:58:06] d2.utils.events INFO:  eta: 1 day, 0:01:25  iter: 899  total_loss: 99.87  loss_ce: 3.595  loss_mask: 0.8033  loss_dice: 4.665  loss_ce_0: 5.703  loss_mask_0: 0.9865  loss_dice_0: 4.48  loss_ce_1: 3.232  loss_mask_1: 0.9011  loss_dice_1: 4.571  loss_ce_2: 3.176  loss_mask_2: 0.8602  loss_dice_2: 4.545  loss_ce_3: 3.104  loss_mask_3: 0.9013  loss_dice_3: 4.493  loss_ce_4: 3.13  loss_mask_4: 0.8633  loss_dice_4: 4.445  loss_ce_5: 2.976  loss_mask_5: 0.7138  loss_dice_5: 4.598  loss_ce_6: 2.982  loss_mask_6: 0.739  loss_dice_6: 4.688  loss_ce_7: 3.138  loss_mask_7: 0.6961  loss_dice_7: 4.678  loss_ce_8: 3.159  loss_mask_8: 0.7003  loss_dice_8: 4.71  loss_mars: 0.4235    time: 5.9458  last_time: 5.3531  data_time: 0.0024  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/17 09:00:00] d2.utils.events INFO:  eta: 23:56:35  iter: 919  total_loss: 94.81  loss_ce: 4.099  loss_mask: 0.9926  loss_dice: 4.615  loss_ce_0: 5.702  loss_mask_0: 1.037  loss_dice_0: 4.467  loss_ce_1: 3.101  loss_mask_1: 1.091  loss_dice_1: 4.428  loss_ce_2: 3.119  loss_mask_2: 0.9707  loss_dice_2: 4.471  loss_ce_3: 3.139  loss_mask_3: 1.015  loss_dice_3: 4.417  loss_ce_4: 3.192  loss_mask_4: 0.9842  loss_dice_4: 4.485  loss_ce_5: 3.406  loss_mask_5: 1.002  loss_dice_5: 4.546  loss_ce_6: 3.336  loss_mask_6: 1.001  loss_dice_6: 4.535  loss_ce_7: 3.518  loss_mask_7: 0.992  loss_dice_7: 4.578  loss_ce_8: 3.738  loss_mask_8: 1.084  loss_dice_8: 4.554  loss_mars: 0.2519    time: 5.9406  last_time: 7.6286  data_time: 0.0034  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 09:01:49] d2.utils.events INFO:  eta: 23:53:31  iter: 939  total_loss: 88.37  loss_ce: 2.698  loss_mask: 1.464  loss_dice: 4.082  loss_ce_0: 5.147  loss_mask_0: 1.398  loss_dice_0: 3.73  loss_ce_1: 2.475  loss_mask_1: 1.503  loss_dice_1: 3.876  loss_ce_2: 2.408  loss_mask_2: 1.556  loss_dice_2: 3.872  loss_ce_3: 2.668  loss_mask_3: 1.371  loss_dice_3: 3.824  loss_ce_4: 2.706  loss_mask_4: 1.378  loss_dice_4: 3.8  loss_ce_5: 2.685  loss_mask_5: 1.491  loss_dice_5: 3.973  loss_ce_6: 2.757  loss_mask_6: 1.442  loss_dice_6: 4.083  loss_ce_7: 2.7  loss_mask_7: 1.502  loss_dice_7: 4.144  loss_ce_8: 2.754  loss_mask_8: 1.473  loss_dice_8: 4.137  loss_mars: 0.3682    time: 5.9301  last_time: 5.2271  data_time: 0.0027  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/17 09:03:40] d2.utils.events INFO:  eta: 23:51:56  iter: 959  total_loss: 91.26  loss_ce: 3.21  loss_mask: 1.165  loss_dice: 4.451  loss_ce_0: 5.454  loss_mask_0: 1.014  loss_dice_0: 4.382  loss_ce_1: 2.788  loss_mask_1: 1.25  loss_dice_1: 4.317  loss_ce_2: 2.678  loss_mask_2: 1.164  loss_dice_2: 4.195  loss_ce_3: 2.669  loss_mask_3: 1.03  loss_dice_3: 4.298  loss_ce_4: 2.847  loss_mask_4: 1.014  loss_dice_4: 4.285  loss_ce_5: 2.771  loss_mask_5: 1.116  loss_dice_5: 4.331  loss_ce_6: 2.935  loss_mask_6: 1.119  loss_dice_6: 4.312  loss_ce_7: 2.951  loss_mask_7: 1.142  loss_dice_7: 4.441  loss_ce_8: 3.035  loss_mask_8: 1.131  loss_dice_8: 4.511  loss_mars: 0.5807    time: 5.9221  last_time: 6.0043  data_time: 0.0031  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/17 09:05:33] d2.utils.events INFO:  eta: 23:47:57  iter: 979  total_loss: 91.77  loss_ce: 3.411  loss_mask: 1.158  loss_dice: 4.532  loss_ce_0: 5.413  loss_mask_0: 1.215  loss_dice_0: 4.197  loss_ce_1: 3.494  loss_mask_1: 1.258  loss_dice_1: 4.157  loss_ce_2: 3.204  loss_mask_2: 1.298  loss_dice_2: 4.205  loss_ce_3: 3.201  loss_mask_3: 1.281  loss_dice_3: 4.225  loss_ce_4: 3.172  loss_mask_4: 1.235  loss_dice_4: 4.061  loss_ce_5: 3.059  loss_mask_5: 1.19  loss_dice_5: 4.303  loss_ce_6: 3.012  loss_mask_6: 1.254  loss_dice_6: 4.348  loss_ce_7: 3.135  loss_mask_7: 1.136  loss_dice_7: 4.515  loss_ce_8: 3.335  loss_mask_8: 1.259  loss_dice_8: 4.53  loss_mars: 0.8048    time: 5.9164  last_time: 5.3095  data_time: 0.0026  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 09:07:28] d2.utils.events INFO:  eta: 23:43:51  iter: 999  total_loss: 88.03  loss_ce: 3.488  loss_mask: 0.951  loss_dice: 4.45  loss_ce_0: 5.221  loss_mask_0: 0.9916  loss_dice_0: 4.28  loss_ce_1: 2.811  loss_mask_1: 1.046  loss_dice_1: 4.15  loss_ce_2: 2.862  loss_mask_2: 0.9788  loss_dice_2: 4.352  loss_ce_3: 2.76  loss_mask_3: 1.022  loss_dice_3: 4.339  loss_ce_4: 2.782  loss_mask_4: 0.9457  loss_dice_4: 4.17  loss_ce_5: 3.016  loss_mask_5: 0.9235  loss_dice_5: 4.276  loss_ce_6: 3.131  loss_mask_6: 0.9212  loss_dice_6: 4.402  loss_ce_7: 2.958  loss_mask_7: 0.9297  loss_dice_7: 4.367  loss_ce_8: 3.303  loss_mask_8: 1.041  loss_dice_8: 4.529  loss_mars: 0.3962    time: 5.9134  last_time: 6.3591  data_time: 0.0026  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/17 09:09:21] d2.utils.events INFO:  eta: 23:35:31  iter: 1019  total_loss: 90.15  loss_ce: 3.289  loss_mask: 1.107  loss_dice: 3.963  loss_ce_0: 5.269  loss_mask_0: 1.139  loss_dice_0: 3.837  loss_ce_1: 3.195  loss_mask_1: 1.361  loss_dice_1: 3.865  loss_ce_2: 3.296  loss_mask_2: 1.403  loss_dice_2: 3.788  loss_ce_3: 3.49  loss_mask_3: 1.606  loss_dice_3: 3.91  loss_ce_4: 3.449  loss_mask_4: 1.346  loss_dice_4: 3.93  loss_ce_5: 3.481  loss_mask_5: 1.185  loss_dice_5: 3.966  loss_ce_6: 3.309  loss_mask_6: 1.114  loss_dice_6: 3.908  loss_ce_7: 3.111  loss_mask_7: 1.002  loss_dice_7: 3.929  loss_ce_8: 2.985  loss_mask_8: 1.16  loss_dice_8: 4.047  loss_mars: 0.6466    time: 5.9080  last_time: 6.0597  data_time: 0.0026  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 09:11:13] d2.utils.events INFO:  eta: 23:33:35  iter: 1039  total_loss: 81.28  loss_ce: 2.535  loss_mask: 1.068  loss_dice: 4.37  loss_ce_0: 4.757  loss_mask_0: 1.076  loss_dice_0: 4.018  loss_ce_1: 2.543  loss_mask_1: 1.062  loss_dice_1: 4.006  loss_ce_2: 2.449  loss_mask_2: 1.241  loss_dice_2: 4.099  loss_ce_3: 2.369  loss_mask_3: 1.161  loss_dice_3: 4.081  loss_ce_4: 2.231  loss_mask_4: 1.277  loss_dice_4: 4.2  loss_ce_5: 2.201  loss_mask_5: 1.242  loss_dice_5: 4.213  loss_ce_6: 2.232  loss_mask_6: 1.518  loss_dice_6: 4.373  loss_ce_7: 2.099  loss_mask_7: 1.255  loss_dice_7: 4.388  loss_ce_8: 2.216  loss_mask_8: 0.9567  loss_dice_8: 4.268  loss_mars: 0.7123    time: 5.9023  last_time: 6.1068  data_time: 0.0033  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 09:13:11] d2.utils.events INFO:  eta: 23:38:46  iter: 1059  total_loss: 88.14  loss_ce: 3.707  loss_mask: 1.308  loss_dice: 4.366  loss_ce_0: 5.022  loss_mask_0: 1.211  loss_dice_0: 4.061  loss_ce_1: 2.56  loss_mask_1: 1.329  loss_dice_1: 3.894  loss_ce_2: 2.688  loss_mask_2: 1.342  loss_dice_2: 3.865  loss_ce_3: 2.52  loss_mask_3: 1.382  loss_dice_3: 4.115  loss_ce_4: 2.626  loss_mask_4: 1.424  loss_dice_4: 4.2  loss_ce_5: 3.07  loss_mask_5: 1.215  loss_dice_5: 4.067  loss_ce_6: 3.021  loss_mask_6: 1.298  loss_dice_6: 4.114  loss_ce_7: 3.158  loss_mask_7: 1.166  loss_dice_7: 4.132  loss_ce_8: 3.225  loss_mask_8: 1.177  loss_dice_8: 4.243  loss_mars: 0.5809    time: 5.9021  last_time: 6.9981  data_time: 0.0028  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 09:15:06] d2.utils.events INFO:  eta: 23:38:12  iter: 1079  total_loss: 95.85  loss_ce: 3.815  loss_mask: 1.367  loss_dice: 4.187  loss_ce_0: 4.737  loss_mask_0: 1.202  loss_dice_0: 4.03  loss_ce_1: 2.775  loss_mask_1: 1.49  loss_dice_1: 4.091  loss_ce_2: 2.94  loss_mask_2: 1.255  loss_dice_2: 4.118  loss_ce_3: 3.006  loss_mask_3: 1.425  loss_dice_3: 4.065  loss_ce_4: 3.147  loss_mask_4: 1.172  loss_dice_4: 4.307  loss_ce_5: 3.212  loss_mask_5: 1.604  loss_dice_5: 4.305  loss_ce_6: 3.007  loss_mask_6: 1.603  loss_dice_6: 4.555  loss_ce_7: 3.209  loss_mask_7: 1.456  loss_dice_7: 4.3  loss_ce_8: 3.231  loss_mask_8: 1.438  loss_dice_8: 4.281  loss_mars: 0.4531    time: 5.8988  last_time: 5.3029  data_time: 0.0028  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 09:17:00] d2.utils.events INFO:  eta: 23:30:43  iter: 1099  total_loss: 95.3  loss_ce: 4.047  loss_mask: 0.8203  loss_dice: 4.448  loss_ce_0: 5.559  loss_mask_0: 0.8277  loss_dice_0: 4.236  loss_ce_1: 3.534  loss_mask_1: 0.9111  loss_dice_1: 4.285  loss_ce_2: 3.515  loss_mask_2: 0.9563  loss_dice_2: 4.171  loss_ce_3: 3.434  loss_mask_3: 1.013  loss_dice_3: 4.143  loss_ce_4: 3.458  loss_mask_4: 1.035  loss_dice_4: 4.125  loss_ce_5: 3.379  loss_mask_5: 0.9404  loss_dice_5: 4.281  loss_ce_6: 3.459  loss_mask_6: 0.9005  loss_dice_6: 4.209  loss_ce_7: 3.352  loss_mask_7: 1.052  loss_dice_7: 4.231  loss_ce_8: 3.542  loss_mask_8: 0.9839  loss_dice_8: 4.212  loss_mars: 0.3968    time: 5.8954  last_time: 6.0817  data_time: 0.0030  last_data_time: 0.0036   lr: 0.0001  max_mem: 0M
[10/17 09:19:00] d2.utils.events INFO:  eta: 23:21:49  iter: 1119  total_loss: 91.89  loss_ce: 3.616  loss_mask: 1.145  loss_dice: 4.391  loss_ce_0: 5.037  loss_mask_0: 1.153  loss_dice_0: 4.144  loss_ce_1: 3.365  loss_mask_1: 1.233  loss_dice_1: 4.103  loss_ce_2: 3.371  loss_mask_2: 1.133  loss_dice_2: 4.171  loss_ce_3: 3.402  loss_mask_3: 1.197  loss_dice_3: 4.141  loss_ce_4: 3.468  loss_mask_4: 1.144  loss_dice_4: 4.146  loss_ce_5: 3.484  loss_mask_5: 1.084  loss_dice_5: 4.158  loss_ce_6: 3.366  loss_mask_6: 1.173  loss_dice_6: 4.266  loss_ce_7: 3.418  loss_mask_7: 1.231  loss_dice_7: 4.303  loss_ce_8: 3.481  loss_mask_8: 1.209  loss_dice_8: 4.274  loss_mars: 0.4918    time: 5.8968  last_time: 5.8865  data_time: 0.0033  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 09:20:52] d2.utils.events INFO:  eta: 23:10:22  iter: 1139  total_loss: 88.49  loss_ce: 3.155  loss_mask: 1.211  loss_dice: 4.367  loss_ce_0: 4.736  loss_mask_0: 1.193  loss_dice_0: 4.402  loss_ce_1: 3.012  loss_mask_1: 1.298  loss_dice_1: 4.407  loss_ce_2: 2.837  loss_mask_2: 1.289  loss_dice_2: 4.48  loss_ce_3: 2.803  loss_mask_3: 1.25  loss_dice_3: 4.559  loss_ce_4: 2.675  loss_mask_4: 1.193  loss_dice_4: 4.563  loss_ce_5: 2.736  loss_mask_5: 0.9903  loss_dice_5: 4.525  loss_ce_6: 2.727  loss_mask_6: 1.041  loss_dice_6: 4.532  loss_ce_7: 2.588  loss_mask_7: 1.015  loss_dice_7: 4.645  loss_ce_8: 2.645  loss_mask_8: 1.049  loss_dice_8: 4.533  loss_mars: 0.262    time: 5.8921  last_time: 5.2935  data_time: 0.0040  last_data_time: 0.0084   lr: 0.0001  max_mem: 0M
[10/17 09:22:46] d2.utils.events INFO:  eta: 23:02:31  iter: 1159  total_loss: 95.77  loss_ce: 4.571  loss_mask: 1.002  loss_dice: 4.418  loss_ce_0: 5.41  loss_mask_0: 0.8538  loss_dice_0: 4.069  loss_ce_1: 3.596  loss_mask_1: 0.957  loss_dice_1: 3.985  loss_ce_2: 3.474  loss_mask_2: 0.9759  loss_dice_2: 4.03  loss_ce_3: 3.665  loss_mask_3: 1.02  loss_dice_3: 4.023  loss_ce_4: 3.916  loss_mask_4: 1.013  loss_dice_4: 4.088  loss_ce_5: 4.098  loss_mask_5: 0.9705  loss_dice_5: 4.123  loss_ce_6: 3.876  loss_mask_6: 0.9569  loss_dice_6: 4.156  loss_ce_7: 4.065  loss_mask_7: 0.9139  loss_dice_7: 4.25  loss_ce_8: 4.147  loss_mask_8: 1.002  loss_dice_8: 4.231  loss_mars: 0.6835    time: 5.8883  last_time: 6.0170  data_time: 0.0026  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/17 09:24:40] d2.utils.events INFO:  eta: 22:56:01  iter: 1179  total_loss: 88.22  loss_ce: 3.204  loss_mask: 1.014  loss_dice: 4.322  loss_ce_0: 4.835  loss_mask_0: 0.8829  loss_dice_0: 4.108  loss_ce_1: 3.312  loss_mask_1: 1.016  loss_dice_1: 4.097  loss_ce_2: 2.95  loss_mask_2: 0.9481  loss_dice_2: 4.123  loss_ce_3: 3.021  loss_mask_3: 1.034  loss_dice_3: 4.099  loss_ce_4: 2.946  loss_mask_4: 1.02  loss_dice_4: 4.059  loss_ce_5: 3.041  loss_mask_5: 1.033  loss_dice_5: 4.004  loss_ce_6: 2.914  loss_mask_6: 0.994  loss_dice_6: 4.175  loss_ce_7: 2.806  loss_mask_7: 1.097  loss_dice_7: 4.169  loss_ce_8: 2.916  loss_mask_8: 0.9488  loss_dice_8: 4.106  loss_mars: 0.7065    time: 5.8858  last_time: 5.1767  data_time: 0.0032  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 09:26:29] d2.utils.events INFO:  eta: 22:45:15  iter: 1199  total_loss: 85.27  loss_ce: 3.652  loss_mask: 1.351  loss_dice: 4.407  loss_ce_0: 4.478  loss_mask_0: 1.195  loss_dice_0: 4.07  loss_ce_1: 2.181  loss_mask_1: 1.304  loss_dice_1: 4.126  loss_ce_2: 2.317  loss_mask_2: 1.299  loss_dice_2: 4.222  loss_ce_3: 2.263  loss_mask_3: 1.461  loss_dice_3: 4.094  loss_ce_4: 2.338  loss_mask_4: 1.505  loss_dice_4: 4.186  loss_ce_5: 2.563  loss_mask_5: 1.358  loss_dice_5: 4.312  loss_ce_6: 2.418  loss_mask_6: 1.283  loss_dice_6: 4.282  loss_ce_7: 2.785  loss_mask_7: 1.226  loss_dice_7: 4.341  loss_ce_8: 2.977  loss_mask_8: 1.292  loss_dice_8: 4.383  loss_mars: 0.7025    time: 5.8781  last_time: 5.4985  data_time: 0.0034  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 09:28:21] d2.utils.events INFO:  eta: 22:44:54  iter: 1219  total_loss: 78.51  loss_ce: 2.598  loss_mask: 1.4  loss_dice: 3.693  loss_ce_0: 4.109  loss_mask_0: 1.48  loss_dice_0: 3.513  loss_ce_1: 2.502  loss_mask_1: 1.512  loss_dice_1: 3.453  loss_ce_2: 2.38  loss_mask_2: 1.372  loss_dice_2: 3.47  loss_ce_3: 2.615  loss_mask_3: 1.44  loss_dice_3: 3.403  loss_ce_4: 2.468  loss_mask_4: 1.422  loss_dice_4: 3.436  loss_ce_5: 2.607  loss_mask_5: 1.446  loss_dice_5: 3.651  loss_ce_6: 2.714  loss_mask_6: 1.444  loss_dice_6: 3.612  loss_ce_7: 2.745  loss_mask_7: 1.449  loss_dice_7: 3.638  loss_ce_8: 2.701  loss_mask_8: 1.455  loss_dice_8: 3.659  loss_mars: 0.8827    time: 5.8740  last_time: 5.9750  data_time: 0.0026  last_data_time: 0.0037   lr: 0.0001  max_mem: 0M
[10/17 09:30:12] d2.utils.events INFO:  eta: 22:38:39  iter: 1239  total_loss: 88.52  loss_ce: 3.632  loss_mask: 1.608  loss_dice: 4.22  loss_ce_0: 4.556  loss_mask_0: 1.449  loss_dice_0: 3.86  loss_ce_1: 3.038  loss_mask_1: 1.455  loss_dice_1: 3.964  loss_ce_2: 3.114  loss_mask_2: 1.57  loss_dice_2: 4.066  loss_ce_3: 2.754  loss_mask_3: 1.533  loss_dice_3: 4.267  loss_ce_4: 2.886  loss_mask_4: 1.704  loss_dice_4: 4.181  loss_ce_5: 2.93  loss_mask_5: 1.574  loss_dice_5: 4.198  loss_ce_6: 2.918  loss_mask_6: 1.707  loss_dice_6: 4.221  loss_ce_7: 2.911  loss_mask_7: 1.683  loss_dice_7: 4.159  loss_ce_8: 3.232  loss_mask_8: 2  loss_dice_8: 4.386  loss_mars: 0.594    time: 5.8685  last_time: 5.1795  data_time: 0.0029  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 09:31:58] d2.utils.events INFO:  eta: 22:31:06  iter: 1259  total_loss: 85.33  loss_ce: 3.406  loss_mask: 1.087  loss_dice: 4.293  loss_ce_0: 4.523  loss_mask_0: 1.095  loss_dice_0: 4.038  loss_ce_1: 2.635  loss_mask_1: 1.036  loss_dice_1: 4.01  loss_ce_2: 2.629  loss_mask_2: 1.178  loss_dice_2: 4.057  loss_ce_3: 2.556  loss_mask_3: 1.186  loss_dice_3: 4.14  loss_ce_4: 2.46  loss_mask_4: 1.217  loss_dice_4: 4.086  loss_ce_5: 2.551  loss_mask_5: 1.17  loss_dice_5: 4.144  loss_ce_6: 2.592  loss_mask_6: 1.214  loss_dice_6: 4.252  loss_ce_7: 2.592  loss_mask_7: 1.251  loss_dice_7: 4.126  loss_ce_8: 2.9  loss_mask_8: 1.149  loss_dice_8: 4.147  loss_mars: 0.6223    time: 5.8591  last_time: 6.0029  data_time: 0.0021  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 09:33:52] d2.utils.events INFO:  eta: 22:30:10  iter: 1279  total_loss: 85.7  loss_ce: 2.894  loss_mask: 0.7822  loss_dice: 4.374  loss_ce_0: 4.277  loss_mask_0: 0.7197  loss_dice_0: 4.252  loss_ce_1: 2.701  loss_mask_1: 0.7547  loss_dice_1: 4.22  loss_ce_2: 2.642  loss_mask_2: 0.7755  loss_dice_2: 4.291  loss_ce_3: 2.59  loss_mask_3: 0.8248  loss_dice_3: 4.373  loss_ce_4: 2.657  loss_mask_4: 0.7354  loss_dice_4: 4.432  loss_ce_5: 2.715  loss_mask_5: 0.864  loss_dice_5: 4.267  loss_ce_6: 2.807  loss_mask_6: 0.8282  loss_dice_6: 4.289  loss_ce_7: 2.706  loss_mask_7: 0.8143  loss_dice_7: 4.305  loss_ce_8: 2.723  loss_mask_8: 0.8003  loss_dice_8: 4.317  loss_mars: 0.6007    time: 5.8569  last_time: 5.0417  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 09:35:50] d2.utils.events INFO:  eta: 22:32:28  iter: 1299  total_loss: 94.34  loss_ce: 3.889  loss_mask: 1.032  loss_dice: 4.461  loss_ce_0: 4.535  loss_mask_0: 0.9949  loss_dice_0: 4.241  loss_ce_1: 3.487  loss_mask_1: 0.9817  loss_dice_1: 4.33  loss_ce_2: 3.552  loss_mask_2: 1.015  loss_dice_2: 4.35  loss_ce_3: 3.54  loss_mask_3: 0.9829  loss_dice_3: 4.335  loss_ce_4: 3.286  loss_mask_4: 0.9826  loss_dice_4: 4.386  loss_ce_5: 3.639  loss_mask_5: 0.9988  loss_dice_5: 4.351  loss_ce_6: 3.694  loss_mask_6: 1.037  loss_dice_6: 4.43  loss_ce_7: 3.815  loss_mask_7: 1.086  loss_dice_7: 4.493  loss_ce_8: 3.616  loss_mask_8: 1.07  loss_dice_8: 4.475  loss_mars: 0.7578    time: 5.8571  last_time: 5.0047  data_time: 0.0023  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/17 09:37:44] d2.utils.events INFO:  eta: 22:30:35  iter: 1319  total_loss: 102.8  loss_ce: 4.954  loss_mask: 0.9236  loss_dice: 4.368  loss_ce_0: 5.617  loss_mask_0: 0.9548  loss_dice_0: 4.27  loss_ce_1: 4.235  loss_mask_1: 1.037  loss_dice_1: 4.177  loss_ce_2: 4.186  loss_mask_2: 0.9644  loss_dice_2: 4.248  loss_ce_3: 4.575  loss_mask_3: 1.027  loss_dice_3: 4.338  loss_ce_4: 5.055  loss_mask_4: 1.029  loss_dice_4: 4.357  loss_ce_5: 5.003  loss_mask_5: 0.9652  loss_dice_5: 4.31  loss_ce_6: 5.095  loss_mask_6: 0.9729  loss_dice_6: 4.388  loss_ce_7: 4.964  loss_mask_7: 0.9344  loss_dice_7: 4.407  loss_ce_8: 4.849  loss_mask_8: 0.9261  loss_dice_8: 4.433  loss_mars: 0.8787    time: 5.8547  last_time: 4.8702  data_time: 0.0031  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/17 09:39:38] d2.utils.events INFO:  eta: 22:30:52  iter: 1339  total_loss: 105.1  loss_ce: 4.824  loss_mask: 1.205  loss_dice: 4.445  loss_ce_0: 5.199  loss_mask_0: 1.312  loss_dice_0: 4.148  loss_ce_1: 4.302  loss_mask_1: 1.348  loss_dice_1: 4.175  loss_ce_2: 4.299  loss_mask_2: 1.325  loss_dice_2: 4.213  loss_ce_3: 4.368  loss_mask_3: 1.424  loss_dice_3: 4.431  loss_ce_4: 4.299  loss_mask_4: 1.332  loss_dice_4: 4.384  loss_ce_5: 4.713  loss_mask_5: 1.318  loss_dice_5: 4.399  loss_ce_6: 4.419  loss_mask_6: 1.363  loss_dice_6: 4.362  loss_ce_7: 4.527  loss_mask_7: 1.33  loss_dice_7: 4.388  loss_ce_8: 4.564  loss_mask_8: 1.247  loss_dice_8: 4.471  loss_mars: 0.6023    time: 5.8530  last_time: 5.8079  data_time: 0.0037  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 09:41:36] d2.utils.events INFO:  eta: 22:30:06  iter: 1359  total_loss: 91.52  loss_ce: 3.86  loss_mask: 0.6572  loss_dice: 4.503  loss_ce_0: 5.789  loss_mask_0: 0.6343  loss_dice_0: 4.43  loss_ce_1: 3.77  loss_mask_1: 0.655  loss_dice_1: 4.456  loss_ce_2: 3.61  loss_mask_2: 0.6425  loss_dice_2: 4.532  loss_ce_3: 3.498  loss_mask_3: 0.7287  loss_dice_3: 4.515  loss_ce_4: 3.416  loss_mask_4: 0.7328  loss_dice_4: 4.551  loss_ce_5: 3.417  loss_mask_5: 0.7525  loss_dice_5: 4.571  loss_ce_6: 3.746  loss_mask_6: 0.7268  loss_dice_6: 4.569  loss_ce_7: 3.6  loss_mask_7: 0.7789  loss_dice_7: 4.595  loss_ce_8: 3.447  loss_mask_8: 0.6669  loss_dice_8: 4.558  loss_mars: 0.5137    time: 5.8533  last_time: 6.7072  data_time: 0.0036  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 09:43:34] d2.utils.events INFO:  eta: 22:28:13  iter: 1379  total_loss: 80.81  loss_ce: 2.465  loss_mask: 1.338  loss_dice: 4.063  loss_ce_0: 4.106  loss_mask_0: 0.9439  loss_dice_0: 4.036  loss_ce_1: 2.415  loss_mask_1: 0.9822  loss_dice_1: 4.041  loss_ce_2: 2.277  loss_mask_2: 1.354  loss_dice_2: 4.046  loss_ce_3: 2.321  loss_mask_3: 1.136  loss_dice_3: 3.977  loss_ce_4: 2.305  loss_mask_4: 1.173  loss_dice_4: 4.015  loss_ce_5: 2.144  loss_mask_5: 1.053  loss_dice_5: 3.983  loss_ce_6: 2.191  loss_mask_6: 1.221  loss_dice_6: 4.044  loss_ce_7: 2.283  loss_mask_7: 1.194  loss_dice_7: 4.164  loss_ce_8: 2.279  loss_mask_8: 1.109  loss_dice_8: 4.153  loss_mars: 0.6085    time: 5.8541  last_time: 5.0912  data_time: 0.0025  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 09:45:26] d2.utils.events INFO:  eta: 22:31:10  iter: 1399  total_loss: 87.16  loss_ce: 3.335  loss_mask: 1.193  loss_dice: 4.277  loss_ce_0: 4.425  loss_mask_0: 1.226  loss_dice_0: 4.233  loss_ce_1: 3.113  loss_mask_1: 1.21  loss_dice_1: 4.11  loss_ce_2: 3.046  loss_mask_2: 1.241  loss_dice_2: 4.178  loss_ce_3: 2.923  loss_mask_3: 1.274  loss_dice_3: 4.128  loss_ce_4: 3.042  loss_mask_4: 1.191  loss_dice_4: 4.102  loss_ce_5: 3.002  loss_mask_5: 1.23  loss_dice_5: 4.11  loss_ce_6: 3.027  loss_mask_6: 1.262  loss_dice_6: 4.204  loss_ce_7: 3.245  loss_mask_7: 1.213  loss_dice_7: 4.123  loss_ce_8: 3.251  loss_mask_8: 1.141  loss_dice_8: 4.188  loss_mars: 0.6709    time: 5.8505  last_time: 5.7821  data_time: 0.0029  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 09:47:19] d2.utils.events INFO:  eta: 22:29:16  iter: 1419  total_loss: 89.51  loss_ce: 3.424  loss_mask: 1.429  loss_dice: 4.042  loss_ce_0: 4.172  loss_mask_0: 1.402  loss_dice_0: 3.781  loss_ce_1: 3.144  loss_mask_1: 1.502  loss_dice_1: 3.809  loss_ce_2: 2.944  loss_mask_2: 1.515  loss_dice_2: 3.845  loss_ce_3: 3.062  loss_mask_3: 1.653  loss_dice_3: 3.821  loss_ce_4: 3.257  loss_mask_4: 1.561  loss_dice_4: 3.877  loss_ce_5: 3.241  loss_mask_5: 1.549  loss_dice_5: 4  loss_ce_6: 3.399  loss_mask_6: 1.704  loss_dice_6: 4.051  loss_ce_7: 3.593  loss_mask_7: 1.526  loss_dice_7: 4.064  loss_ce_8: 3.366  loss_mask_8: 1.705  loss_dice_8: 4.156  loss_mars: 0.8506    time: 5.8474  last_time: 6.4552  data_time: 0.0028  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 09:49:11] d2.utils.events INFO:  eta: 22:28:52  iter: 1439  total_loss: 86.15  loss_ce: 3.06  loss_mask: 1.329  loss_dice: 4.164  loss_ce_0: 3.918  loss_mask_0: 1.214  loss_dice_0: 4.123  loss_ce_1: 2.466  loss_mask_1: 1.502  loss_dice_1: 4.054  loss_ce_2: 2.503  loss_mask_2: 1.273  loss_dice_2: 4.251  loss_ce_3: 2.485  loss_mask_3: 1.334  loss_dice_3: 4.267  loss_ce_4: 2.366  loss_mask_4: 1.279  loss_dice_4: 4.304  loss_ce_5: 2.547  loss_mask_5: 1.412  loss_dice_5: 4.279  loss_ce_6: 2.812  loss_mask_6: 1.441  loss_dice_6: 4.211  loss_ce_7: 2.769  loss_mask_7: 1.318  loss_dice_7: 4.305  loss_ce_8: 2.753  loss_mask_8: 1.298  loss_dice_8: 4.354  loss_mars: 0.6425    time: 5.8442  last_time: 8.4865  data_time: 0.0035  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/17 09:50:59] d2.utils.events INFO:  eta: 22:24:26  iter: 1459  total_loss: 87.26  loss_ce: 3.354  loss_mask: 1.602  loss_dice: 4.072  loss_ce_0: 3.938  loss_mask_0: 1.525  loss_dice_0: 3.961  loss_ce_1: 2.706  loss_mask_1: 1.579  loss_dice_1: 3.954  loss_ce_2: 2.527  loss_mask_2: 1.622  loss_dice_2: 3.938  loss_ce_3: 2.62  loss_mask_3: 1.576  loss_dice_3: 3.945  loss_ce_4: 2.484  loss_mask_4: 1.58  loss_dice_4: 3.956  loss_ce_5: 2.488  loss_mask_5: 1.658  loss_dice_5: 4.04  loss_ce_6: 2.645  loss_mask_6: 1.73  loss_dice_6: 4.166  loss_ce_7: 2.774  loss_mask_7: 1.598  loss_dice_7: 4.196  loss_ce_8: 2.972  loss_mask_8: 1.738  loss_dice_8: 4.183  loss_mars: 0.3602    time: 5.8380  last_time: 5.8325  data_time: 0.0037  last_data_time: 0.0085   lr: 0.0001  max_mem: 0M
[10/17 09:52:55] d2.utils.events INFO:  eta: 22:25:04  iter: 1479  total_loss: 92.67  loss_ce: 3.651  loss_mask: 0.9927  loss_dice: 4.49  loss_ce_0: 4.447  loss_mask_0: 0.8644  loss_dice_0: 4.325  loss_ce_1: 3.661  loss_mask_1: 0.86  loss_dice_1: 4.221  loss_ce_2: 3.354  loss_mask_2: 0.81  loss_dice_2: 4.327  loss_ce_3: 3.721  loss_mask_3: 0.9105  loss_dice_3: 4.428  loss_ce_4: 3.567  loss_mask_4: 1.016  loss_dice_4: 4.447  loss_ce_5: 3.5  loss_mask_5: 0.8611  loss_dice_5: 4.451  loss_ce_6: 3.504  loss_mask_6: 0.8383  loss_dice_6: 4.52  loss_ce_7: 3.644  loss_mask_7: 0.9329  loss_dice_7: 4.643  loss_ce_8: 3.714  loss_mask_8: 0.9642  loss_dice_8: 4.644  loss_mars: 0.4064    time: 5.8375  last_time: 6.1468  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 09:54:53] d2.utils.events INFO:  eta: 22:22:21  iter: 1499  total_loss: 85.45  loss_ce: 3.029  loss_mask: 1.283  loss_dice: 3.917  loss_ce_0: 3.87  loss_mask_0: 1.392  loss_dice_0: 4.026  loss_ce_1: 2.486  loss_mask_1: 1.281  loss_dice_1: 3.848  loss_ce_2: 2.209  loss_mask_2: 1.185  loss_dice_2: 3.938  loss_ce_3: 2.256  loss_mask_3: 1.056  loss_dice_3: 4.103  loss_ce_4: 2.392  loss_mask_4: 1.11  loss_dice_4: 4.134  loss_ce_5: 2.431  loss_mask_5: 1.114  loss_dice_5: 4.108  loss_ce_6: 2.433  loss_mask_6: 1.063  loss_dice_6: 4.222  loss_ce_7: 2.551  loss_mask_7: 1.157  loss_dice_7: 4.041  loss_ce_8: 2.55  loss_mask_8: 1.18  loss_dice_8: 4.097  loss_mars: 0.7545    time: 5.8382  last_time: 5.9897  data_time: 0.0033  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 09:56:43] d2.utils.events INFO:  eta: 22:20:27  iter: 1519  total_loss: 84.43  loss_ce: 2.942  loss_mask: 1.286  loss_dice: 4.206  loss_ce_0: 4.318  loss_mask_0: 1.263  loss_dice_0: 4.099  loss_ce_1: 3.381  loss_mask_1: 1.238  loss_dice_1: 3.946  loss_ce_2: 3.035  loss_mask_2: 1.28  loss_dice_2: 4.011  loss_ce_3: 3.089  loss_mask_3: 1.395  loss_dice_3: 4.047  loss_ce_4: 3.084  loss_mask_4: 1.347  loss_dice_4: 4.042  loss_ce_5: 2.954  loss_mask_5: 1.34  loss_dice_5: 4.012  loss_ce_6: 2.994  loss_mask_6: 1.377  loss_dice_6: 4.012  loss_ce_7: 3.147  loss_mask_7: 1.262  loss_dice_7: 4.114  loss_ce_8: 3.029  loss_mask_8: 1.361  loss_dice_8: 4.209  loss_mars: 0.5576    time: 5.8337  last_time: 5.9798  data_time: 0.0028  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 09:58:36] d2.utils.events INFO:  eta: 22:19:21  iter: 1539  total_loss: 86.11  loss_ce: 2.546  loss_mask: 1.621  loss_dice: 3.65  loss_ce_0: 3.756  loss_mask_0: 1.696  loss_dice_0: 3.068  loss_ce_1: 2.381  loss_mask_1: 1.599  loss_dice_1: 3.157  loss_ce_2: 2.53  loss_mask_2: 1.729  loss_dice_2: 3.246  loss_ce_3: 2.402  loss_mask_3: 1.707  loss_dice_3: 3.222  loss_ce_4: 2.415  loss_mask_4: 1.677  loss_dice_4: 3.467  loss_ce_5: 2.358  loss_mask_5: 1.679  loss_dice_5: 3.586  loss_ce_6: 2.425  loss_mask_6: 1.619  loss_dice_6: 3.37  loss_ce_7: 2.539  loss_mask_7: 1.745  loss_dice_7: 3.536  loss_ce_8: 2.449  loss_mask_8: 1.676  loss_dice_8: 3.464  loss_mars: 0.4463    time: 5.8316  last_time: 5.8604  data_time: 0.0027  last_data_time: 0.0081   lr: 0.0001  max_mem: 0M
[10/17 10:00:32] d2.utils.events INFO:  eta: 22:18:31  iter: 1559  total_loss: 89.09  loss_ce: 3.693  loss_mask: 0.8603  loss_dice: 4.617  loss_ce_0: 4.862  loss_mask_0: 0.8989  loss_dice_0: 4.461  loss_ce_1: 3.403  loss_mask_1: 0.8801  loss_dice_1: 4.436  loss_ce_2: 3.379  loss_mask_2: 0.8173  loss_dice_2: 4.467  loss_ce_3: 3.477  loss_mask_3: 0.9767  loss_dice_3: 4.432  loss_ce_4: 3.558  loss_mask_4: 0.9989  loss_dice_4: 4.493  loss_ce_5: 3.696  loss_mask_5: 0.9123  loss_dice_5: 4.476  loss_ce_6: 3.431  loss_mask_6: 0.9399  loss_dice_6: 4.498  loss_ce_7: 3.511  loss_mask_7: 0.8642  loss_dice_7: 4.497  loss_ce_8: 3.635  loss_mask_8: 0.9279  loss_dice_8: 4.601  loss_mars: 0.5433    time: 5.8311  last_time: 5.3735  data_time: 0.0042  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/17 10:00:50] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0001562.pth
[10/17 10:00:50] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 2199         |   bicycle    | 78           |      car      | 345          |
|  motorcycle   | 61           |   airplane   | 26           |      bus      | 56           |
|     train     | 36           |    truck     | 63           |     boat      | 81           |
| traffic light | 69           | fire hydrant | 21           |   stop sign   | 14           |
| parking meter | 16           |    bench     | 82           |     bird      | 97           |
|      cat      | 48           |     dog      | 40           |     horse     | 45           |
|     sheep     | 42           |     cow      | 46           |   elephant    | 46           |
|     bear      | 14           |    zebra     | 69           |    giraffe    | 36           |
|   backpack    | 98           |   umbrella   | 39           |    handbag    | 93           |
|      tie      | 75           |   suitcase   | 57           |    frisbee    | 22           |
|     skis      | 44           |  snowboard   | 19           |  sports ball  | 45           |
|     kite      | 47           | baseball bat | 32           | baseball gl.. | 25           |
|  skateboard   | 32           |  surfboard   | 56           | tennis racket | 52           |
|    bottle     | 208          |  wine glass  | 65           |      cup      | 185          |
|     fork      | 43           |    knife     | 65           |     spoon     | 52           |
|     bowl      | 128          |    banana    | 90           |     apple     | 41           |
|   sandwich    | 33           |    orange    | 38           |   broccoli    | 96           |
|    carrot     | 64           |   hot dog    | 33           |     pizza     | 54           |
|     donut     | 51           |     cake     | 29           |     chair     | 391          |
|     couch     | 54           | potted plant | 64           |      bed      | 41           |
| dining table  | 152          |    toilet    | 36           |      tv       | 49           |
|    laptop     | 53           |    mouse     | 21           |    remote     | 57           |
|   keyboard    | 32           |  cell phone  | 58           |   microwave   | 8            |
|     oven      | 28           |   toaster    | 2            |     sink      | 47           |
| refrigerator  | 24           |     book     | 206          |     clock     | 47           |
|     vase      | 53           |   scissors   | 5            |  teddy bear   | 46           |
|  hair drier   | 0            |  toothbrush  | 2            |               |              |
|     total     | 7117         |              |              |               |              |[0m
[10/17 10:00:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/17 10:00:50] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/17 10:00:51] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/17 10:00:51] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/17 10:02:32] d2.utils.events INFO:  eta: 22:20:52  iter: 1579  total_loss: 96.73  loss_ce: 3.943  loss_mask: 1.423  loss_dice: 4.207  loss_ce_0: 4.632  loss_mask_0: 1.478  loss_dice_0: 3.862  loss_ce_1: 3.506  loss_mask_1: 1.455  loss_dice_1: 4.063  loss_ce_2: 3.51  loss_mask_2: 1.491  loss_dice_2: 4.056  loss_ce_3: 3.447  loss_mask_3: 1.607  loss_dice_3: 4.016  loss_ce_4: 3.514  loss_mask_4: 1.613  loss_dice_4: 3.943  loss_ce_5: 3.402  loss_mask_5: 1.646  loss_dice_5: 4.009  loss_ce_6: 3.297  loss_mask_6: 1.641  loss_dice_6: 4.146  loss_ce_7: 3.624  loss_mask_7: 1.751  loss_dice_7: 4.12  loss_ce_8: 3.559  loss_mask_8: 1.67  loss_dice_8: 4.143  loss_mars: 0.5956    time: 5.8325  last_time: 5.4468  data_time: 0.0035  last_data_time: 0.0084   lr: 0.0001  max_mem: 0M
[10/17 10:04:32] d2.utils.events INFO:  eta: 22:20:50  iter: 1599  total_loss: 91.99  loss_ce: 3.594  loss_mask: 1.59  loss_dice: 4.264  loss_ce_0: 3.985  loss_mask_0: 1.221  loss_dice_0: 4.058  loss_ce_1: 3.144  loss_mask_1: 1.199  loss_dice_1: 4.087  loss_ce_2: 2.809  loss_mask_2: 1.271  loss_dice_2: 4.114  loss_ce_3: 2.862  loss_mask_3: 1.35  loss_dice_3: 4.048  loss_ce_4: 3.055  loss_mask_4: 1.359  loss_dice_4: 4.068  loss_ce_5: 3.064  loss_mask_5: 1.454  loss_dice_5: 4.23  loss_ce_6: 3.045  loss_mask_6: 1.532  loss_dice_6: 4.254  loss_ce_7: 3.021  loss_mask_7: 1.389  loss_dice_7: 4.331  loss_ce_8: 3.527  loss_mask_8: 1.367  loss_dice_8: 4.396  loss_mars: 0.7557    time: 5.8346  last_time: 6.0082  data_time: 0.0032  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/17 10:06:31] d2.utils.events INFO:  eta: 22:18:55  iter: 1619  total_loss: 87.86  loss_ce: 2.992  loss_mask: 1.022  loss_dice: 4.452  loss_ce_0: 3.814  loss_mask_0: 0.93  loss_dice_0: 4.4  loss_ce_1: 2.614  loss_mask_1: 1.055  loss_dice_1: 4.418  loss_ce_2: 2.558  loss_mask_2: 0.9859  loss_dice_2: 4.315  loss_ce_3: 2.564  loss_mask_3: 0.9359  loss_dice_3: 4.349  loss_ce_4: 2.693  loss_mask_4: 0.926  loss_dice_4: 4.341  loss_ce_5: 2.859  loss_mask_5: 1.195  loss_dice_5: 4.399  loss_ce_6: 2.784  loss_mask_6: 1.01  loss_dice_6: 4.433  loss_ce_7: 2.913  loss_mask_7: 1.116  loss_dice_7: 4.455  loss_ce_8: 3.009  loss_mask_8: 1.078  loss_dice_8: 4.417  loss_mars: 0.3502    time: 5.8356  last_time: 6.2422  data_time: 0.0026  last_data_time: 0.0013   lr: 0.0001  max_mem: 0M
[10/17 10:08:27] d2.utils.events INFO:  eta: 22:19:18  iter: 1639  total_loss: 93.07  loss_ce: 4.286  loss_mask: 1.218  loss_dice: 4.02  loss_ce_0: 5.217  loss_mask_0: 1.185  loss_dice_0: 4.138  loss_ce_1: 3.956  loss_mask_1: 1.175  loss_dice_1: 4.003  loss_ce_2: 3.853  loss_mask_2: 1.327  loss_dice_2: 4.091  loss_ce_3: 3.875  loss_mask_3: 1.307  loss_dice_3: 4.163  loss_ce_4: 3.774  loss_mask_4: 1.265  loss_dice_4: 4.233  loss_ce_5: 3.825  loss_mask_5: 1.136  loss_dice_5: 4.322  loss_ce_6: 3.883  loss_mask_6: 1.268  loss_dice_6: 4.202  loss_ce_7: 3.913  loss_mask_7: 1.569  loss_dice_7: 4.018  loss_ce_8: 3.807  loss_mask_8: 1.488  loss_dice_8: 4.176  loss_mars: 0.2224    time: 5.8356  last_time: 5.8661  data_time: 0.0031  last_data_time: 0.0082   lr: 0.0001  max_mem: 0M
[10/17 10:10:24] d2.utils.events INFO:  eta: 22:18:33  iter: 1659  total_loss: 87.17  loss_ce: 2.716  loss_mask: 0.8716  loss_dice: 4.351  loss_ce_0: 3.829  loss_mask_0: 0.8448  loss_dice_0: 4.102  loss_ce_1: 2.835  loss_mask_1: 0.8464  loss_dice_1: 4.182  loss_ce_2: 2.969  loss_mask_2: 0.9322  loss_dice_2: 4.159  loss_ce_3: 2.901  loss_mask_3: 0.9597  loss_dice_3: 3.988  loss_ce_4: 2.789  loss_mask_4: 0.9551  loss_dice_4: 4.02  loss_ce_5: 2.797  loss_mask_5: 0.9553  loss_dice_5: 4.212  loss_ce_6: 2.824  loss_mask_6: 0.873  loss_dice_6: 4.295  loss_ce_7: 2.696  loss_mask_7: 0.9229  loss_dice_7: 4.24  loss_ce_8: 2.545  loss_mask_8: 1.067  loss_dice_8: 4.191  loss_mars: 0.5708    time: 5.8355  last_time: 4.1026  data_time: 0.0046  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 10:12:25] d2.utils.events INFO:  eta: 22:17:27  iter: 1679  total_loss: 87.62  loss_ce: 2.938  loss_mask: 0.9771  loss_dice: 4.487  loss_ce_0: 4.164  loss_mask_0: 0.8383  loss_dice_0: 4.417  loss_ce_1: 2.756  loss_mask_1: 0.9582  loss_dice_1: 4.353  loss_ce_2: 2.695  loss_mask_2: 0.9157  loss_dice_2: 4.391  loss_ce_3: 2.77  loss_mask_3: 0.907  loss_dice_3: 4.38  loss_ce_4: 2.988  loss_mask_4: 0.854  loss_dice_4: 4.451  loss_ce_5: 2.959  loss_mask_5: 0.8675  loss_dice_5: 4.467  loss_ce_6: 3.145  loss_mask_6: 0.899  loss_dice_6: 4.412  loss_ce_7: 2.969  loss_mask_7: 0.9546  loss_dice_7: 4.409  loss_ce_8: 2.875  loss_mask_8: 0.9211  loss_dice_8: 4.468  loss_mars: 0.5735    time: 5.8383  last_time: 6.7859  data_time: 0.0036  last_data_time: 0.0080   lr: 0.0001  max_mem: 0M
[10/17 10:14:22] d2.utils.events INFO:  eta: 22:16:16  iter: 1699  total_loss: 90.18  loss_ce: 3.179  loss_mask: 1.264  loss_dice: 4.434  loss_ce_0: 4.98  loss_mask_0: 0.8206  loss_dice_0: 4.175  loss_ce_1: 3.205  loss_mask_1: 0.9746  loss_dice_1: 4.362  loss_ce_2: 3.167  loss_mask_2: 1.094  loss_dice_2: 4.348  loss_ce_3: 3.257  loss_mask_3: 1.036  loss_dice_3: 4.366  loss_ce_4: 3.322  loss_mask_4: 0.8173  loss_dice_4: 4.36  loss_ce_5: 3.337  loss_mask_5: 0.9331  loss_dice_5: 4.417  loss_ce_6: 3.144  loss_mask_6: 1.112  loss_dice_6: 4.392  loss_ce_7: 3.076  loss_mask_7: 1.02  loss_dice_7: 4.419  loss_ce_8: 3.129  loss_mask_8: 0.9383  loss_dice_8: 4.298  loss_mars: 0.6528    time: 5.8384  last_time: 5.9097  data_time: 0.0035  last_data_time: 0.0043   lr: 0.0001  max_mem: 0M
[10/17 10:16:25] d2.utils.events INFO:  eta: 22:13:00  iter: 1719  total_loss: 86.81  loss_ce: 2.721  loss_mask: 0.9406  loss_dice: 3.962  loss_ce_0: 3.588  loss_mask_0: 0.9441  loss_dice_0: 3.911  loss_ce_1: 2.624  loss_mask_1: 0.9909  loss_dice_1: 3.989  loss_ce_2: 2.72  loss_mask_2: 0.897  loss_dice_2: 4.115  loss_ce_3: 2.762  loss_mask_3: 0.9556  loss_dice_3: 4.159  loss_ce_4: 2.904  loss_mask_4: 1.006  loss_dice_4: 4.279  loss_ce_5: 2.903  loss_mask_5: 1.036  loss_dice_5: 4.002  loss_ce_6: 2.913  loss_mask_6: 0.9278  loss_dice_6: 4.246  loss_ce_7: 2.881  loss_mask_7: 0.9488  loss_dice_7: 4.056  loss_ce_8: 2.875  loss_mask_8: 0.9781  loss_dice_8: 4.255  loss_mars: 0.469    time: 5.8419  last_time: 5.3118  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 10:18:27] d2.utils.events INFO:  eta: 22:15:39  iter: 1739  total_loss: 97.06  loss_ce: 4.018  loss_mask: 1.2  loss_dice: 4.12  loss_ce_0: 5.278  loss_mask_0: 1.028  loss_dice_0: 3.911  loss_ce_1: 3.864  loss_mask_1: 1.113  loss_dice_1: 4.001  loss_ce_2: 3.688  loss_mask_2: 1.134  loss_dice_2: 3.949  loss_ce_3: 3.764  loss_mask_3: 1.204  loss_dice_3: 3.952  loss_ce_4: 4.148  loss_mask_4: 1.19  loss_dice_4: 4.166  loss_ce_5: 4.031  loss_mask_5: 1.281  loss_dice_5: 4.079  loss_ce_6: 4.048  loss_mask_6: 1.245  loss_dice_6: 4.026  loss_ce_7: 3.84  loss_mask_7: 1.225  loss_dice_7: 4.282  loss_ce_8: 3.776  loss_mask_8: 1.25  loss_dice_8: 4.072  loss_mars: 0.7849    time: 5.8445  last_time: 6.8972  data_time: 0.0029  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 10:20:27] d2.utils.events INFO:  eta: 22:15:38  iter: 1759  total_loss: 94.73  loss_ce: 5.05  loss_mask: 0.6951  loss_dice: 4.483  loss_ce_0: 5.641  loss_mask_0: 0.657  loss_dice_0: 4.421  loss_ce_1: 4.071  loss_mask_1: 0.6591  loss_dice_1: 4.374  loss_ce_2: 3.836  loss_mask_2: 0.6833  loss_dice_2: 4.44  loss_ce_3: 4.121  loss_mask_3: 0.7506  loss_dice_3: 4.461  loss_ce_4: 4.531  loss_mask_4: 0.7106  loss_dice_4: 4.504  loss_ce_5: 4.136  loss_mask_5: 0.7047  loss_dice_5: 4.419  loss_ce_6: 4.264  loss_mask_6: 0.6752  loss_dice_6: 4.47  loss_ce_7: 4.511  loss_mask_7: 0.6748  loss_dice_7: 4.468  loss_ce_8: 4.59  loss_mask_8: 0.6269  loss_dice_8: 4.485  loss_mars: 0.7531    time: 5.8463  last_time: 5.7444  data_time: 0.0033  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 10:22:28] d2.utils.events INFO:  eta: 22:14:50  iter: 1779  total_loss: 88.06  loss_ce: 3.215  loss_mask: 0.7638  loss_dice: 4.59  loss_ce_0: 4.273  loss_mask_0: 0.8401  loss_dice_0: 4.453  loss_ce_1: 2.648  loss_mask_1: 0.961  loss_dice_1: 4.493  loss_ce_2: 2.679  loss_mask_2: 0.9047  loss_dice_2: 4.482  loss_ce_3: 3.118  loss_mask_3: 0.757  loss_dice_3: 4.527  loss_ce_4: 3.022  loss_mask_4: 0.8273  loss_dice_4: 4.419  loss_ce_5: 2.986  loss_mask_5: 0.9236  loss_dice_5: 4.488  loss_ce_6: 2.908  loss_mask_6: 0.8098  loss_dice_6: 4.61  loss_ce_7: 2.964  loss_mask_7: 0.8649  loss_dice_7: 4.532  loss_ce_8: 3.085  loss_mask_8: 0.8988  loss_dice_8: 4.538  loss_mars: 0.4365    time: 5.8486  last_time: 5.3474  data_time: 0.0039  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 10:24:20] d2.utils.events INFO:  eta: 22:09:53  iter: 1799  total_loss: 95.89  loss_ce: 4.291  loss_mask: 1.149  loss_dice: 4.458  loss_ce_0: 4.444  loss_mask_0: 1.016  loss_dice_0: 4.123  loss_ce_1: 3.653  loss_mask_1: 1.092  loss_dice_1: 4.102  loss_ce_2: 3.943  loss_mask_2: 1.043  loss_dice_2: 4.088  loss_ce_3: 3.692  loss_mask_3: 1.02  loss_dice_3: 4.109  loss_ce_4: 3.855  loss_mask_4: 1.174  loss_dice_4: 4.206  loss_ce_5: 3.84  loss_mask_5: 1.19  loss_dice_5: 4.215  loss_ce_6: 3.745  loss_mask_6: 1.276  loss_dice_6: 4.156  loss_ce_7: 3.72  loss_mask_7: 1.148  loss_dice_7: 4.146  loss_ce_8: 3.903  loss_mask_8: 1.155  loss_dice_8: 4.12  loss_mars: 0.8327    time: 5.8462  last_time: 6.1519  data_time: 0.0033  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 10:26:22] d2.utils.events INFO:  eta: 22:16:16  iter: 1819  total_loss: 85.43  loss_ce: 2.906  loss_mask: 1.311  loss_dice: 4.231  loss_ce_0: 3.554  loss_mask_0: 1.264  loss_dice_0: 4.184  loss_ce_1: 2.544  loss_mask_1: 1.308  loss_dice_1: 4.233  loss_ce_2: 2.543  loss_mask_2: 1.317  loss_dice_2: 4.273  loss_ce_3: 2.522  loss_mask_3: 1.314  loss_dice_3: 4.244  loss_ce_4: 2.78  loss_mask_4: 1.154  loss_dice_4: 4.264  loss_ce_5: 2.707  loss_mask_5: 1.274  loss_dice_5: 4.295  loss_ce_6: 2.718  loss_mask_6: 1.293  loss_dice_6: 4.36  loss_ce_7: 2.818  loss_mask_7: 1.382  loss_dice_7: 4.393  loss_ce_8: 2.72  loss_mask_8: 1.441  loss_dice_8: 4.314  loss_mars: 0.4788    time: 5.8486  last_time: 6.0388  data_time: 0.0030  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/17 10:28:12] d2.utils.events INFO:  eta: 22:11:20  iter: 1839  total_loss: 84.77  loss_ce: 2.648  loss_mask: 1.07  loss_dice: 4.221  loss_ce_0: 3.59  loss_mask_0: 0.9475  loss_dice_0: 4.115  loss_ce_1: 2.404  loss_mask_1: 1.106  loss_dice_1: 4.09  loss_ce_2: 2.646  loss_mask_2: 1.038  loss_dice_2: 4.112  loss_ce_3: 2.491  loss_mask_3: 1.067  loss_dice_3: 4.067  loss_ce_4: 2.662  loss_mask_4: 1.025  loss_dice_4: 4.079  loss_ce_5: 2.613  loss_mask_5: 1.085  loss_dice_5: 4.128  loss_ce_6: 2.57  loss_mask_6: 1.07  loss_dice_6: 4.147  loss_ce_7: 2.591  loss_mask_7: 1.156  loss_dice_7: 4.26  loss_ce_8: 2.607  loss_mask_8: 1.096  loss_dice_8: 4.398  loss_mars: 0.6901    time: 5.8448  last_time: 3.8854  data_time: 0.0033  last_data_time: 0.0089   lr: 0.0001  max_mem: 0M
[10/17 10:30:06] d2.utils.events INFO:  eta: 22:06:00  iter: 1859  total_loss: 91.21  loss_ce: 3.433  loss_mask: 1.194  loss_dice: 3.968  loss_ce_0: 3.661  loss_mask_0: 1.261  loss_dice_0: 3.918  loss_ce_1: 3.046  loss_mask_1: 1.143  loss_dice_1: 3.827  loss_ce_2: 3.327  loss_mask_2: 1.086  loss_dice_2: 3.984  loss_ce_3: 3.479  loss_mask_3: 1.169  loss_dice_3: 3.887  loss_ce_4: 3.824  loss_mask_4: 1.312  loss_dice_4: 3.853  loss_ce_5: 3.286  loss_mask_5: 1.213  loss_dice_5: 3.738  loss_ce_6: 3.328  loss_mask_6: 1.279  loss_dice_6: 3.852  loss_ce_7: 3.264  loss_mask_7: 1.392  loss_dice_7: 4.112  loss_ce_8: 3.474  loss_mask_8: 1.235  loss_dice_8: 3.862  loss_mars: 0.7337    time: 5.8435  last_time: 3.7624  data_time: 0.0038  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/17 10:32:12] d2.utils.events INFO:  eta: 22:11:13  iter: 1879  total_loss: 83.14  loss_ce: 2.536  loss_mask: 1.187  loss_dice: 4.273  loss_ce_0: 3.52  loss_mask_0: 1.176  loss_dice_0: 4.067  loss_ce_1: 2.41  loss_mask_1: 1.045  loss_dice_1: 4.206  loss_ce_2: 2.459  loss_mask_2: 1.352  loss_dice_2: 4.079  loss_ce_3: 2.406  loss_mask_3: 1.341  loss_dice_3: 4.258  loss_ce_4: 2.293  loss_mask_4: 1.478  loss_dice_4: 4.157  loss_ce_5: 2.229  loss_mask_5: 1.095  loss_dice_5: 4.255  loss_ce_6: 2.281  loss_mask_6: 1.032  loss_dice_6: 4.35  loss_ce_7: 2.497  loss_mask_7: 1.209  loss_dice_7: 4.098  loss_ce_8: 2.484  loss_mask_8: 1.071  loss_dice_8: 4.452  loss_mars: 0.343    time: 5.8480  last_time: 5.7518  data_time: 0.0030  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 10:34:11] d2.utils.events INFO:  eta: 22:10:33  iter: 1899  total_loss: 87.97  loss_ce: 3.513  loss_mask: 0.8938  loss_dice: 4.341  loss_ce_0: 4.55  loss_mask_0: 0.8465  loss_dice_0: 4.24  loss_ce_1: 3.72  loss_mask_1: 0.8701  loss_dice_1: 4.459  loss_ce_2: 3.468  loss_mask_2: 0.8842  loss_dice_2: 4.35  loss_ce_3: 3.257  loss_mask_3: 0.8724  loss_dice_3: 4.436  loss_ce_4: 3.34  loss_mask_4: 0.8566  loss_dice_4: 4.351  loss_ce_5: 3.332  loss_mask_5: 0.8584  loss_dice_5: 4.443  loss_ce_6: 3.407  loss_mask_6: 0.8409  loss_dice_6: 4.486  loss_ce_7: 3.32  loss_mask_7: 0.8713  loss_dice_7: 4.491  loss_ce_8: 3.261  loss_mask_8: 0.7903  loss_dice_8: 4.565  loss_mars: 0.5426    time: 5.8495  last_time: 5.7999  data_time: 0.0032  last_data_time: 0.0016   lr: 0.0001  max_mem: 0M
[10/17 10:36:55] d2.utils.events INFO:  eta: 22:09:33  iter: 1919  total_loss: 91.71  loss_ce: 4.189  loss_mask: 0.9617  loss_dice: 4.076  loss_ce_0: 4.095  loss_mask_0: 0.9125  loss_dice_0: 4.214  loss_ce_1: 3.638  loss_mask_1: 0.9803  loss_dice_1: 4.182  loss_ce_2: 3.431  loss_mask_2: 0.9523  loss_dice_2: 4.154  loss_ce_3: 3.413  loss_mask_3: 0.9061  loss_dice_3: 4.203  loss_ce_4: 3.547  loss_mask_4: 0.8907  loss_dice_4: 4.305  loss_ce_5: 3.602  loss_mask_5: 0.8938  loss_dice_5: 4.252  loss_ce_6: 3.52  loss_mask_6: 0.9172  loss_dice_6: 4.258  loss_ce_7: 3.951  loss_mask_7: 0.9419  loss_dice_7: 4.287  loss_ce_8: 4.036  loss_mask_8: 0.9566  loss_dice_8: 4.29  loss_mars: 0.6805    time: 5.8739  last_time: 7.7062  data_time: 0.0036  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/17 10:38:57] d2.utils.events INFO:  eta: 22:09:03  iter: 1939  total_loss: 86.49  loss_ce: 3.22  loss_mask: 1.176  loss_dice: 4.318  loss_ce_0: 3.999  loss_mask_0: 1.071  loss_dice_0: 4.174  loss_ce_1: 3.018  loss_mask_1: 1.204  loss_dice_1: 4.105  loss_ce_2: 2.892  loss_mask_2: 1.032  loss_dice_2: 4.307  loss_ce_3: 2.833  loss_mask_3: 1.136  loss_dice_3: 4.4  loss_ce_4: 2.85  loss_mask_4: 1.081  loss_dice_4: 4.325  loss_ce_5: 2.898  loss_mask_5: 1.061  loss_dice_5: 4.244  loss_ce_6: 3.032  loss_mask_6: 1.072  loss_dice_6: 4.348  loss_ce_7: 2.735  loss_mask_7: 1.07  loss_dice_7: 4.31  loss_ce_8: 2.979  loss_mask_8: 1.108  loss_dice_8: 4.468  loss_mars: 0.4332    time: 5.8764  last_time: 7.0796  data_time: 0.0041  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 10:41:12] d2.utils.events INFO:  eta: 22:07:24  iter: 1959  total_loss: 85.9  loss_ce: 3.196  loss_mask: 1.567  loss_dice: 3.714  loss_ce_0: 3.443  loss_mask_0: 1.376  loss_dice_0: 3.916  loss_ce_1: 2.663  loss_mask_1: 1.586  loss_dice_1: 3.842  loss_ce_2: 2.749  loss_mask_2: 1.689  loss_dice_2: 3.831  loss_ce_3: 2.865  loss_mask_3: 1.648  loss_dice_3: 3.841  loss_ce_4: 3.007  loss_mask_4: 1.681  loss_dice_4: 3.959  loss_ce_5: 2.685  loss_mask_5: 1.584  loss_dice_5: 3.959  loss_ce_6: 2.872  loss_mask_6: 1.624  loss_dice_6: 3.835  loss_ce_7: 2.947  loss_mask_7: 1.794  loss_dice_7: 4.199  loss_ce_8: 3.01  loss_mask_8: 1.464  loss_dice_8: 4.062  loss_mars: 0.4283    time: 5.8853  last_time: 6.9490  data_time: 0.0030  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/17 10:43:12] d2.utils.events INFO:  eta: 22:07:02  iter: 1979  total_loss: 83.1  loss_ce: 3.239  loss_mask: 1.2  loss_dice: 4.283  loss_ce_0: 3.993  loss_mask_0: 1.056  loss_dice_0: 4.024  loss_ce_1: 2.766  loss_mask_1: 1.075  loss_dice_1: 4.017  loss_ce_2: 2.669  loss_mask_2: 0.9602  loss_dice_2: 3.964  loss_ce_3: 2.957  loss_mask_3: 1.057  loss_dice_3: 3.97  loss_ce_4: 3.127  loss_mask_4: 0.9797  loss_dice_4: 4.122  loss_ce_5: 3.077  loss_mask_5: 1.044  loss_dice_5: 4.055  loss_ce_6: 3.034  loss_mask_6: 1.061  loss_dice_6: 3.995  loss_ce_7: 3.051  loss_mask_7: 1.043  loss_dice_7: 4.337  loss_ce_8: 3.058  loss_mask_8: 1.077  loss_dice_8: 4.147  loss_mars: 0.743    time: 5.8861  last_time: 5.6315  data_time: 0.0031  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/17 10:45:06] d2.utils.events INFO:  eta: 22:05:35  iter: 1999  total_loss: 83.2  loss_ce: 2.695  loss_mask: 1.18  loss_dice: 3.987  loss_ce_0: 3.255  loss_mask_0: 1.25  loss_dice_0: 4.001  loss_ce_1: 2.509  loss_mask_1: 1.234  loss_dice_1: 4.011  loss_ce_2: 2.509  loss_mask_2: 1.17  loss_dice_2: 4.071  loss_ce_3: 2.461  loss_mask_3: 1.231  loss_dice_3: 3.952  loss_ce_4: 2.753  loss_mask_4: 1.309  loss_dice_4: 4.009  loss_ce_5: 2.637  loss_mask_5: 1.33  loss_dice_5: 3.993  loss_ce_6: 2.651  loss_mask_6: 1.255  loss_dice_6: 3.97  loss_ce_7: 2.551  loss_mask_7: 1.271  loss_dice_7: 4.036  loss_ce_8: 2.817  loss_mask_8: 1.262  loss_dice_8: 3.932  loss_mars: 0.7527    time: 5.8841  last_time: 5.3816  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 10:47:05] d2.utils.events INFO:  eta: 22:04:15  iter: 2019  total_loss: 90.29  loss_ce: 3.266  loss_mask: 1.359  loss_dice: 4.236  loss_ce_0: 4.21  loss_mask_0: 1.216  loss_dice_0: 4.14  loss_ce_1: 2.961  loss_mask_1: 1.292  loss_dice_1: 4.043  loss_ce_2: 2.85  loss_mask_2: 1.376  loss_dice_2: 4.116  loss_ce_3: 2.93  loss_mask_3: 1.303  loss_dice_3: 4.246  loss_ce_4: 3.276  loss_mask_4: 1.291  loss_dice_4: 4.329  loss_ce_5: 3.297  loss_mask_5: 1.248  loss_dice_5: 4.174  loss_ce_6: 3.368  loss_mask_6: 1.333  loss_dice_6: 4.165  loss_ce_7: 3.724  loss_mask_7: 1.39  loss_dice_7: 4.173  loss_ce_8: 3.555  loss_mask_8: 1.46  loss_dice_8: 4.197  loss_mars: 0.792    time: 5.8849  last_time: 6.0814  data_time: 0.0029  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 10:48:59] d2.utils.events INFO:  eta: 22:01:12  iter: 2039  total_loss: 88.88  loss_ce: 3.167  loss_mask: 1.274  loss_dice: 3.928  loss_ce_0: 3.795  loss_mask_0: 1.208  loss_dice_0: 3.794  loss_ce_1: 3.011  loss_mask_1: 1.294  loss_dice_1: 3.909  loss_ce_2: 2.892  loss_mask_2: 1.358  loss_dice_2: 4.025  loss_ce_3: 2.838  loss_mask_3: 1.256  loss_dice_3: 3.891  loss_ce_4: 2.932  loss_mask_4: 1.279  loss_dice_4: 3.906  loss_ce_5: 3.008  loss_mask_5: 1.29  loss_dice_5: 3.818  loss_ce_6: 2.886  loss_mask_6: 1.307  loss_dice_6: 3.856  loss_ce_7: 3.126  loss_mask_7: 1.454  loss_dice_7: 3.91  loss_ce_8: 3.212  loss_mask_8: 1.268  loss_dice_8: 3.93  loss_mars: 0.7756    time: 5.8829  last_time: 5.9669  data_time: 0.0033  last_data_time: 0.0080   lr: 0.0001  max_mem: 0M
[10/17 10:50:51] d2.utils.events INFO:  eta: 21:57:41  iter: 2059  total_loss: 89.45  loss_ce: 2.839  loss_mask: 0.9693  loss_dice: 4.009  loss_ce_0: 4.847  loss_mask_0: 0.9795  loss_dice_0: 4.155  loss_ce_1: 3.305  loss_mask_1: 1.004  loss_dice_1: 4.039  loss_ce_2: 3.203  loss_mask_2: 0.9998  loss_dice_2: 4.147  loss_ce_3: 3.523  loss_mask_3: 0.9969  loss_dice_3: 4.098  loss_ce_4: 3.014  loss_mask_4: 0.9252  loss_dice_4: 4.126  loss_ce_5: 3.266  loss_mask_5: 0.9601  loss_dice_5: 4.038  loss_ce_6: 3.32  loss_mask_6: 1.176  loss_dice_6: 4.058  loss_ce_7: 3.038  loss_mask_7: 1.082  loss_dice_7: 4.192  loss_ce_8: 2.913  loss_mask_8: 1.118  loss_dice_8: 4.089  loss_mars: 0.7348    time: 5.8803  last_time: 6.1810  data_time: 0.0029  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/17 10:53:18] d2.utils.events INFO:  eta: 21:57:19  iter: 2079  total_loss: 86.59  loss_ce: 2.717  loss_mask: 0.7783  loss_dice: 4.442  loss_ce_0: 3.829  loss_mask_0: 0.7764  loss_dice_0: 4.19  loss_ce_1: 2.672  loss_mask_1: 0.8079  loss_dice_1: 4.035  loss_ce_2: 2.67  loss_mask_2: 0.7816  loss_dice_2: 4.321  loss_ce_3: 2.631  loss_mask_3: 0.8913  loss_dice_3: 4.219  loss_ce_4: 2.381  loss_mask_4: 0.8176  loss_dice_4: 4.234  loss_ce_5: 2.439  loss_mask_5: 0.8201  loss_dice_5: 4.144  loss_ce_6: 2.53  loss_mask_6: 0.7438  loss_dice_6: 4.248  loss_ce_7: 2.416  loss_mask_7: 0.7694  loss_dice_7: 4.179  loss_ce_8: 2.474  loss_mask_8: 0.8518  loss_dice_8: 3.989  loss_mars: 0.4867    time: 5.8944  last_time: 10.5033  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 10:55:15] d2.utils.events INFO:  eta: 21:56:28  iter: 2099  total_loss: 80.36  loss_ce: 2.508  loss_mask: 0.8486  loss_dice: 4.438  loss_ce_0: 3.374  loss_mask_0: 0.8573  loss_dice_0: 4.293  loss_ce_1: 2.328  loss_mask_1: 0.8216  loss_dice_1: 4.271  loss_ce_2: 2.33  loss_mask_2: 0.9091  loss_dice_2: 4.412  loss_ce_3: 2.338  loss_mask_3: 0.8502  loss_dice_3: 4.371  loss_ce_4: 2.401  loss_mask_4: 0.8133  loss_dice_4: 4.377  loss_ce_5: 2.392  loss_mask_5: 0.8377  loss_dice_5: 4.433  loss_ce_6: 2.367  loss_mask_6: 0.7978  loss_dice_6: 4.477  loss_ce_7: 2.222  loss_mask_7: 0.7212  loss_dice_7: 4.547  loss_ce_8: 2.344  loss_mask_8: 0.6928  loss_dice_8: 4.562  loss_mars: 0.5726    time: 5.8938  last_time: 6.1140  data_time: 0.0033  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 10:57:20] d2.utils.events INFO:  eta: 21:54:20  iter: 2119  total_loss: 87.29  loss_ce: 2.653  loss_mask: 1.942  loss_dice: 3.306  loss_ce_0: 3.225  loss_mask_0: 1.833  loss_dice_0: 3.496  loss_ce_1: 2.373  loss_mask_1: 1.835  loss_dice_1: 3.188  loss_ce_2: 2.376  loss_mask_2: 1.61  loss_dice_2: 3.358  loss_ce_3: 2.209  loss_mask_3: 1.681  loss_dice_3: 3.427  loss_ce_4: 2.437  loss_mask_4: 1.69  loss_dice_4: 3.462  loss_ce_5: 2.548  loss_mask_5: 1.818  loss_dice_5: 3.429  loss_ce_6: 2.554  loss_mask_6: 1.636  loss_dice_6: 3.563  loss_ce_7: 2.522  loss_mask_7: 1.707  loss_dice_7: 3.509  loss_ce_8: 2.472  loss_mask_8: 1.738  loss_dice_8: 3.551  loss_mars: 0.2905    time: 5.8976  last_time: 6.0811  data_time: 0.0028  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 10:59:22] d2.utils.events INFO:  eta: 21:53:08  iter: 2139  total_loss: 89.4  loss_ce: 2.713  loss_mask: 1.062  loss_dice: 4.117  loss_ce_0: 3.567  loss_mask_0: 0.8916  loss_dice_0: 4.072  loss_ce_1: 2.529  loss_mask_1: 0.9614  loss_dice_1: 4.227  loss_ce_2: 2.529  loss_mask_2: 0.9588  loss_dice_2: 4.123  loss_ce_3: 2.551  loss_mask_3: 0.91  loss_dice_3: 4.265  loss_ce_4: 2.833  loss_mask_4: 0.9464  loss_dice_4: 4.139  loss_ce_5: 2.741  loss_mask_5: 0.9569  loss_dice_5: 4.067  loss_ce_6: 2.656  loss_mask_6: 0.9929  loss_dice_6: 4.205  loss_ce_7: 2.586  loss_mask_7: 0.9482  loss_dice_7: 4.265  loss_ce_8: 2.734  loss_mask_8: 0.9353  loss_dice_8: 4.483  loss_mars: 0.7976    time: 5.8994  last_time: 5.6424  data_time: 0.0029  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/17 11:01:30] d2.utils.events INFO:  eta: 21:51:36  iter: 2159  total_loss: 93.91  loss_ce: 3.418  loss_mask: 1.166  loss_dice: 4.385  loss_ce_0: 4.043  loss_mask_0: 1.06  loss_dice_0: 4.17  loss_ce_1: 3.175  loss_mask_1: 1.161  loss_dice_1: 4.283  loss_ce_2: 3.013  loss_mask_2: 1.126  loss_dice_2: 4.326  loss_ce_3: 3.09  loss_mask_3: 1.136  loss_dice_3: 4.312  loss_ce_4: 3.38  loss_mask_4: 1.161  loss_dice_4: 4.26  loss_ce_5: 3.377  loss_mask_5: 1.264  loss_dice_5: 4.302  loss_ce_6: 3.359  loss_mask_6: 1.156  loss_dice_6: 4.401  loss_ce_7: 3.345  loss_mask_7: 1.042  loss_dice_7: 4.285  loss_ce_8: 3.363  loss_mask_8: 1.161  loss_dice_8: 4.409  loss_mars: 0.0001132    time: 5.9039  last_time: 10.0836  data_time: 0.0036  last_data_time: 0.0081   lr: 0.0001  max_mem: 0M
[10/17 11:04:19] d2.utils.events INFO:  eta: 21:52:28  iter: 2179  total_loss: 83.3  loss_ce: 3.094  loss_mask: 0.87  loss_dice: 4.181  loss_ce_0: 3.881  loss_mask_0: 0.7908  loss_dice_0: 4.035  loss_ce_1: 3.029  loss_mask_1: 0.8743  loss_dice_1: 4.184  loss_ce_2: 2.81  loss_mask_2: 0.8965  loss_dice_2: 4.012  loss_ce_3: 2.672  loss_mask_3: 0.9051  loss_dice_3: 4.198  loss_ce_4: 2.778  loss_mask_4: 0.8139  loss_dice_4: 4.187  loss_ce_5: 2.822  loss_mask_5: 0.751  loss_dice_5: 4.205  loss_ce_6: 2.967  loss_mask_6: 0.8304  loss_dice_6: 4.257  loss_ce_7: 2.986  loss_mask_7: 0.9753  loss_dice_7: 4.093  loss_ce_8: 3.176  loss_mask_8: 0.944  loss_dice_8: 4.303  loss_mars: 0.6205    time: 5.9275  last_time: 16.5513  data_time: 0.0031  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/17 11:07:12] d2.utils.events INFO:  eta: 21:53:49  iter: 2199  total_loss: 85.93  loss_ce: 3.48  loss_mask: 1.267  loss_dice: 4.373  loss_ce_0: 3.732  loss_mask_0: 1.099  loss_dice_0: 4.103  loss_ce_1: 3.112  loss_mask_1: 1.108  loss_dice_1: 4.112  loss_ce_2: 3.093  loss_mask_2: 1.334  loss_dice_2: 4.126  loss_ce_3: 3.018  loss_mask_3: 1.203  loss_dice_3: 4.207  loss_ce_4: 2.942  loss_mask_4: 1.218  loss_dice_4: 4.157  loss_ce_5: 2.951  loss_mask_5: 1.178  loss_dice_5: 4.134  loss_ce_6: 3.248  loss_mask_6: 1.333  loss_dice_6: 4.171  loss_ce_7: 3.335  loss_mask_7: 1.428  loss_dice_7: 4.255  loss_ce_8: 3.442  loss_mask_8: 1.392  loss_dice_8: 4.23  loss_mars: 0.797    time: 5.9521  last_time: 5.3766  data_time: 0.0031  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 11:09:07] d2.utils.events INFO:  eta: 21:50:07  iter: 2219  total_loss: 91.17  loss_ce: 3.425  loss_mask: 1.096  loss_dice: 4.152  loss_ce_0: 3.92  loss_mask_0: 0.8186  loss_dice_0: 4.173  loss_ce_1: 3.131  loss_mask_1: 0.8125  loss_dice_1: 4.056  loss_ce_2: 3.705  loss_mask_2: 1.239  loss_dice_2: 4.041  loss_ce_3: 3.835  loss_mask_3: 0.9945  loss_dice_3: 4.072  loss_ce_4: 3.699  loss_mask_4: 0.9941  loss_dice_4: 3.934  loss_ce_5: 3.662  loss_mask_5: 0.7414  loss_dice_5: 4.135  loss_ce_6: 3.482  loss_mask_6: 0.7522  loss_dice_6: 4.192  loss_ce_7: 3.533  loss_mask_7: 0.9291  loss_dice_7: 4.156  loss_ce_8: 3.491  loss_mask_8: 0.8282  loss_dice_8: 4.111  loss_mars: 0.8376    time: 5.9504  last_time: 3.7941  data_time: 0.0032  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/17 11:11:01] d2.utils.events INFO:  eta: 21:49:54  iter: 2239  total_loss: 88.17  loss_ce: 3.091  loss_mask: 1.57  loss_dice: 3.829  loss_ce_0: 3.581  loss_mask_0: 1.415  loss_dice_0: 3.873  loss_ce_1: 2.833  loss_mask_1: 1.569  loss_dice_1: 3.699  loss_ce_2: 2.839  loss_mask_2: 1.701  loss_dice_2: 3.829  loss_ce_3: 2.99  loss_mask_3: 1.795  loss_dice_3: 3.906  loss_ce_4: 3.09  loss_mask_4: 1.589  loss_dice_4: 3.75  loss_ce_5: 2.862  loss_mask_5: 1.622  loss_dice_5: 3.707  loss_ce_6: 2.891  loss_mask_6: 1.674  loss_dice_6: 3.801  loss_ce_7: 2.889  loss_mask_7: 1.543  loss_dice_7: 3.808  loss_ce_8: 2.893  loss_mask_8: 1.536  loss_dice_8: 3.741  loss_mars: 0.5785    time: 5.9477  last_time: 5.3191  data_time: 0.0029  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 11:12:55] d2.utils.events INFO:  eta: 21:50:11  iter: 2259  total_loss: 86.75  loss_ce: 2.5  loss_mask: 1.566  loss_dice: 3.908  loss_ce_0: 3.317  loss_mask_0: 1.365  loss_dice_0: 3.854  loss_ce_1: 2.556  loss_mask_1: 1.38  loss_dice_1: 3.735  loss_ce_2: 2.545  loss_mask_2: 1.495  loss_dice_2: 3.81  loss_ce_3: 2.617  loss_mask_3: 1.425  loss_dice_3: 3.901  loss_ce_4: 2.605  loss_mask_4: 1.469  loss_dice_4: 3.769  loss_ce_5: 2.576  loss_mask_5: 1.413  loss_dice_5: 3.838  loss_ce_6: 2.452  loss_mask_6: 1.376  loss_dice_6: 3.821  loss_ce_7: 2.475  loss_mask_7: 1.339  loss_dice_7: 3.836  loss_ce_8: 2.552  loss_mask_8: 1.393  loss_dice_8: 3.902  loss_mars: 0.6639    time: 5.9458  last_time: 5.4878  data_time: 0.0027  last_data_time: 0.0036   lr: 0.0001  max_mem: 0M
[10/17 11:14:53] d2.utils.events INFO:  eta: 21:47:59  iter: 2279  total_loss: 90.41  loss_ce: 3.445  loss_mask: 0.8442  loss_dice: 4.296  loss_ce_0: 3.793  loss_mask_0: 0.9208  loss_dice_0: 4.257  loss_ce_1: 3.296  loss_mask_1: 0.7873  loss_dice_1: 4.078  loss_ce_2: 3.428  loss_mask_2: 0.8278  loss_dice_2: 4.285  loss_ce_3: 3.298  loss_mask_3: 0.9568  loss_dice_3: 4.232  loss_ce_4: 3.408  loss_mask_4: 0.8926  loss_dice_4: 4.264  loss_ce_5: 3.283  loss_mask_5: 0.8584  loss_dice_5: 4.23  loss_ce_6: 3.562  loss_mask_6: 0.8992  loss_dice_6: 4.256  loss_ce_7: 3.314  loss_mask_7: 0.9011  loss_dice_7: 4.315  loss_ce_8: 3.383  loss_mask_8: 0.9374  loss_dice_8: 4.275  loss_mars: 0.7473    time: 5.9451  last_time: 5.6914  data_time: 0.0031  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 11:16:45] d2.utils.events INFO:  eta: 21:46:01  iter: 2299  total_loss: 90.88  loss_ce: 3.53  loss_mask: 1.018  loss_dice: 4.272  loss_ce_0: 4.35  loss_mask_0: 1.034  loss_dice_0: 4.225  loss_ce_1: 3.55  loss_mask_1: 1.018  loss_dice_1: 4.116  loss_ce_2: 3.426  loss_mask_2: 1.024  loss_dice_2: 4.194  loss_ce_3: 3.284  loss_mask_3: 0.9385  loss_dice_3: 4.219  loss_ce_4: 3.45  loss_mask_4: 1.007  loss_dice_4: 4.178  loss_ce_5: 3.666  loss_mask_5: 0.9755  loss_dice_5: 4.224  loss_ce_6: 3.579  loss_mask_6: 1.024  loss_dice_6: 4.256  loss_ce_7: 3.407  loss_mask_7: 1.079  loss_dice_7: 4.154  loss_ce_8: 3.624  loss_mask_8: 1.044  loss_dice_8: 4.296  loss_mars: 0.8078    time: 5.9424  last_time: 4.1318  data_time: 0.0031  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/17 11:18:38] d2.utils.events INFO:  eta: 21:43:00  iter: 2319  total_loss: 88.95  loss_ce: 3.555  loss_mask: 1.579  loss_dice: 3.847  loss_ce_0: 3.497  loss_mask_0: 1.294  loss_dice_0: 3.908  loss_ce_1: 3.217  loss_mask_1: 1.376  loss_dice_1: 3.869  loss_ce_2: 3.336  loss_mask_2: 1.469  loss_dice_2: 3.808  loss_ce_3: 3.343  loss_mask_3: 1.368  loss_dice_3: 3.799  loss_ce_4: 3.438  loss_mask_4: 1.588  loss_dice_4: 3.76  loss_ce_5: 3.472  loss_mask_5: 1.49  loss_dice_5: 3.744  loss_ce_6: 3.51  loss_mask_6: 1.357  loss_dice_6: 3.693  loss_ce_7: 3.507  loss_mask_7: 1.376  loss_dice_7: 3.759  loss_ce_8: 3.513  loss_mask_8: 1.47  loss_dice_8: 3.779  loss_mars: 0.854    time: 5.9397  last_time: 5.2060  data_time: 0.0029  last_data_time: 0.0049   lr: 0.0001  max_mem: 0M
[10/17 11:20:27] d2.utils.events INFO:  eta: 21:41:57  iter: 2339  total_loss: 80.97  loss_ce: 2.668  loss_mask: 1.19  loss_dice: 3.711  loss_ce_0: 2.922  loss_mask_0: 1.179  loss_dice_0: 3.952  loss_ce_1: 2.445  loss_mask_1: 1.143  loss_dice_1: 3.83  loss_ce_2: 2.57  loss_mask_2: 1.14  loss_dice_2: 3.849  loss_ce_3: 2.489  loss_mask_3: 1.171  loss_dice_3: 3.712  loss_ce_4: 2.576  loss_mask_4: 1.042  loss_dice_4: 3.757  loss_ce_5: 2.684  loss_mask_5: 1.061  loss_dice_5: 3.668  loss_ce_6: 2.787  loss_mask_6: 1.063  loss_dice_6: 3.705  loss_ce_7: 2.934  loss_mask_7: 1.076  loss_dice_7: 3.759  loss_ce_8: 2.759  loss_mask_8: 1.065  loss_dice_8: 3.819  loss_mars: 0.8957    time: 5.9356  last_time: 5.3050  data_time: 0.0028  last_data_time: 0.0088   lr: 0.0001  max_mem: 0M
[10/17 11:22:23] d2.utils.events INFO:  eta: 21:38:09  iter: 2359  total_loss: 103.7  loss_ce: 4.698  loss_mask: 1.062  loss_dice: 4.192  loss_ce_0: 5.076  loss_mask_0: 1.064  loss_dice_0: 4.26  loss_ce_1: 4.473  loss_mask_1: 1.043  loss_dice_1: 4.335  loss_ce_2: 4.534  loss_mask_2: 1.168  loss_dice_2: 4.262  loss_ce_3: 4.287  loss_mask_3: 1.2  loss_dice_3: 4.295  loss_ce_4: 4.359  loss_mask_4: 1.22  loss_dice_4: 4.431  loss_ce_5: 4.345  loss_mask_5: 1.195  loss_dice_5: 4.315  loss_ce_6: 4.423  loss_mask_6: 1.218  loss_dice_6: 4.322  loss_ce_7: 4.457  loss_mask_7: 1.132  loss_dice_7: 4.314  loss_ce_8: 4.541  loss_mask_8: 1.156  loss_dice_8: 4.304  loss_mars: 0.8059    time: 5.9343  last_time: 6.2927  data_time: 0.0037  last_data_time: 0.0080   lr: 0.0001  max_mem: 0M
[10/17 11:24:20] d2.utils.events INFO:  eta: 21:35:43  iter: 2379  total_loss: 85.13  loss_ce: 3.306  loss_mask: 1.14  loss_dice: 3.87  loss_ce_0: 3.581  loss_mask_0: 1.074  loss_dice_0: 3.747  loss_ce_1: 3.204  loss_mask_1: 1.056  loss_dice_1: 4.01  loss_ce_2: 3.102  loss_mask_2: 1.209  loss_dice_2: 3.947  loss_ce_3: 3.055  loss_mask_3: 1.178  loss_dice_3: 3.926  loss_ce_4: 3.025  loss_mask_4: 1.178  loss_dice_4: 3.965  loss_ce_5: 3.01  loss_mask_5: 1.208  loss_dice_5: 3.916  loss_ce_6: 3.042  loss_mask_6: 1.281  loss_dice_6: 3.782  loss_ce_7: 2.992  loss_mask_7: 1.366  loss_dice_7: 3.752  loss_ce_8: 3.138  loss_mask_8: 1.165  loss_dice_8: 4.003  loss_mars: 0.7645    time: 5.9337  last_time: 5.3563  data_time: 0.0027  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/17 11:26:28] d2.utils.events INFO:  eta: 21:36:29  iter: 2399  total_loss: 87.88  loss_ce: 2.726  loss_mask: 1.265  loss_dice: 4.101  loss_ce_0: 3.252  loss_mask_0: 1.174  loss_dice_0: 4.19  loss_ce_1: 2.505  loss_mask_1: 1.231  loss_dice_1: 4.215  loss_ce_2: 2.729  loss_mask_2: 1.241  loss_dice_2: 4.176  loss_ce_3: 2.585  loss_mask_3: 1.351  loss_dice_3: 4.365  loss_ce_4: 2.544  loss_mask_4: 1.239  loss_dice_4: 4.207  loss_ce_5: 2.624  loss_mask_5: 1.308  loss_dice_5: 4.289  loss_ce_6: 2.654  loss_mask_6: 1.158  loss_dice_6: 4.234  loss_ce_7: 2.55  loss_mask_7: 1.232  loss_dice_7: 4.154  loss_ce_8: 2.697  loss_mask_8: 1.181  loss_dice_8: 4.136  loss_mars: 0.7793    time: 5.9375  last_time: 7.6953  data_time: 0.0027  last_data_time: 0.0086   lr: 0.0001  max_mem: 0M
[10/17 11:30:55] d2.utils.events INFO:  eta: 21:36:10  iter: 2419  total_loss: 82.91  loss_ce: 2.844  loss_mask: 1.543  loss_dice: 3.88  loss_ce_0: 3.235  loss_mask_0: 1.532  loss_dice_0: 3.794  loss_ce_1: 2.768  loss_mask_1: 1.636  loss_dice_1: 3.801  loss_ce_2: 2.741  loss_mask_2: 1.579  loss_dice_2: 3.682  loss_ce_3: 2.532  loss_mask_3: 1.557  loss_dice_3: 3.748  loss_ce_4: 2.536  loss_mask_4: 1.649  loss_dice_4: 3.608  loss_ce_5: 2.637  loss_mask_5: 1.52  loss_dice_5: 3.784  loss_ce_6: 2.819  loss_mask_6: 1.626  loss_dice_6: 3.748  loss_ce_7: 2.812  loss_mask_7: 1.524  loss_dice_7: 3.715  loss_ce_8: 2.958  loss_mask_8: 1.685  loss_dice_8: 3.669  loss_mars: 0.8653    time: 5.9987  last_time: 4.9690  data_time: 0.0037  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/17 11:32:49] d2.utils.events INFO:  eta: 21:32:54  iter: 2439  total_loss: 85.07  loss_ce: 3.794  loss_mask: 0.7026  loss_dice: 3.923  loss_ce_0: 5.289  loss_mask_0: 0.6649  loss_dice_0: 4.089  loss_ce_1: 3.756  loss_mask_1: 0.6036  loss_dice_1: 4.16  loss_ce_2: 3.657  loss_mask_2: 0.6504  loss_dice_2: 4.23  loss_ce_3: 3.873  loss_mask_3: 0.6564  loss_dice_3: 4.103  loss_ce_4: 3.973  loss_mask_4: 0.647  loss_dice_4: 3.966  loss_ce_5: 3.744  loss_mask_5: 0.5934  loss_dice_5: 4.062  loss_ce_6: 3.649  loss_mask_6: 0.6476  loss_dice_6: 3.963  loss_ce_7: 3.611  loss_mask_7: 0.5864  loss_dice_7: 4.151  loss_ce_8: 3.761  loss_mask_8: 0.6186  loss_dice_8: 4.022  loss_mars: 0.8216    time: 5.9965  last_time: 5.8137  data_time: 0.0033  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/17 11:34:43] d2.utils.events INFO:  eta: 21:32:17  iter: 2459  total_loss: 89.77  loss_ce: 3.516  loss_mask: 1.444  loss_dice: 3.579  loss_ce_0: 3.753  loss_mask_0: 1.543  loss_dice_0: 3.72  loss_ce_1: 3.368  loss_mask_1: 1.557  loss_dice_1: 3.792  loss_ce_2: 3.428  loss_mask_2: 1.417  loss_dice_2: 3.645  loss_ce_3: 3.388  loss_mask_3: 1.507  loss_dice_3: 3.746  loss_ce_4: 3.303  loss_mask_4: 1.476  loss_dice_4: 3.66  loss_ce_5: 3.332  loss_mask_5: 1.613  loss_dice_5: 3.714  loss_ce_6: 3.247  loss_mask_6: 1.37  loss_dice_6: 3.796  loss_ce_7: 3.326  loss_mask_7: 1.399  loss_dice_7: 3.804  loss_ce_8: 3.371  loss_mask_8: 1.496  loss_dice_8: 3.825  loss_mars: 0.8872    time: 5.9938  last_time: 5.1907  data_time: 0.0025  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/17 11:36:40] d2.utils.events INFO:  eta: 21:30:37  iter: 2479  total_loss: 89.96  loss_ce: 3.572  loss_mask: 1.136  loss_dice: 4.222  loss_ce_0: 4.136  loss_mask_0: 1.148  loss_dice_0: 4.279  loss_ce_1: 3.846  loss_mask_1: 1.008  loss_dice_1: 4.3  loss_ce_2: 4.008  loss_mask_2: 0.9782  loss_dice_2: 4.27  loss_ce_3: 3.789  loss_mask_3: 0.9833  loss_dice_3: 4.21  loss_ce_4: 3.844  loss_mask_4: 1.061  loss_dice_4: 4.266  loss_ce_5: 3.731  loss_mask_5: 1.007  loss_dice_5: 4.319  loss_ce_6: 3.696  loss_mask_6: 1.009  loss_dice_6: 4.266  loss_ce_7: 3.906  loss_mask_7: 0.9462  loss_dice_7: 4.283  loss_ce_8: 3.528  loss_mask_8: 0.9715  loss_dice_8: 4.264  loss_mars: 0.943    time: 5.9929  last_time: 6.1908  data_time: 0.0032  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/17 11:38:35] d2.utils.events INFO:  eta: 21:30:18  iter: 2499  total_loss: 91.45  loss_ce: 3.04  loss_mask: 1.517  loss_dice: 3.801  loss_ce_0: 3.564  loss_mask_0: 0.9326  loss_dice_0: 3.963  loss_ce_1: 3.199  loss_mask_1: 0.9837  loss_dice_1: 3.705  loss_ce_2: 3.167  loss_mask_2: 1.306  loss_dice_2: 3.834  loss_ce_3: 3.115  loss_mask_3: 1.262  loss_dice_3: 3.772  loss_ce_4: 3.032  loss_mask_4: 0.9975  loss_dice_4: 3.872  loss_ce_5: 3.297  loss_mask_5: 1.008  loss_dice_5: 3.941  loss_ce_6: 3.337  loss_mask_6: 0.9581  loss_dice_6: 3.864  loss_ce_7: 3.378  loss_mask_7: 1.111  loss_dice_7: 4.07  loss_ce_8: 3.362  loss_mask_8: 1.343  loss_dice_8: 3.828  loss_mars: 0.8205    time: 5.9908  last_time: 5.3224  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 11:40:26] d2.utils.events INFO:  eta: 21:29:04  iter: 2519  total_loss: 89.13  loss_ce: 3.014  loss_mask: 1.405  loss_dice: 3.961  loss_ce_0: 3.036  loss_mask_0: 1.385  loss_dice_0: 3.775  loss_ce_1: 2.592  loss_mask_1: 1.33  loss_dice_1: 3.783  loss_ce_2: 2.645  loss_mask_2: 1.472  loss_dice_2: 3.759  loss_ce_3: 2.795  loss_mask_3: 1.389  loss_dice_3: 3.827  loss_ce_4: 2.779  loss_mask_4: 1.402  loss_dice_4: 4.133  loss_ce_5: 2.777  loss_mask_5: 1.519  loss_dice_5: 4.033  loss_ce_6: 2.837  loss_mask_6: 1.365  loss_dice_6: 4.132  loss_ce_7: 3.212  loss_mask_7: 1.541  loss_dice_7: 4.222  loss_ce_8: 3.097  loss_mask_8: 1.428  loss_dice_8: 3.944  loss_mars: 0.7387    time: 5.9873  last_time: 5.2392  data_time: 0.0032  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/17 11:42:22] d2.utils.events INFO:  eta: 21:28:13  iter: 2539  total_loss: 102.8  loss_ce: 3.775  loss_mask: 1.537  loss_dice: 4.496  loss_ce_0: 3.803  loss_mask_0: 1.387  loss_dice_0: 4.25  loss_ce_1: 3.493  loss_mask_1: 1.404  loss_dice_1: 4.61  loss_ce_2: 3.414  loss_mask_2: 1.578  loss_dice_2: 4.572  loss_ce_3: 3.293  loss_mask_3: 1.533  loss_dice_3: 4.68  loss_ce_4: 3.397  loss_mask_4: 1.601  loss_dice_4: 4.612  loss_ce_5: 3.472  loss_mask_5: 1.609  loss_dice_5: 4.544  loss_ce_6: 3.516  loss_mask_6: 1.674  loss_dice_6: 4.532  loss_ce_7: 3.467  loss_mask_7: 1.73  loss_dice_7: 4.512  loss_ce_8: 3.537  loss_mask_8: 1.706  loss_dice_8: 4.301  loss_mars: 0.405    time: 5.9859  last_time: 6.1329  data_time: 0.0035  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/17 11:44:17] d2.utils.events INFO:  eta: 21:26:15  iter: 2559  total_loss: 87.29  loss_ce: 2.943  loss_mask: 1.622  loss_dice: 3.756  loss_ce_0: 3.056  loss_mask_0: 1.351  loss_dice_0: 3.889  loss_ce_1: 3.031  loss_mask_1: 1.4  loss_dice_1: 3.976  loss_ce_2: 2.808  loss_mask_2: 1.426  loss_dice_2: 3.87  loss_ce_3: 2.731  loss_mask_3: 1.545  loss_dice_3: 3.938  loss_ce_4: 2.802  loss_mask_4: 1.436  loss_dice_4: 3.94  loss_ce_5: 2.643  loss_mask_5: 1.495  loss_dice_5: 3.843  loss_ce_6: 2.793  loss_mask_6: 1.574  loss_dice_6: 3.896  loss_ce_7: 2.843  loss_mask_7: 1.516  loss_dice_7: 3.925  loss_ce_8: 2.992  loss_mask_8: 1.62  loss_dice_8: 3.698  loss_mars: 0.6365    time: 5.9840  last_time: 6.0095  data_time: 0.0033  last_data_time: 0.0077   lr: 0.0001  max_mem: 0M
[10/17 11:46:12] d2.utils.events INFO:  eta: 21:24:41  iter: 2579  total_loss: 88.43  loss_ce: 2.744  loss_mask: 1.449  loss_dice: 4.176  loss_ce_0: 2.778  loss_mask_0: 1.208  loss_dice_0: 4.259  loss_ce_1: 2.985  loss_mask_1: 1.244  loss_dice_1: 4.336  loss_ce_2: 3.01  loss_mask_2: 1.463  loss_dice_2: 4.322  loss_ce_3: 2.934  loss_mask_3: 1.626  loss_dice_3: 3.91  loss_ce_4: 3.287  loss_mask_4: 1.538  loss_dice_4: 4.425  loss_ce_5: 2.8  loss_mask_5: 1.606  loss_dice_5: 4.446  loss_ce_6: 2.85  loss_mask_6: 1.513  loss_dice_6: 4.268  loss_ce_7: 2.781  loss_mask_7: 1.406  loss_dice_7: 4.194  loss_ce_8: 2.631  loss_mask_8: 1.593  loss_dice_8: 4.494  loss_mars: 0.6492    time: 5.9821  last_time: 5.4054  data_time: 0.0036  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 11:48:08] d2.utils.events INFO:  eta: 21:21:37  iter: 2599  total_loss: 102.7  loss_ce: 4.765  loss_mask: 0.9231  loss_dice: 4.052  loss_ce_0: 5.38  loss_mask_0: 1.104  loss_dice_0: 4.065  loss_ce_1: 4.861  loss_mask_1: 0.9346  loss_dice_1: 4.006  loss_ce_2: 5.054  loss_mask_2: 0.9408  loss_dice_2: 4.095  loss_ce_3: 4.982  loss_mask_3: 0.8683  loss_dice_3: 4.072  loss_ce_4: 4.748  loss_mask_4: 0.9342  loss_dice_4: 4.227  loss_ce_5: 4.691  loss_mask_5: 1.015  loss_dice_5: 4.039  loss_ce_6: 4.926  loss_mask_6: 0.9867  loss_dice_6: 4.2  loss_ce_7: 4.478  loss_mask_7: 1.023  loss_dice_7: 4.127  loss_ce_8: 4.669  loss_mask_8: 0.989  loss_dice_8: 4.055  loss_mars: 0.8579    time: 5.9809  last_time: 5.5465  data_time: 0.0040  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/17 11:50:10] d2.utils.events INFO:  eta: 21:20:45  iter: 2619  total_loss: 101.6  loss_ce: 4.754  loss_mask: 0.6872  loss_dice: 4.56  loss_ce_0: 5.263  loss_mask_0: 0.7227  loss_dice_0: 4.637  loss_ce_1: 5.405  loss_mask_1: 0.6635  loss_dice_1: 4.613  loss_ce_2: 5.504  loss_mask_2: 0.6727  loss_dice_2: 4.523  loss_ce_3: 5.085  loss_mask_3: 0.6986  loss_dice_3: 4.521  loss_ce_4: 5.231  loss_mask_4: 0.6676  loss_dice_4: 4.615  loss_ce_5: 5.128  loss_mask_5: 0.6799  loss_dice_5: 4.55  loss_ce_6: 4.891  loss_mask_6: 0.706  loss_dice_6: 4.513  loss_ce_7: 4.848  loss_mask_7: 0.6873  loss_dice_7: 4.547  loss_ce_8: 4.732  loss_mask_8: 0.6577  loss_dice_8: 4.532  loss_mars: 0.6021    time: 5.9818  last_time: 6.3306  data_time: 0.0036  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/17 11:52:02] d2.utils.events INFO:  eta: 21:18:47  iter: 2639  total_loss: 94.4  loss_ce: 3.925  loss_mask: 1.188  loss_dice: 4.383  loss_ce_0: 3.972  loss_mask_0: 1.045  loss_dice_0: 4.152  loss_ce_1: 4.468  loss_mask_1: 1.01  loss_dice_1: 4.157  loss_ce_2: 3.942  loss_mask_2: 1.026  loss_dice_2: 4.277  loss_ce_3: 3.904  loss_mask_3: 0.9338  loss_dice_3: 4.157  loss_ce_4: 3.95  loss_mask_4: 1.017  loss_dice_4: 4.113  loss_ce_5: 3.864  loss_mask_5: 1.052  loss_dice_5: 4.053  loss_ce_6: 3.867  loss_mask_6: 1.141  loss_dice_6: 4.243  loss_ce_7: 4.031  loss_mask_7: 1.032  loss_dice_7: 4.179  loss_ce_8: 3.896  loss_mask_8: 1.131  loss_dice_8: 4.194  loss_mars: 0.5358    time: 5.9786  last_time: 6.0746  data_time: 0.0022  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/17 11:54:00] d2.utils.events INFO:  eta: 21:17:10  iter: 2659  total_loss: 90.91  loss_ce: 3.183  loss_mask: 1.225  loss_dice: 4.522  loss_ce_0: 3.428  loss_mask_0: 1.074  loss_dice_0: 4.35  loss_ce_1: 3.046  loss_mask_1: 1.11  loss_dice_1: 4.367  loss_ce_2: 3.163  loss_mask_2: 1.163  loss_dice_2: 4.404  loss_ce_3: 2.872  loss_mask_3: 1.165  loss_dice_3: 4.278  loss_ce_4: 2.902  loss_mask_4: 1.213  loss_dice_4: 4.249  loss_ce_5: 3.002  loss_mask_5: 1.179  loss_dice_5: 4.227  loss_ce_6: 3.055  loss_mask_6: 1.095  loss_dice_6: 4.335  loss_ce_7: 3.311  loss_mask_7: 1.037  loss_dice_7: 4.405  loss_ce_8: 3.087  loss_mask_8: 1.199  loss_dice_8: 4.585  loss_mars: 0.5895    time: 5.9782  last_time: 6.0204  data_time: 0.0024  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 11:55:52] d2.utils.events INFO:  eta: 21:15:12  iter: 2679  total_loss: 93.85  loss_ce: 3.73  loss_mask: 1.277  loss_dice: 4.057  loss_ce_0: 4.01  loss_mask_0: 1.201  loss_dice_0: 4.176  loss_ce_1: 4.113  loss_mask_1: 1.198  loss_dice_1: 4.106  loss_ce_2: 3.709  loss_mask_2: 1.285  loss_dice_2: 4.152  loss_ce_3: 3.688  loss_mask_3: 1.244  loss_dice_3: 4.153  loss_ce_4: 3.698  loss_mask_4: 1.33  loss_dice_4: 4.119  loss_ce_5: 3.505  loss_mask_5: 1.297  loss_dice_5: 4.089  loss_ce_6: 3.778  loss_mask_6: 1.237  loss_dice_6: 4.045  loss_ce_7: 3.704  loss_mask_7: 1.153  loss_dice_7: 4.104  loss_ce_8: 3.6  loss_mask_8: 1.192  loss_dice_8: 4.044  loss_mars: 0.9229    time: 5.9752  last_time: 6.0914  data_time: 0.0031  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/17 11:57:49] d2.utils.events INFO:  eta: 21:13:34  iter: 2699  total_loss: 93.01  loss_ce: 3.37  loss_mask: 1.434  loss_dice: 3.941  loss_ce_0: 3.169  loss_mask_0: 1.309  loss_dice_0: 3.844  loss_ce_1: 3.339  loss_mask_1: 1.337  loss_dice_1: 3.791  loss_ce_2: 3.337  loss_mask_2: 1.284  loss_dice_2: 3.927  loss_ce_3: 3.589  loss_mask_3: 1.247  loss_dice_3: 3.86  loss_ce_4: 3.23  loss_mask_4: 1.303  loss_dice_4: 3.923  loss_ce_5: 3.446  loss_mask_5: 1.35  loss_dice_5: 4.261  loss_ce_6: 3.286  loss_mask_6: 1.267  loss_dice_6: 3.871  loss_ce_7: 3.452  loss_mask_7: 1.379  loss_dice_7: 4.022  loss_ce_8: 3.325  loss_mask_8: 1.5  loss_dice_8: 4.132  loss_mars: 0.804    time: 5.9743  last_time: 5.6402  data_time: 0.0036  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/17 11:59:45] d2.utils.events INFO:  eta: 21:11:03  iter: 2719  total_loss: 81.63  loss_ce: 2.6  loss_mask: 1.459  loss_dice: 4.27  loss_ce_0: 2.808  loss_mask_0: 1.493  loss_dice_0: 4.16  loss_ce_1: 2.759  loss_mask_1: 1.621  loss_dice_1: 4.113  loss_ce_2: 2.517  loss_mask_2: 1.576  loss_dice_2: 4.103  loss_ce_3: 2.477  loss_mask_3: 1.56  loss_dice_3: 4.201  loss_ce_4: 2.434  loss_mask_4: 1.462  loss_dice_4: 4.201  loss_ce_5: 2.327  loss_mask_5: 1.711  loss_dice_5: 4.166  loss_ce_6: 2.341  loss_mask_6: 1.455  loss_dice_6: 4.193  loss_ce_7: 2.389  loss_mask_7: 1.499  loss_dice_7: 4.269  loss_ce_8: 2.369  loss_mask_8: 1.487  loss_dice_8: 4.254  loss_mars: 0.9044    time: 5.9731  last_time: 5.5673  data_time: 0.0022  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 12:01:30] d2.utils.events INFO:  eta: 21:06:54  iter: 2739  total_loss: 80.35  loss_ce: 2.35  loss_mask: 1.671  loss_dice: 4.029  loss_ce_0: 2.645  loss_mask_0: 1.724  loss_dice_0: 3.753  loss_ce_1: 2.357  loss_mask_1: 1.832  loss_dice_1: 3.777  loss_ce_2: 2.107  loss_mask_2: 1.57  loss_dice_2: 4.424  loss_ce_3: 2.193  loss_mask_3: 1.952  loss_dice_3: 4.005  loss_ce_4: 2.154  loss_mask_4: 1.706  loss_dice_4: 4.119  loss_ce_5: 2.154  loss_mask_5: 1.727  loss_dice_5: 4.055  loss_ce_6: 2.034  loss_mask_6: 1.705  loss_dice_6: 4.021  loss_ce_7: 2.109  loss_mask_7: 1.906  loss_dice_7: 3.997  loss_ce_8: 2.039  loss_mask_8: 1.795  loss_dice_8: 3.908  loss_mars: 0.7261    time: 5.9678  last_time: 5.2015  data_time: 0.0036  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 12:03:23] d2.utils.events INFO:  eta: 21:03:46  iter: 2759  total_loss: 96.85  loss_ce: 3.886  loss_mask: 0.9835  loss_dice: 4.415  loss_ce_0: 3.496  loss_mask_0: 1.043  loss_dice_0: 4.151  loss_ce_1: 3.314  loss_mask_1: 1.1  loss_dice_1: 4.199  loss_ce_2: 3.313  loss_mask_2: 1.058  loss_dice_2: 4.379  loss_ce_3: 3.17  loss_mask_3: 1.152  loss_dice_3: 4.332  loss_ce_4: 3.469  loss_mask_4: 1.229  loss_dice_4: 4.22  loss_ce_5: 3.585  loss_mask_5: 1.144  loss_dice_5: 4.31  loss_ce_6: 3.886  loss_mask_6: 1.157  loss_dice_6: 4.232  loss_ce_7: 3.842  loss_mask_7: 1.149  loss_dice_7: 4.181  loss_ce_8: 3.333  loss_mask_8: 1.322  loss_dice_8: 4.198  loss_mars: 0.2703    time: 5.9654  last_time: 6.0307  data_time: 0.0029  last_data_time: 0.0046   lr: 0.0001  max_mem: 0M
[10/17 12:05:11] d2.utils.events INFO:  eta: 20:59:03  iter: 2779  total_loss: 89.73  loss_ce: 3.301  loss_mask: 0.9606  loss_dice: 4.101  loss_ce_0: 3.857  loss_mask_0: 0.9876  loss_dice_0: 4.215  loss_ce_1: 3.365  loss_mask_1: 1.081  loss_dice_1: 4.07  loss_ce_2: 3.396  loss_mask_2: 1.021  loss_dice_2: 4.061  loss_ce_3: 3.188  loss_mask_3: 0.9965  loss_dice_3: 4.204  loss_ce_4: 3.244  loss_mask_4: 1.064  loss_dice_4: 4.243  loss_ce_5: 3.286  loss_mask_5: 1.048  loss_dice_5: 4.267  loss_ce_6: 3.218  loss_mask_6: 1.037  loss_dice_6: 4.25  loss_ce_7: 3.421  loss_mask_7: 0.987  loss_dice_7: 4.183  loss_ce_8: 3.213  loss_mask_8: 1.047  loss_dice_8: 4.24  loss_mars: 0.8748    time: 5.9615  last_time: 3.7231  data_time: 0.0031  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 12:07:13] d2.utils.events INFO:  eta: 20:58:50  iter: 2799  total_loss: 95.4  loss_ce: 3.522  loss_mask: 1.438  loss_dice: 4.421  loss_ce_0: 4.087  loss_mask_0: 1.294  loss_dice_0: 4.355  loss_ce_1: 3.712  loss_mask_1: 1.44  loss_dice_1: 4.357  loss_ce_2: 3.47  loss_mask_2: 1.47  loss_dice_2: 4.364  loss_ce_3: 3.551  loss_mask_3: 1.377  loss_dice_3: 4.357  loss_ce_4: 3.431  loss_mask_4: 1.47  loss_dice_4: 4.249  loss_ce_5: 3.499  loss_mask_5: 1.565  loss_dice_5: 4.296  loss_ce_6: 3.472  loss_mask_6: 1.604  loss_dice_6: 4.268  loss_ce_7: 3.611  loss_mask_7: 1.379  loss_dice_7: 4.329  loss_ce_8: 3.49  loss_mask_8: 1.517  loss_dice_8: 4.388  loss_mars: 0.2583    time: 5.9625  last_time: 5.3431  data_time: 0.0030  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/17 12:09:08] d2.utils.events INFO:  eta: 20:54:16  iter: 2819  total_loss: 95.21  loss_ce: 3.978  loss_mask: 0.9212  loss_dice: 4.008  loss_ce_0: 4.011  loss_mask_0: 0.8212  loss_dice_0: 4.123  loss_ce_1: 3.923  loss_mask_1: 0.8  loss_dice_1: 4.168  loss_ce_2: 3.963  loss_mask_2: 0.8766  loss_dice_2: 4.104  loss_ce_3: 4.071  loss_mask_3: 0.8071  loss_dice_3: 4.197  loss_ce_4: 4.02  loss_mask_4: 0.8792  loss_dice_4: 4.155  loss_ce_5: 4.098  loss_mask_5: 0.9167  loss_dice_5: 4.177  loss_ce_6: 4.116  loss_mask_6: 1.04  loss_dice_6: 4.124  loss_ce_7: 4.003  loss_mask_7: 0.8155  loss_dice_7: 4.141  loss_ce_8: 3.923  loss_mask_8: 0.8484  loss_dice_8: 4.102  loss_mars: 0.8376    time: 5.9607  last_time: 5.3055  data_time: 0.0035  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/17 12:11:08] d2.utils.events INFO:  eta: 20:53:28  iter: 2839  total_loss: 99.55  loss_ce: 4.353  loss_mask: 1.265  loss_dice: 4.541  loss_ce_0: 4.555  loss_mask_0: 1.064  loss_dice_0: 4.449  loss_ce_1: 4.455  loss_mask_1: 1.025  loss_dice_1: 4.354  loss_ce_2: 4.627  loss_mask_2: 1.134  loss_dice_2: 4.414  loss_ce_3: 4.527  loss_mask_3: 1.202  loss_dice_3: 4.428  loss_ce_4: 4.492  loss_mask_4: 1.268  loss_dice_4: 4.42  loss_ce_5: 4.344  loss_mask_5: 1.269  loss_dice_5: 4.433  loss_ce_6: 4.244  loss_mask_6: 1.215  loss_dice_6: 4.474  loss_ce_7: 4.256  loss_mask_7: 1.161  loss_dice_7: 4.538  loss_ce_8: 4.286  loss_mask_8: 1.208  loss_dice_8: 4.564  loss_mars: 0.8875    time: 5.9611  last_time: 7.2056  data_time: 0.0030  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/17 12:13:05] d2.utils.events INFO:  eta: 20:52:57  iter: 2859  total_loss: 86.13  loss_ce: 2.592  loss_mask: 1.493  loss_dice: 4.154  loss_ce_0: 2.82  loss_mask_0: 1.442  loss_dice_0: 3.98  loss_ce_1: 2.649  loss_mask_1: 1.599  loss_dice_1: 4.026  loss_ce_2: 2.436  loss_mask_2: 1.589  loss_dice_2: 4.059  loss_ce_3: 2.45  loss_mask_3: 1.648  loss_dice_3: 4.059  loss_ce_4: 2.464  loss_mask_4: 1.602  loss_dice_4: 4.086  loss_ce_5: 2.604  loss_mask_5: 1.685  loss_dice_5: 4.144  loss_ce_6: 2.613  loss_mask_6: 1.566  loss_dice_6: 4.12  loss_ce_7: 2.64  loss_mask_7: 1.675  loss_dice_7: 4.069  loss_ce_8: 2.609  loss_mask_8: 1.668  loss_dice_8: 4.126  loss_mars: 0.6852    time: 5.9603  last_time: 5.3977  data_time: 0.0029  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 12:14:58] d2.utils.events INFO:  eta: 20:50:25  iter: 2879  total_loss: 94.6  loss_ce: 3.896  loss_mask: 0.955  loss_dice: 4.418  loss_ce_0: 4.818  loss_mask_0: 1.084  loss_dice_0: 4.538  loss_ce_1: 4.005  loss_mask_1: 1.034  loss_dice_1: 4.497  loss_ce_2: 3.962  loss_mask_2: 1.067  loss_dice_2: 4.561  loss_ce_3: 4.058  loss_mask_3: 1.101  loss_dice_3: 4.422  loss_ce_4: 4.027  loss_mask_4: 1.119  loss_dice_4: 4.473  loss_ce_5: 3.973  loss_mask_5: 1.059  loss_dice_5: 4.379  loss_ce_6: 3.88  loss_mask_6: 1.017  loss_dice_6: 4.443  loss_ce_7: 3.842  loss_mask_7: 0.9726  loss_dice_7: 4.512  loss_ce_8: 3.792  loss_mask_8: 0.9889  loss_dice_8: 4.46  loss_mars: 0.2864    time: 5.9582  last_time: 5.5557  data_time: 0.0025  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/17 12:16:51] d2.utils.events INFO:  eta: 20:46:25  iter: 2899  total_loss: 93.86  loss_ce: 3.953  loss_mask: 0.9285  loss_dice: 4.125  loss_ce_0: 4.099  loss_mask_0: 0.8618  loss_dice_0: 4.212  loss_ce_1: 3.828  loss_mask_1: 1.059  loss_dice_1: 4.297  loss_ce_2: 3.812  loss_mask_2: 0.921  loss_dice_2: 4.149  loss_ce_3: 4.052  loss_mask_3: 0.9851  loss_dice_3: 4.186  loss_ce_4: 4.038  loss_mask_4: 0.9912  loss_dice_4: 4.083  loss_ce_5: 3.886  loss_mask_5: 0.9879  loss_dice_5: 4.16  loss_ce_6: 4.024  loss_mask_6: 0.9572  loss_dice_6: 4.152  loss_ce_7: 3.888  loss_mask_7: 0.9066  loss_dice_7: 4.119  loss_ce_8: 3.942  loss_mask_8: 1.039  loss_dice_8: 4.214  loss_mars: 0.8844    time: 5.9560  last_time: 7.5818  data_time: 0.0027  last_data_time: 0.0047   lr: 0.0001  max_mem: 0M
[10/17 12:18:44] d2.utils.events INFO:  eta: 20:42:03  iter: 2919  total_loss: 87  loss_ce: 2.893  loss_mask: 1.631  loss_dice: 4.127  loss_ce_0: 3.482  loss_mask_0: 1.472  loss_dice_0: 3.649  loss_ce_1: 2.904  loss_mask_1: 1.713  loss_dice_1: 3.981  loss_ce_2: 3.046  loss_mask_2: 1.666  loss_dice_2: 3.911  loss_ce_3: 2.925  loss_mask_3: 1.717  loss_dice_3: 3.848  loss_ce_4: 2.872  loss_mask_4: 1.506  loss_dice_4: 3.771  loss_ce_5: 2.73  loss_mask_5: 1.485  loss_dice_5: 3.776  loss_ce_6: 2.825  loss_mask_6: 1.608  loss_dice_6: 3.796  loss_ce_7: 2.905  loss_mask_7: 1.632  loss_dice_7: 3.919  loss_ce_8: 2.853  loss_mask_8: 1.601  loss_dice_8: 3.895  loss_mars: 0.7743    time: 5.9540  last_time: 5.5911  data_time: 0.0021  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 12:20:38] d2.utils.events INFO:  eta: 20:40:07  iter: 2939  total_loss: 90.1  loss_ce: 3.39  loss_mask: 0.8909  loss_dice: 4.372  loss_ce_0: 3.844  loss_mask_0: 0.6964  loss_dice_0: 4.322  loss_ce_1: 3.419  loss_mask_1: 0.7558  loss_dice_1: 4.344  loss_ce_2: 3.309  loss_mask_2: 0.9164  loss_dice_2: 4.247  loss_ce_3: 3.412  loss_mask_3: 0.939  loss_dice_3: 4.237  loss_ce_4: 3.322  loss_mask_4: 0.8283  loss_dice_4: 4.294  loss_ce_5: 3.363  loss_mask_5: 0.9114  loss_dice_5: 4.235  loss_ce_6: 3.446  loss_mask_6: 0.8789  loss_dice_6: 4.413  loss_ce_7: 3.408  loss_mask_7: 0.7816  loss_dice_7: 4.337  loss_ce_8: 3.409  loss_mask_8: 0.9018  loss_dice_8: 4.333  loss_mars: 0.6649    time: 5.9524  last_time: 6.0609  data_time: 0.0031  last_data_time: 0.0085   lr: 0.0001  max_mem: 0M
[10/17 12:22:33] d2.utils.events INFO:  eta: 20:35:10  iter: 2959  total_loss: 99.04  loss_ce: 3.398  loss_mask: 1.408  loss_dice: 4.357  loss_ce_0: 4.436  loss_mask_0: 1.242  loss_dice_0: 4.168  loss_ce_1: 3.482  loss_mask_1: 1.129  loss_dice_1: 4.159  loss_ce_2: 3.515  loss_mask_2: 1.153  loss_dice_2: 4.086  loss_ce_3: 3.529  loss_mask_3: 1.222  loss_dice_3: 4.186  loss_ce_4: 3.496  loss_mask_4: 1.427  loss_dice_4: 4.129  loss_ce_5: 3.386  loss_mask_5: 1.482  loss_dice_5: 4.36  loss_ce_6: 3.291  loss_mask_6: 1.431  loss_dice_6: 4.37  loss_ce_7: 3.47  loss_mask_7: 1.112  loss_dice_7: 4.18  loss_ce_8: 3.285  loss_mask_8: 1.651  loss_dice_8: 4.346  loss_mars: 0.4966    time: 5.9508  last_time: 6.1201  data_time: 0.0039  last_data_time: 0.0014   lr: 0.0001  max_mem: 0M
[10/17 12:24:31] d2.utils.events INFO:  eta: 20:33:13  iter: 2979  total_loss: 82.38  loss_ce: 2.617  loss_mask: 1.416  loss_dice: 4.115  loss_ce_0: 2.714  loss_mask_0: 1.569  loss_dice_0: 3.949  loss_ce_1: 2.231  loss_mask_1: 1.323  loss_dice_1: 3.593  loss_ce_2: 2.288  loss_mask_2: 1.476  loss_dice_2: 3.64  loss_ce_3: 2.36  loss_mask_3: 1.523  loss_dice_3: 3.903  loss_ce_4: 2.101  loss_mask_4: 1.66  loss_dice_4: 3.833  loss_ce_5: 2.32  loss_mask_5: 1.53  loss_dice_5: 4.098  loss_ce_6: 2.244  loss_mask_6: 1.645  loss_dice_6: 4.105  loss_ce_7: 2.543  loss_mask_7: 1.585  loss_dice_7: 4.004  loss_ce_8: 2.33  loss_mask_8: 2.266  loss_dice_8: 4.368  loss_mars: 0.5429    time: 5.9506  last_time: 6.0717  data_time: 0.0026  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/17 12:26:25] d2.utils.events INFO:  eta: 20:34:14  iter: 2999  total_loss: 96.73  loss_ce: 4.862  loss_mask: 1.095  loss_dice: 4.376  loss_ce_0: 4.397  loss_mask_0: 1.065  loss_dice_0: 4.149  loss_ce_1: 3.969  loss_mask_1: 1.121  loss_dice_1: 4.214  loss_ce_2: 3.771  loss_mask_2: 1.107  loss_dice_2: 4.094  loss_ce_3: 3.623  loss_mask_3: 1.259  loss_dice_3: 4.353  loss_ce_4: 3.804  loss_mask_4: 1.136  loss_dice_4: 4.284  loss_ce_5: 3.944  loss_mask_5: 1.123  loss_dice_5: 4.247  loss_ce_6: 4.584  loss_mask_6: 1.088  loss_dice_6: 4.213  loss_ce_7: 4.643  loss_mask_7: 1.082  loss_dice_7: 4.251  loss_ce_8: 3.979  loss_mask_8: 1.141  loss_dice_8: 4.276  loss_mars: 0.7977    time: 5.9490  last_time: 3.8447  data_time: 0.0028  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/17 12:28:21] d2.utils.events INFO:  eta: 20:30:48  iter: 3019  total_loss: 94.68  loss_ce: 4.029  loss_mask: 1.412  loss_dice: 4.395  loss_ce_0: 3.472  loss_mask_0: 1.239  loss_dice_0: 4.416  loss_ce_1: 3.682  loss_mask_1: 1.395  loss_dice_1: 4.347  loss_ce_2: 3.41  loss_mask_2: 1.31  loss_dice_2: 4.277  loss_ce_3: 3.065  loss_mask_3: 1.329  loss_dice_3: 4.435  loss_ce_4: 3.385  loss_mask_4: 1.345  loss_dice_4: 4.407  loss_ce_5: 3.387  loss_mask_5: 1.303  loss_dice_5: 4.506  loss_ce_6: 3.89  loss_mask_6: 1.263  loss_dice_6: 4.338  loss_ce_7: 3.948  loss_mask_7: 1.407  loss_dice_7: 4.325  loss_ce_8: 3.718  loss_mask_8: 1.304  loss_dice_8: 4.421  loss_mars: 0.722    time: 5.9477  last_time: 4.3551  data_time: 0.0033  last_data_time: 0.0062   lr: 0.0001  max_mem: 0M
[10/17 12:30:14] d2.utils.events INFO:  eta: 20:30:23  iter: 3039  total_loss: 83.89  loss_ce: 2.992  loss_mask: 1.315  loss_dice: 4.125  loss_ce_0: 3.341  loss_mask_0: 1.16  loss_dice_0: 4.141  loss_ce_1: 2.825  loss_mask_1: 1.196  loss_dice_1: 4.06  loss_ce_2: 2.726  loss_mask_2: 1.311  loss_dice_2: 4.114  loss_ce_3: 2.509  loss_mask_3: 1.204  loss_dice_3: 4.142  loss_ce_4: 2.903  loss_mask_4: 1.327  loss_dice_4: 4.179  loss_ce_5: 3.131  loss_mask_5: 1.377  loss_dice_5: 4.115  loss_ce_6: 2.972  loss_mask_6: 1.298  loss_dice_6: 4.024  loss_ce_7: 2.881  loss_mask_7: 1.262  loss_dice_7: 4.05  loss_ce_8: 2.685  loss_mask_8: 1.261  loss_dice_8: 4.157  loss_mars: 0.867    time: 5.9460  last_time: 5.2358  data_time: 0.0026  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 12:32:10] d2.utils.events INFO:  eta: 20:29:49  iter: 3059  total_loss: 90.84  loss_ce: 3.185  loss_mask: 1.927  loss_dice: 3.7  loss_ce_0: 3.486  loss_mask_0: 1.72  loss_dice_0: 3.849  loss_ce_1: 2.958  loss_mask_1: 1.677  loss_dice_1: 3.633  loss_ce_2: 3.083  loss_mask_2: 1.828  loss_dice_2: 3.494  loss_ce_3: 2.699  loss_mask_3: 1.848  loss_dice_3: 3.561  loss_ce_4: 2.951  loss_mask_4: 1.783  loss_dice_4: 3.587  loss_ce_5: 2.977  loss_mask_5: 1.887  loss_dice_5: 3.465  loss_ce_6: 3.249  loss_mask_6: 1.736  loss_dice_6: 3.63  loss_ce_7: 3.258  loss_mask_7: 1.705  loss_dice_7: 3.837  loss_ce_8: 2.971  loss_mask_8: 1.945  loss_dice_8: 3.87  loss_mars: 0.838    time: 5.9451  last_time: 5.4005  data_time: 0.0035  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 12:34:05] d2.utils.events INFO:  eta: 20:24:57  iter: 3079  total_loss: 82.12  loss_ce: 2.995  loss_mask: 1.113  loss_dice: 3.829  loss_ce_0: 3.428  loss_mask_0: 1.161  loss_dice_0: 4.143  loss_ce_1: 2.916  loss_mask_1: 1.084  loss_dice_1: 3.668  loss_ce_2: 2.86  loss_mask_2: 1.132  loss_dice_2: 3.591  loss_ce_3: 2.735  loss_mask_3: 1.124  loss_dice_3: 3.752  loss_ce_4: 2.827  loss_mask_4: 1.072  loss_dice_4: 3.609  loss_ce_5: 2.872  loss_mask_5: 1.089  loss_dice_5: 3.833  loss_ce_6: 2.98  loss_mask_6: 1.274  loss_dice_6: 3.717  loss_ce_7: 2.969  loss_mask_7: 1.2  loss_dice_7: 3.689  loss_ce_8: 2.649  loss_mask_8: 1.211  loss_dice_8: 3.57  loss_mars: 0.8554    time: 5.9437  last_time: 5.7252  data_time: 0.0028  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/17 12:35:56] d2.utils.events INFO:  eta: 20:21:29  iter: 3099  total_loss: 97.22  loss_ce: 4.238  loss_mask: 1.161  loss_dice: 4.167  loss_ce_0: 5.101  loss_mask_0: 1.029  loss_dice_0: 4.129  loss_ce_1: 4.027  loss_mask_1: 1.057  loss_dice_1: 4.247  loss_ce_2: 4.141  loss_mask_2: 1.027  loss_dice_2: 4.177  loss_ce_3: 3.999  loss_mask_3: 1.025  loss_dice_3: 4.316  loss_ce_4: 3.976  loss_mask_4: 0.998  loss_dice_4: 4.208  loss_ce_5: 3.905  loss_mask_5: 1.028  loss_dice_5: 4.046  loss_ce_6: 4.029  loss_mask_6: 0.9125  loss_dice_6: 4.252  loss_ce_7: 4.078  loss_mask_7: 0.9211  loss_dice_7: 4.425  loss_ce_8: 3.86  loss_mask_8: 1.189  loss_dice_8: 4.257  loss_mars: 0.6927    time: 5.9412  last_time: 5.3092  data_time: 0.0034  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 12:37:49] d2.utils.events INFO:  eta: 20:16:50  iter: 3119  total_loss: 91  loss_ce: 3.255  loss_mask: 1.105  loss_dice: 4.163  loss_ce_0: 3.605  loss_mask_0: 1.155  loss_dice_0: 4.197  loss_ce_1: 3.345  loss_mask_1: 1.244  loss_dice_1: 4.125  loss_ce_2: 3.426  loss_mask_2: 1.151  loss_dice_2: 4.197  loss_ce_3: 3.231  loss_mask_3: 1.284  loss_dice_3: 4.203  loss_ce_4: 3.375  loss_mask_4: 1.211  loss_dice_4: 4.26  loss_ce_5: 3.23  loss_mask_5: 1.292  loss_dice_5: 4.198  loss_ce_6: 3.16  loss_mask_6: 1.19  loss_dice_6: 4.346  loss_ce_7: 3.313  loss_mask_7: 1.151  loss_dice_7: 4.128  loss_ce_8: 3.392  loss_mask_8: 1.213  loss_dice_8: 4.236  loss_mars: 0.6861    time: 5.9393  last_time: 6.0202  data_time: 0.0030  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/17 12:38:22] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0003125.pth
[10/17 12:38:22] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch/serialization.py", line 652, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch/serialization.py", line 886, in _save
    zip_file.write_record(name, storage, num_bytes)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 156, in train
    self.after_step()
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 190, in after_step
    h.after_step()
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2/engine/hooks.py", line 207, in after_step
    self.step(self.trainer.iter)
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/fvcore/common/checkpoint.py", line 428, in step
    self.checkpointer.save(
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/fvcore/common/checkpoint.py", line 126, in save
    torch.save(data, cast(IO[bytes], f))
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch/serialization.py", line 653, in save
    return
  File "/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch/serialization.py", line 515, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_cont[10/19 00:55:32] detectron2 INFO: Rank of current process: 0. World size: 1
[10/19 00:55:32] detectron2 INFO: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.10 (default, Mar 18 2025, 20:04:55) [GCC 9.4.0]
numpy                            1.24.4
detectron2                       0.6 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX A6000 (arch=8.6)
Driver version                   535.230.02
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           10.4.0
torchvision                      0.19.1+cu118 @/home/vaishali/projects/Mask2Former/mask2former_git/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.12.0
-------------------------------  ---------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/19 00:55:32] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/mars_5k_50ep_corrected.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=True)
[10/19 00:55:32] detectron2 INFO: Contents of args.config_file=configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/19 00:55:32] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/19 00:55:32] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/19 00:55:40] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.40 seconds.
[10/19 00:55:40] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/19 00:55:44] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/19 00:55:45] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/19 00:55:45] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/19 00:55:45] d2.data.build INFO: Using training sampler TrainingSampler
[10/19 00:55:45] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 00:55:45] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/19 00:55:45] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/19 00:55:45] d2.data.build INFO: Making batched data loader with batch_size=1
[10/19 00:55:45] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./output_mars_fixed_5k_50ep/model_0001562.pth ...
[10/19 00:55:45] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output_mars_fixed_5k_50ep/model_0001562.pth ...
[10/19 00:55:46] fvcore.common.checkpoint INFO: Loading trainer from ./output_mars_fixed_5k_50ep/model_0001562.pth ...
[10/19 00:55:46] d2.engine.hooks INFO: Loading scheduler from state_dict ...
[10/19 00:55:46] d2.engine.train_loop INFO: Starting training from iteration 1563
[10/19 00:57:27] d2.utils.events INFO:  eta: 23:03:15  iter: 1579  total_loss: 81.01  loss_ce: 2.219  loss_mask: 1.685  loss_dice: 3.82  loss_ce_0: 3.524  loss_mask_0: 1.6  loss_dice_0: 3.657  loss_ce_1: 2.182  loss_mask_1: 1.654  loss_dice_1: 3.67  loss_ce_2: 2.088  loss_mask_2: 1.584  loss_dice_2: 3.814  loss_ce_3: 2.152  loss_mask_3: 1.53  loss_dice_3: 3.865  loss_ce_4: 1.969  loss_mask_4: 1.673  loss_dice_4: 3.735  loss_ce_5: 2.012  loss_mask_5: 1.92  loss_dice_5: 3.959  loss_ce_6: 1.953  loss_mask_6: 1.969  loss_dice_6: 4.051  loss_ce_7: 2.078  loss_mask_7: 1.65  loss_dice_7: 3.834  loss_ce_8: 1.937  loss_mask_8: 1.759  loss_dice_8: 3.83  loss_mars: 0.9366    time: 6.0717  last_time: 5.2305  data_time: 0.0111  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 00:59:24] d2.utils.events INFO:  eta: 22:10:48  iter: 1599  total_loss: 96.02  loss_ce: 3.83  loss_mask: 1.516  loss_dice: 4.218  loss_ce_0: 5.302  loss_mask_0: 1.494  loss_dice_0: 4.151  loss_ce_1: 3.425  loss_mask_1: 1.511  loss_dice_1: 4.167  loss_ce_2: 3.791  loss_mask_2: 1.503  loss_dice_2: 4.182  loss_ce_3: 3.398  loss_mask_3: 1.562  loss_dice_3: 4.221  loss_ce_4: 3.706  loss_mask_4: 1.536  loss_dice_4: 4.218  loss_ce_5: 3.655  loss_mask_5: 1.569  loss_dice_5: 4.22  loss_ce_6: 3.503  loss_mask_6: 1.492  loss_dice_6: 4.313  loss_ce_7: 3.481  loss_mask_7: 1.49  loss_dice_7: 4.297  loss_ce_8: 3.642  loss_mask_8: 1.536  loss_dice_8: 4.273  loss_mars: 0.923    time: 5.9318  last_time: 5.4273  data_time: 0.0027  last_data_time: 0.0041   lr: 0.0001  max_mem: 0M
[10/19 01:01:20] d2.utils.events INFO:  eta: 22:37:57  iter: 1619  total_loss: 86.61  loss_ce: 3.199  loss_mask: 1.031  loss_dice: 4.323  loss_ce_0: 4.128  loss_mask_0: 0.9918  loss_dice_0: 4.169  loss_ce_1: 3.109  loss_mask_1: 0.9575  loss_dice_1: 4.183  loss_ce_2: 3.125  loss_mask_2: 1.001  loss_dice_2: 4.051  loss_ce_3: 3.086  loss_mask_3: 1.059  loss_dice_3: 4.256  loss_ce_4: 3.183  loss_mask_4: 1.006  loss_dice_4: 4.169  loss_ce_5: 3.056  loss_mask_5: 1.023  loss_dice_5: 4.264  loss_ce_6: 3.006  loss_mask_6: 1.114  loss_dice_6: 4.373  loss_ce_7: 3.101  loss_mask_7: 1.058  loss_dice_7: 4.268  loss_ce_8: 3.081  loss_mask_8: 1.114  loss_dice_8: 4.32  loss_mars: 0.6974    time: 5.8817  last_time: 6.2668  data_time: 0.0025  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 01:03:09] d2.utils.events INFO:  eta: 22:04:42  iter: 1639  total_loss: 89.56  loss_ce: 3.44  loss_mask: 1.45  loss_dice: 4.284  loss_ce_0: 4.027  loss_mask_0: 1.253  loss_dice_0: 4.132  loss_ce_1: 2.997  loss_mask_1: 1.187  loss_dice_1: 4.153  loss_ce_2: 3.152  loss_mask_2: 1.275  loss_dice_2: 4.142  loss_ce_3: 3.162  loss_mask_3: 1.387  loss_dice_3: 4.097  loss_ce_4: 3.052  loss_mask_4: 1.28  loss_dice_4: 4.11  loss_ce_5: 3.222  loss_mask_5: 1.384  loss_dice_5: 4.124  loss_ce_6: 3.351  loss_mask_6: 1.364  loss_dice_6: 4.319  loss_ce_7: 3.368  loss_mask_7: 1.271  loss_dice_7: 4.15  loss_ce_8: 3.381  loss_mask_8: 1.383  loss_dice_8: 4.269  loss_mars: 0.2541    time: 5.7686  last_time: 6.0537  data_time: 0.0024  last_data_time: 0.0035   lr: 0.0001  max_mem: 0M
[10/19 01:05:06] d2.utils.events INFO:  eta: 22:05:07  iter: 1659  total_loss: 90.3  loss_ce: 3.714  loss_mask: 1.071  loss_dice: 4.3  loss_ce_0: 4.687  loss_mask_0: 0.981  loss_dice_0: 4.22  loss_ce_1: 3.227  loss_mask_1: 0.9974  loss_dice_1: 4.244  loss_ce_2: 3.432  loss_mask_2: 1.047  loss_dice_2: 4.284  loss_ce_3: 3.284  loss_mask_3: 1.04  loss_dice_3: 4.299  loss_ce_4: 3.392  loss_mask_4: 0.9645  loss_dice_4: 4.226  loss_ce_5: 3.555  loss_mask_5: 0.9824  loss_dice_5: 4.194  loss_ce_6: 3.808  loss_mask_6: 0.9529  loss_dice_6: 4.269  loss_ce_7: 3.928  loss_mask_7: 1.037  loss_dice_7: 4.26  loss_ce_8: 3.881  loss_mask_8: 1.012  loss_dice_8: 4.221  loss_mars: 0.8845    time: 5.7876  last_time: 5.1642  data_time: 0.0026  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 01:07:00] d2.utils.events INFO:  eta: 22:00:54  iter: 1679  total_loss: 89.04  loss_ce: 3.748  loss_mask: 0.9496  loss_dice: 4.019  loss_ce_0: 4.587  loss_mask_0: 0.8589  loss_dice_0: 3.897  loss_ce_1: 3.342  loss_mask_1: 0.9216  loss_dice_1: 3.987  loss_ce_2: 3.295  loss_mask_2: 0.9998  loss_dice_2: 3.894  loss_ce_3: 3.483  loss_mask_3: 0.9201  loss_dice_3: 3.888  loss_ce_4: 3.525  loss_mask_4: 0.9647  loss_dice_4: 3.797  loss_ce_5: 3.657  loss_mask_5: 1.03  loss_dice_5: 3.817  loss_ce_6: 3.971  loss_mask_6: 1.055  loss_dice_6: 3.906  loss_ce_7: 4.13  loss_mask_7: 0.9793  loss_dice_7: 3.884  loss_ce_8: 4.033  loss_mask_8: 0.8322  loss_dice_8: 3.805  loss_mars: 0.9017    time: 5.7701  last_time: 5.9498  data_time: 0.0025  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 01:08:52] d2.utils.events INFO:  eta: 22:03:55  iter: 1699  total_loss: 86.43  loss_ce: 3.202  loss_mask: 1.506  loss_dice: 3.88  loss_ce_0: 3.62  loss_mask_0: 1.561  loss_dice_0: 3.615  loss_ce_1: 2.777  loss_mask_1: 1.483  loss_dice_1: 3.741  loss_ce_2: 2.889  loss_mask_2: 1.489  loss_dice_2: 3.751  loss_ce_3: 2.915  loss_mask_3: 1.591  loss_dice_3: 3.716  loss_ce_4: 3.155  loss_mask_4: 1.583  loss_dice_4: 3.844  loss_ce_5: 2.877  loss_mask_5: 1.561  loss_dice_5: 3.663  loss_ce_6: 2.905  loss_mask_6: 1.592  loss_dice_6: 3.628  loss_ce_7: 3.094  loss_mask_7: 1.539  loss_dice_7: 3.74  loss_ce_8: 3.072  loss_mask_8: 1.711  loss_dice_8: 3.806  loss_mars: 0.7738    time: 5.7474  last_time: 5.3035  data_time: 0.0022  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 01:10:41] d2.utils.events INFO:  eta: 21:59:25  iter: 1719  total_loss: 99.05  loss_ce: 4.259  loss_mask: 0.8945  loss_dice: 4.414  loss_ce_0: 4.266  loss_mask_0: 0.8869  loss_dice_0: 4.296  loss_ce_1: 4.015  loss_mask_1: 0.8967  loss_dice_1: 4.342  loss_ce_2: 4.133  loss_mask_2: 0.8928  loss_dice_2: 4.286  loss_ce_3: 4.214  loss_mask_3: 0.9091  loss_dice_3: 4.306  loss_ce_4: 4.095  loss_mask_4: 0.9013  loss_dice_4: 4.325  loss_ce_5: 4.219  loss_mask_5: 0.893  loss_dice_5: 4.306  loss_ce_6: 3.986  loss_mask_6: 0.9511  loss_dice_6: 4.291  loss_ce_7: 4.096  loss_mask_7: 0.9335  loss_dice_7: 4.295  loss_ce_8: 4.346  loss_mask_8: 0.9844  loss_dice_8: 4.317  loss_mars: 0.8666    time: 5.7109  last_time: 5.3508  data_time: 0.0026  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 01:12:33] d2.utils.events INFO:  eta: 21:57:31  iter: 1739  total_loss: 87.61  loss_ce: 3.294  loss_mask: 1.351  loss_dice: 4.043  loss_ce_0: 3.46  loss_mask_0: 1.26  loss_dice_0: 4.034  loss_ce_1: 3.112  loss_mask_1: 1.461  loss_dice_1: 3.963  loss_ce_2: 3.009  loss_mask_2: 1.315  loss_dice_2: 3.912  loss_ce_3: 3.305  loss_mask_3: 1.142  loss_dice_3: 3.989  loss_ce_4: 3.534  loss_mask_4: 1.315  loss_dice_4: 4.099  loss_ce_5: 3.206  loss_mask_5: 1.374  loss_dice_5: 3.943  loss_ce_6: 3.425  loss_mask_6: 1.074  loss_dice_6: 3.979  loss_ce_7: 3.206  loss_mask_7: 1.183  loss_dice_7: 4.01  loss_ce_8: 3.091  loss_mask_8: 1.196  loss_dice_8: 4.056  loss_mars: 0.7726    time: 5.6966  last_time: 5.4520  data_time: 0.0024  last_data_time: 0.0041   lr: 0.0001  max_mem: 0M
[10/19 01:14:25] d2.utils.events INFO:  eta: 21:55:38  iter: 1759  total_loss: 85.11  loss_ce: 2.873  loss_mask: 1.478  loss_dice: 4.394  loss_ce_0: 3.234  loss_mask_0: 1.408  loss_dice_0: 4.171  loss_ce_1: 2.797  loss_mask_1: 1.336  loss_dice_1: 4.164  loss_ce_2: 2.915  loss_mask_2: 1.374  loss_dice_2: 4.202  loss_ce_3: 2.73  loss_mask_3: 1.36  loss_dice_3: 4.324  loss_ce_4: 2.614  loss_mask_4: 1.371  loss_dice_4: 4.379  loss_ce_5: 2.741  loss_mask_5: 1.364  loss_dice_5: 4.472  loss_ce_6: 2.741  loss_mask_6: 1.492  loss_dice_6: 4.469  loss_ce_7: 2.601  loss_mask_7: 1.611  loss_dice_7: 4.474  loss_ce_8: 2.763  loss_mask_8: 1.318  loss_dice_8: 4.474  loss_mars: 0.8306    time: 5.6883  last_time: 3.8122  data_time: 0.0024  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 01:16:19] d2.utils.events INFO:  eta: 21:55:18  iter: 1779  total_loss: 79.37  loss_ce: 2.761  loss_mask: 1.175  loss_dice: 3.795  loss_ce_0: 3.156  loss_mask_0: 1.137  loss_dice_0: 3.781  loss_ce_1: 2.464  loss_mask_1: 1.116  loss_dice_1: 3.836  loss_ce_2: 2.35  loss_mask_2: 1.141  loss_dice_2: 3.823  loss_ce_3: 2.743  loss_mask_3: 1.109  loss_dice_3: 3.905  loss_ce_4: 2.777  loss_mask_4: 1.097  loss_dice_4: 3.818  loss_ce_5: 2.735  loss_mask_5: 1.11  loss_dice_5: 3.853  loss_ce_6: 2.642  loss_mask_6: 1.146  loss_dice_6: 3.789  loss_ce_7: 2.594  loss_mask_7: 1.146  loss_dice_7: 3.873  loss_ce_8: 2.669  loss_mask_8: 1.137  loss_dice_8: 3.77  loss_mars: 0.9481    time: 5.6856  last_time: 5.9191  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 01:18:16] d2.utils.events INFO:  eta: 22:09:39  iter: 1799  total_loss: 89.24  loss_ce: 3.897  loss_mask: 0.7595  loss_dice: 4.38  loss_ce_0: 4.813  loss_mask_0: 0.6887  loss_dice_0: 4.409  loss_ce_1: 3.614  loss_mask_1: 0.8024  loss_dice_1: 4.365  loss_ce_2: 3.749  loss_mask_2: 0.7611  loss_dice_2: 4.332  loss_ce_3: 3.998  loss_mask_3: 0.7002  loss_dice_3: 4.31  loss_ce_4: 3.593  loss_mask_4: 0.6217  loss_dice_4: 4.335  loss_ce_5: 3.83  loss_mask_5: 0.7099  loss_dice_5: 4.361  loss_ce_6: 3.705  loss_mask_6: 0.6819  loss_dice_6: 4.4  loss_ce_7: 3.722  loss_mask_7: 0.7391  loss_dice_7: 4.466  loss_ce_8: 3.616  loss_mask_8: 0.7968  loss_dice_8: 4.473  loss_mars: 0.8617    time: 5.6995  last_time: 5.9240  data_time: 0.0029  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 01:20:07] d2.utils.events INFO:  eta: 22:02:36  iter: 1819  total_loss: 81.56  loss_ce: 2.685  loss_mask: 1.644  loss_dice: 3.91  loss_ce_0: 3.323  loss_mask_0: 1.448  loss_dice_0: 3.93  loss_ce_1: 2.701  loss_mask_1: 1.397  loss_dice_1: 3.987  loss_ce_2: 2.758  loss_mask_2: 1.341  loss_dice_2: 3.864  loss_ce_3: 2.559  loss_mask_3: 1.458  loss_dice_3: 3.881  loss_ce_4: 2.554  loss_mask_4: 1.467  loss_dice_4: 3.824  loss_ce_5: 2.44  loss_mask_5: 1.458  loss_dice_5: 4.166  loss_ce_6: 2.494  loss_mask_6: 1.467  loss_dice_6: 3.984  loss_ce_7: 2.387  loss_mask_7: 1.58  loss_dice_7: 4.008  loss_ce_8: 2.404  loss_mask_8: 1.41  loss_dice_8: 4.074  loss_mars: 0.7259    time: 5.6898  last_time: 5.2433  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 01:22:00] d2.utils.events INFO:  eta: 22:00:41  iter: 1839  total_loss: 86.43  loss_ce: 2.489  loss_mask: 1.113  loss_dice: 4.402  loss_ce_0: 3.225  loss_mask_0: 1.008  loss_dice_0: 3.949  loss_ce_1: 2.43  loss_mask_1: 1.025  loss_dice_1: 3.922  loss_ce_2: 2.346  loss_mask_2: 1.009  loss_dice_2: 4.05  loss_ce_3: 2.468  loss_mask_3: 1.066  loss_dice_3: 4.021  loss_ce_4: 2.534  loss_mask_4: 0.9564  loss_dice_4: 3.952  loss_ce_5: 2.384  loss_mask_5: 1.209  loss_dice_5: 4.078  loss_ce_6: 2.158  loss_mask_6: 1.001  loss_dice_6: 4.093  loss_ce_7: 2.158  loss_mask_7: 1.011  loss_dice_7: 4.366  loss_ce_8: 2.223  loss_mask_8: 1.066  loss_dice_8: 4.537  loss_mars: 0.6433    time: 5.6878  last_time: 5.4008  data_time: 0.0025  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 01:23:52] d2.utils.events INFO:  eta: 21:52:23  iter: 1859  total_loss: 88.12  loss_ce: 3.617  loss_mask: 1.359  loss_dice: 4.206  loss_ce_0: 4.071  loss_mask_0: 1.539  loss_dice_0: 4.104  loss_ce_1: 3.291  loss_mask_1: 1.267  loss_dice_1: 4.019  loss_ce_2: 3.272  loss_mask_2: 1.318  loss_dice_2: 4.06  loss_ce_3: 3.66  loss_mask_3: 1.302  loss_dice_3: 3.926  loss_ce_4: 3.57  loss_mask_4: 1.339  loss_dice_4: 3.991  loss_ce_5: 3.468  loss_mask_5: 1.376  loss_dice_5: 4.164  loss_ce_6: 3.715  loss_mask_6: 1.407  loss_dice_6: 4.078  loss_ce_7: 3.437  loss_mask_7: 1.438  loss_dice_7: 3.983  loss_ce_8: 3.504  loss_mask_8: 1.353  loss_dice_8: 4.102  loss_mars: 0.8075    time: 5.6792  last_time: 5.3989  data_time: 0.0025  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 01:25:46] d2.utils.events INFO:  eta: 21:45:48  iter: 1879  total_loss: 82.09  loss_ce: 3.384  loss_mask: 0.7416  loss_dice: 4.268  loss_ce_0: 3.851  loss_mask_0: 0.6962  loss_dice_0: 4.245  loss_ce_1: 2.892  loss_mask_1: 0.7145  loss_dice_1: 4.17  loss_ce_2: 3.184  loss_mask_2: 0.7791  loss_dice_2: 4.168  loss_ce_3: 3.052  loss_mask_3: 0.7804  loss_dice_3: 4.243  loss_ce_4: 2.908  loss_mask_4: 0.7878  loss_dice_4: 4.198  loss_ce_5: 2.908  loss_mask_5: 0.7354  loss_dice_5: 4.26  loss_ce_6: 2.958  loss_mask_6: 0.7212  loss_dice_6: 4.256  loss_ce_7: 2.785  loss_mask_7: 0.7312  loss_dice_7: 4.334  loss_ce_8: 3.175  loss_mask_8: 0.7723  loss_dice_8: 4.125  loss_mars: 0.8048    time: 5.6800  last_time: 5.5871  data_time: 0.0025  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 01:27:36] d2.utils.events INFO:  eta: 21:40:04  iter: 1899  total_loss: 98.88  loss_ce: 4.644  loss_mask: 1.065  loss_dice: 4.543  loss_ce_0: 5.437  loss_mask_0: 1.037  loss_dice_0: 4.377  loss_ce_1: 4.507  loss_mask_1: 1.048  loss_dice_1: 4.394  loss_ce_2: 4.527  loss_mask_2: 1.041  loss_dice_2: 4.453  loss_ce_3: 4.178  loss_mask_3: 1.134  loss_dice_3: 4.46  loss_ce_4: 4.236  loss_mask_4: 1.1  loss_dice_4: 4.519  loss_ce_5: 4.46  loss_mask_5: 1.113  loss_dice_5: 4.369  loss_ce_6: 4.122  loss_mask_6: 1.189  loss_dice_6: 4.44  loss_ce_7: 4.24  loss_mask_7: 1.172  loss_dice_7: 4.418  loss_ce_8: 4.3  loss_mask_8: 1.181  loss_dice_8: 4.404  loss_mars: 0.8033    time: 5.6704  last_time: 6.2543  data_time: 0.0028  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 01:29:32] d2.utils.events INFO:  eta: 21:33:45  iter: 1919  total_loss: 93.58  loss_ce: 3.569  loss_mask: 1.268  loss_dice: 4.331  loss_ce_0: 4.134  loss_mask_0: 1.11  loss_dice_0: 4.246  loss_ce_1: 3.818  loss_mask_1: 1.126  loss_dice_1: 4.29  loss_ce_2: 3.511  loss_mask_2: 1.156  loss_dice_2: 4.365  loss_ce_3: 3.261  loss_mask_3: 1.147  loss_dice_3: 4.535  loss_ce_4: 3.286  loss_mask_4: 1.085  loss_dice_4: 4.327  loss_ce_5: 3.234  loss_mask_5: 1.1  loss_dice_5: 4.473  loss_ce_6: 3.35  loss_mask_6: 1.133  loss_dice_6: 4.371  loss_ce_7: 3.315  loss_mask_7: 1.241  loss_dice_7: 4.419  loss_ce_8: 3.4  loss_mask_8: 1.289  loss_dice_8: 4.357  loss_mars: 0.6124    time: 5.6775  last_time: 5.4679  data_time: 0.0025  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 01:31:28] d2.utils.events INFO:  eta: 21:39:36  iter: 1939  total_loss: 86.33  loss_ce: 3.207  loss_mask: 1.046  loss_dice: 4.258  loss_ce_0: 4.154  loss_mask_0: 0.9328  loss_dice_0: 4.151  loss_ce_1: 3.413  loss_mask_1: 1.049  loss_dice_1: 4.032  loss_ce_2: 3.462  loss_mask_2: 0.9661  loss_dice_2: 4.088  loss_ce_3: 3.32  loss_mask_3: 0.9972  loss_dice_3: 4.176  loss_ce_4: 3.159  loss_mask_4: 0.9067  loss_dice_4: 4.312  loss_ce_5: 3.076  loss_mask_5: 0.9768  loss_dice_5: 4.407  loss_ce_6: 3.088  loss_mask_6: 1.084  loss_dice_6: 4.395  loss_ce_7: 2.925  loss_mask_7: 1.049  loss_dice_7: 4.284  loss_ce_8: 3.174  loss_mask_8: 0.8904  loss_dice_8: 4.308  loss_mars: 0.5944    time: 5.6845  last_time: 5.2925  data_time: 0.0027  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 01:33:20] d2.utils.events INFO:  eta: 21:36:39  iter: 1959  total_loss: 90.45  loss_ce: 2.551  loss_mask: 1.88  loss_dice: 3.794  loss_ce_0: 3.387  loss_mask_0: 1.722  loss_dice_0: 3.816  loss_ce_1: 2.763  loss_mask_1: 2.041  loss_dice_1: 3.919  loss_ce_2: 2.888  loss_mask_2: 1.733  loss_dice_2: 3.833  loss_ce_3: 2.778  loss_mask_3: 1.604  loss_dice_3: 3.928  loss_ce_4: 2.585  loss_mask_4: 1.713  loss_dice_4: 3.854  loss_ce_5: 2.791  loss_mask_5: 1.755  loss_dice_5: 4.13  loss_ce_6: 2.675  loss_mask_6: 1.694  loss_dice_6: 4.007  loss_ce_7: 2.657  loss_mask_7: 1.739  loss_dice_7: 4.116  loss_ce_8: 2.556  loss_mask_8: 1.692  loss_dice_8: 3.963  loss_mars: 0.4946    time: 5.6810  last_time: 6.0510  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 01:35:19] d2.utils.events INFO:  eta: 21:35:48  iter: 1979  total_loss: 86.26  loss_ce: 3.006  loss_mask: 0.9162  loss_dice: 4.37  loss_ce_0: 3.526  loss_mask_0: 1.057  loss_dice_0: 4.384  loss_ce_1: 2.988  loss_mask_1: 1.028  loss_dice_1: 4.348  loss_ce_2: 3.02  loss_mask_2: 1.046  loss_dice_2: 4.332  loss_ce_3: 3.004  loss_mask_3: 1.072  loss_dice_3: 4.362  loss_ce_4: 2.89  loss_mask_4: 0.9551  loss_dice_4: 4.471  loss_ce_5: 3.122  loss_mask_5: 0.9161  loss_dice_5: 4.339  loss_ce_6: 2.895  loss_mask_6: 0.961  loss_dice_6: 4.422  loss_ce_7: 3.038  loss_mask_7: 0.9023  loss_dice_7: 4.44  loss_ce_8: 3.09  loss_mask_8: 0.8989  loss_dice_8: 4.42  loss_mars: 0.7922    time: 5.6923  last_time: 5.3385  data_time: 0.0023  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 01:37:10] d2.utils.events INFO:  eta: 21:30:36  iter: 1999  total_loss: 89.39  loss_ce: 3.258  loss_mask: 1.184  loss_dice: 4.32  loss_ce_0: 3.76  loss_mask_0: 1.224  loss_dice_0: 4.34  loss_ce_1: 3.18  loss_mask_1: 1.144  loss_dice_1: 4.418  loss_ce_2: 2.916  loss_mask_2: 1.209  loss_dice_2: 4.407  loss_ce_3: 2.936  loss_mask_3: 1.195  loss_dice_3: 4.409  loss_ce_4: 2.974  loss_mask_4: 1.182  loss_dice_4: 4.451  loss_ce_5: 3.133  loss_mask_5: 1.136  loss_dice_5: 4.367  loss_ce_6: 3.08  loss_mask_6: 1.167  loss_dice_6: 4.402  loss_ce_7: 3.183  loss_mask_7: 1.173  loss_dice_7: 4.348  loss_ce_8: 3.142  loss_mask_8: 1.172  loss_dice_8: 4.367  loss_mars: 0.6805    time: 5.6868  last_time: 5.5998  data_time: 0.0025  last_data_time: 0.0036   lr: 0.0001  max_mem: 0M
[10/19 01:39:04] d2.utils.events INFO:  eta: 21:32:00  iter: 2019  total_loss: 90.29  loss_ce: 3.523  loss_mask: 1.17  loss_dice: 4.259  loss_ce_0: 4.062  loss_mask_0: 0.8888  loss_dice_0: 4.401  loss_ce_1: 3.395  loss_mask_1: 1.046  loss_dice_1: 4.4  loss_ce_2: 3.447  loss_mask_2: 0.9512  loss_dice_2: 4.4  loss_ce_3: 3.386  loss_mask_3: 0.9333  loss_dice_3: 4.397  loss_ce_4: 3.385  loss_mask_4: 0.9557  loss_dice_4: 4.371  loss_ce_5: 3.418  loss_mask_5: 0.9203  loss_dice_5: 4.482  loss_ce_6: 3.344  loss_mask_6: 0.9659  loss_dice_6: 4.529  loss_ce_7: 3.402  loss_mask_7: 1.009  loss_dice_7: 4.427  loss_ce_8: 3.479  loss_mask_8: 0.9351  loss_dice_8: 4.395  loss_mars: 0.4602    time: 5.6875  last_time: 5.1498  data_time: 0.0027  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 01:40:59] d2.utils.events INFO:  eta: 21:30:36  iter: 2039  total_loss: 85.34  loss_ce: 2.936  loss_mask: 1.141  loss_dice: 4.303  loss_ce_0: 4.174  loss_mask_0: 1.098  loss_dice_0: 4.202  loss_ce_1: 3.075  loss_mask_1: 1.267  loss_dice_1: 4.202  loss_ce_2: 2.789  loss_mask_2: 1.157  loss_dice_2: 4.218  loss_ce_3: 2.655  loss_mask_3: 1.105  loss_dice_3: 4.311  loss_ce_4: 2.564  loss_mask_4: 1.031  loss_dice_4: 4.278  loss_ce_5: 2.633  loss_mask_5: 1.083  loss_dice_5: 4.344  loss_ce_6: 2.644  loss_mask_6: 1.139  loss_dice_6: 4.283  loss_ce_7: 2.776  loss_mask_7: 1.181  loss_dice_7: 4.244  loss_ce_8: 2.949  loss_mask_8: 1.107  loss_dice_8: 4.342  loss_mars: 0.4347    time: 5.6892  last_time: 6.4188  data_time: 0.0027  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 01:42:57] d2.utils.events INFO:  eta: 21:29:41  iter: 2059  total_loss: 92.1  loss_ce: 4.07  loss_mask: 0.8812  loss_dice: 4.085  loss_ce_0: 4.628  loss_mask_0: 1.01  loss_dice_0: 4.16  loss_ce_1: 4.013  loss_mask_1: 1.059  loss_dice_1: 4.21  loss_ce_2: 3.754  loss_mask_2: 0.9204  loss_dice_2: 4.113  loss_ce_3: 3.78  loss_mask_3: 0.9723  loss_dice_3: 4.147  loss_ce_4: 3.802  loss_mask_4: 0.9761  loss_dice_4: 4.098  loss_ce_5: 3.757  loss_mask_5: 1.004  loss_dice_5: 4.109  loss_ce_6: 3.861  loss_mask_6: 0.9267  loss_dice_6: 4.188  loss_ce_7: 3.939  loss_mask_7: 0.9681  loss_dice_7: 4.046  loss_ce_8: 3.989  loss_mask_8: 0.9434  loss_dice_8: 4.049  loss_mars: 0.5153    time: 5.6982  last_time: 5.3482  data_time: 0.0027  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 01:44:50] d2.utils.events INFO:  eta: 21:26:18  iter: 2079  total_loss: 92.68  loss_ce: 3.464  loss_mask: 1.102  loss_dice: 4.183  loss_ce_0: 3.97  loss_mask_0: 1.228  loss_dice_0: 4.096  loss_ce_1: 3.446  loss_mask_1: 1.304  loss_dice_1: 4.089  loss_ce_2: 3.396  loss_mask_2: 1.192  loss_dice_2: 4.196  loss_ce_3: 3.303  loss_mask_3: 1.203  loss_dice_3: 4.119  loss_ce_4: 3.297  loss_mask_4: 1.239  loss_dice_4: 4.142  loss_ce_5: 3.38  loss_mask_5: 1.208  loss_dice_5: 4.019  loss_ce_6: 3.518  loss_mask_6: 1.088  loss_dice_6: 4.187  loss_ce_7: 3.632  loss_mask_7: 1.086  loss_dice_7: 4.156  loss_ce_8: 3.663  loss_mask_8: 1.097  loss_dice_8: 4.047  loss_mars: 0.7783    time: 5.6965  last_time: 5.4496  data_time: 0.0026  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 01:46:43] d2.utils.events INFO:  eta: 21:23:22  iter: 2099  total_loss: 84.06  loss_ce: 2.778  loss_mask: 2.004  loss_dice: 3.711  loss_ce_0: 3.444  loss_mask_0: 2.063  loss_dice_0: 3.668  loss_ce_1: 3.016  loss_mask_1: 2.018  loss_dice_1: 3.534  loss_ce_2: 2.614  loss_mask_2: 2.141  loss_dice_2: 3.7  loss_ce_3: 2.563  loss_mask_3: 2.094  loss_dice_3: 3.602  loss_ce_4: 2.639  loss_mask_4: 1.964  loss_dice_4: 3.585  loss_ce_5: 2.687  loss_mask_5: 1.856  loss_dice_5: 3.52  loss_ce_6: 2.696  loss_mask_6: 1.899  loss_dice_6: 3.466  loss_ce_7: 2.636  loss_mask_7: 1.927  loss_dice_7: 3.556  loss_ce_8: 2.746  loss_mask_8: 2.043  loss_dice_8: 3.538  loss_mars: 0.8809    time: 5.6938  last_time: 6.0933  data_time: 0.0023  last_data_time: 0.0038   lr: 0.0001  max_mem: 0M
[10/19 01:48:40] d2.utils.events INFO:  eta: 21:22:30  iter: 2119  total_loss: 90.96  loss_ce: 4.028  loss_mask: 1.522  loss_dice: 4.095  loss_ce_0: 3.961  loss_mask_0: 1.407  loss_dice_0: 3.95  loss_ce_1: 3.859  loss_mask_1: 1.498  loss_dice_1: 3.74  loss_ce_2: 3.523  loss_mask_2: 1.522  loss_dice_2: 3.969  loss_ce_3: 3.862  loss_mask_3: 1.466  loss_dice_3: 3.858  loss_ce_4: 3.925  loss_mask_4: 1.551  loss_dice_4: 3.811  loss_ce_5: 3.77  loss_mask_5: 1.52  loss_dice_5: 3.775  loss_ce_6: 4.087  loss_mask_6: 1.403  loss_dice_6: 4.015  loss_ce_7: 4.056  loss_mask_7: 1.432  loss_dice_7: 4.117  loss_ce_8: 3.889  loss_mask_8: 1.404  loss_dice_8: 4.073  loss_mars: 0.7575    time: 5.6989  last_time: 5.5385  data_time: 0.0026  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 01:50:33] d2.utils.events INFO:  eta: 21:20:36  iter: 2139  total_loss: 90.14  loss_ce: 3.16  loss_mask: 1.612  loss_dice: 3.722  loss_ce_0: 3.562  loss_mask_0: 1.512  loss_dice_0: 3.73  loss_ce_1: 3.172  loss_mask_1: 1.706  loss_dice_1: 3.748  loss_ce_2: 3.026  loss_mask_2: 1.763  loss_dice_2: 3.725  loss_ce_3: 2.952  loss_mask_3: 1.66  loss_dice_3: 3.699  loss_ce_4: 3.068  loss_mask_4: 1.726  loss_dice_4: 3.731  loss_ce_5: 3.043  loss_mask_5: 1.918  loss_dice_5: 3.804  loss_ce_6: 2.944  loss_mask_6: 1.752  loss_dice_6: 3.748  loss_ce_7: 3.136  loss_mask_7: 1.695  loss_dice_7: 3.874  loss_ce_8: 3.082  loss_mask_8: 1.737  loss_dice_8: 3.858  loss_mars: 0.8038    time: 5.6984  last_time: 6.2473  data_time: 0.0027  last_data_time: 0.0102   lr: 0.0001  max_mem: 0M
[10/19 01:52:28] d2.utils.events INFO:  eta: 21:17:40  iter: 2159  total_loss: 89.55  loss_ce: 2.536  loss_mask: 1.762  loss_dice: 4.084  loss_ce_0: 3.111  loss_mask_0: 1.789  loss_dice_0: 3.908  loss_ce_1: 2.366  loss_mask_1: 1.789  loss_dice_1: 4.124  loss_ce_2: 2.318  loss_mask_2: 1.745  loss_dice_2: 4.073  loss_ce_3: 2.489  loss_mask_3: 1.594  loss_dice_3: 4.218  loss_ce_4: 2.367  loss_mask_4: 2.009  loss_dice_4: 4.567  loss_ce_5: 2.245  loss_mask_5: 1.939  loss_dice_5: 4.56  loss_ce_6: 2.328  loss_mask_6: 1.562  loss_dice_6: 4.561  loss_ce_7: 2.339  loss_mask_7: 1.804  loss_dice_7: 4.466  loss_ce_8: 2.408  loss_mask_8: 1.719  loss_dice_8: 4.13  loss_mars: 0.3679    time: 5.6996  last_time: 5.4939  data_time: 0.0022  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 01:54:20] d2.utils.events INFO:  eta: 21:09:35  iter: 2179  total_loss: 84.16  loss_ce: 2.52  loss_mask: 1.216  loss_dice: 3.966  loss_ce_0: 3.24  loss_mask_0: 1.24  loss_dice_0: 4.096  loss_ce_1: 2.365  loss_mask_1: 1.225  loss_dice_1: 4.066  loss_ce_2: 2.234  loss_mask_2: 1.449  loss_dice_2: 3.804  loss_ce_3: 2.309  loss_mask_3: 1.234  loss_dice_3: 4.113  loss_ce_4: 2.562  loss_mask_4: 1.311  loss_dice_4: 4.301  loss_ce_5: 2.521  loss_mask_5: 1.398  loss_dice_5: 4.333  loss_ce_6: 2.488  loss_mask_6: 1.222  loss_dice_6: 4.263  loss_ce_7: 2.502  loss_mask_7: 1.451  loss_dice_7: 4.2  loss_ce_8: 2.646  loss_mask_8: 1.217  loss_dice_8: 4.121  loss_mars: 0.6714    time: 5.6957  last_time: 5.3333  data_time: 0.0025  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 01:56:16] d2.utils.events INFO:  eta: 21:07:19  iter: 2199  total_loss: 89.54  loss_ce: 3.874  loss_mask: 1.084  loss_dice: 4.322  loss_ce_0: 4.158  loss_mask_0: 0.9507  loss_dice_0: 4.254  loss_ce_1: 3.299  loss_mask_1: 1.06  loss_dice_1: 4.275  loss_ce_2: 2.951  loss_mask_2: 1.098  loss_dice_2: 4.44  loss_ce_3: 3.102  loss_mask_3: 0.8948  loss_dice_3: 4.393  loss_ce_4: 3.07  loss_mask_4: 0.9553  loss_dice_4: 4.33  loss_ce_5: 3.398  loss_mask_5: 1.051  loss_dice_5: 4.303  loss_ce_6: 3.494  loss_mask_6: 1.064  loss_dice_6: 4.218  loss_ce_7: 3.589  loss_mask_7: 1.03  loss_dice_7: 4.291  loss_ce_8: 3.761  loss_mask_8: 1.068  loss_dice_8: 4.344  loss_mars: 0.6019    time: 5.6998  last_time: 6.0168  data_time: 0.0025  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 01:58:11] d2.utils.events INFO:  eta: 21:05:26  iter: 2219  total_loss: 90.98  loss_ce: 3.366  loss_mask: 1.254  loss_dice: 4.117  loss_ce_0: 3.798  loss_mask_0: 1.17  loss_dice_0: 4.2  loss_ce_1: 3.144  loss_mask_1: 1.239  loss_dice_1: 4.101  loss_ce_2: 3.068  loss_mask_2: 1.287  loss_dice_2: 4.217  loss_ce_3: 3.094  loss_mask_3: 1.302  loss_dice_3: 4.203  loss_ce_4: 2.974  loss_mask_4: 1.345  loss_dice_4: 4.204  loss_ce_5: 3.034  loss_mask_5: 1.364  loss_dice_5: 4.3  loss_ce_6: 3.091  loss_mask_6: 1.35  loss_dice_6: 4.303  loss_ce_7: 3.107  loss_mask_7: 1.344  loss_dice_7: 4.258  loss_ce_8: 3.141  loss_mask_8: 1.492  loss_dice_8: 4.285  loss_mars: 0.475    time: 5.7009  last_time: 7.8124  data_time: 0.0025  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 02:00:03] d2.utils.events INFO:  eta: 21:03:07  iter: 2239  total_loss: 90.26  loss_ce: 4.38  loss_mask: 0.9523  loss_dice: 4.321  loss_ce_0: 4.776  loss_mask_0: 0.8835  loss_dice_0: 4.358  loss_ce_1: 3.647  loss_mask_1: 0.985  loss_dice_1: 4.302  loss_ce_2: 3.431  loss_mask_2: 0.9164  loss_dice_2: 4.374  loss_ce_3: 3.603  loss_mask_3: 0.951  loss_dice_3: 4.428  loss_ce_4: 3.476  loss_mask_4: 1.006  loss_dice_4: 4.46  loss_ce_5: 3.679  loss_mask_5: 0.9163  loss_dice_5: 4.466  loss_ce_6: 3.669  loss_mask_6: 0.9698  loss_dice_6: 4.387  loss_ce_7: 3.728  loss_mask_7: 0.9455  loss_dice_7: 4.307  loss_ce_8: 4.027  loss_mask_8: 0.918  loss_dice_8: 4.377  loss_mars: 0.8104    time: 5.6980  last_time: 5.0542  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 02:01:56] d2.utils.events INFO:  eta: 21:01:11  iter: 2259  total_loss: 89.7  loss_ce: 3.374  loss_mask: 1.079  loss_dice: 4.222  loss_ce_0: 4.472  loss_mask_0: 0.9997  loss_dice_0: 4.172  loss_ce_1: 3.368  loss_mask_1: 0.9489  loss_dice_1: 4.103  loss_ce_2: 3.123  loss_mask_2: 1.202  loss_dice_2: 4.236  loss_ce_3: 3.004  loss_mask_3: 1.108  loss_dice_3: 4.181  loss_ce_4: 3.076  loss_mask_4: 1.184  loss_dice_4: 4.207  loss_ce_5: 3.144  loss_mask_5: 1.163  loss_dice_5: 4.206  loss_ce_6: 3.221  loss_mask_6: 1.136  loss_dice_6: 4.113  loss_ce_7: 3.587  loss_mask_7: 1.032  loss_dice_7: 4.147  loss_ce_8: 3.648  loss_mask_8: 1.188  loss_dice_8: 4.198  loss_mars: 0.888    time: 5.6964  last_time: 5.6064  data_time: 0.0026  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/19 02:03:50] d2.utils.events INFO:  eta: 20:59:09  iter: 2279  total_loss: 83.49  loss_ce: 3.001  loss_mask: 0.8696  loss_dice: 4.49  loss_ce_0: 3.835  loss_mask_0: 0.8277  loss_dice_0: 4.341  loss_ce_1: 2.971  loss_mask_1: 0.9184  loss_dice_1: 4.264  loss_ce_2: 2.658  loss_mask_2: 0.8732  loss_dice_2: 4.343  loss_ce_3: 2.918  loss_mask_3: 0.8751  loss_dice_3: 4.392  loss_ce_4: 2.975  loss_mask_4: 0.8353  loss_dice_4: 4.289  loss_ce_5: 2.865  loss_mask_5: 0.862  loss_dice_5: 4.27  loss_ce_6: 2.635  loss_mask_6: 0.8996  loss_dice_6: 4.264  loss_ce_7: 2.641  loss_mask_7: 0.9378  loss_dice_7: 4.407  loss_ce_8: 3.031  loss_mask_8: 0.85  loss_dice_8: 4.329  loss_mars: 0.8755    time: 5.6967  last_time: 5.8997  data_time: 0.0026  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 02:05:44] d2.utils.events INFO:  eta: 20:56:02  iter: 2299  total_loss: 85.97  loss_ce: 2.918  loss_mask: 1.271  loss_dice: 3.911  loss_ce_0: 3.16  loss_mask_0: 1.077  loss_dice_0: 4.236  loss_ce_1: 2.678  loss_mask_1: 1.325  loss_dice_1: 4  loss_ce_2: 2.682  loss_mask_2: 1.067  loss_dice_2: 4.055  loss_ce_3: 2.975  loss_mask_3: 1.142  loss_dice_3: 4.086  loss_ce_4: 3.02  loss_mask_4: 1.229  loss_dice_4: 3.939  loss_ce_5: 3.134  loss_mask_5: 1.313  loss_dice_5: 4.183  loss_ce_6: 3.168  loss_mask_6: 1.344  loss_dice_6: 4.267  loss_ce_7: 3.178  loss_mask_7: 1.213  loss_dice_7: 4.142  loss_ce_8: 3.118  loss_mask_8: 1.146  loss_dice_8: 4.084  loss_mars: 0.9719    time: 5.6963  last_time: 4.0069  data_time: 0.0024  last_data_time: 0.0039   lr: 0.0001  max_mem: 0M
[10/19 02:07:39] d2.utils.events INFO:  eta: 20:53:45  iter: 2319  total_loss: 94.04  loss_ce: 3.922  loss_mask: 1.243  loss_dice: 4.018  loss_ce_0: 4.388  loss_mask_0: 1.25  loss_dice_0: 4.099  loss_ce_1: 3.161  loss_mask_1: 1.317  loss_dice_1: 4.071  loss_ce_2: 3.25  loss_mask_2: 1.241  loss_dice_2: 4.062  loss_ce_3: 3.335  loss_mask_3: 1.187  loss_dice_3: 4.123  loss_ce_4: 3.43  loss_mask_4: 1.228  loss_dice_4: 4.082  loss_ce_5: 3.834  loss_mask_5: 1.22  loss_dice_5: 4.082  loss_ce_6: 4.229  loss_mask_6: 1.261  loss_dice_6: 4.025  loss_ce_7: 4.348  loss_mask_7: 1.279  loss_dice_7: 4.037  loss_ce_8: 3.955  loss_mask_8: 1.154  loss_dice_8: 4.037  loss_mars: 0.8989    time: 5.6977  last_time: 5.5487  data_time: 0.0025  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 02:09:35] d2.utils.events INFO:  eta: 20:53:29  iter: 2339  total_loss: 91.43  loss_ce: 3.502  loss_mask: 1.506  loss_dice: 3.998  loss_ce_0: 4.024  loss_mask_0: 1.448  loss_dice_0: 3.849  loss_ce_1: 3.139  loss_mask_1: 1.651  loss_dice_1: 3.949  loss_ce_2: 3.282  loss_mask_2: 1.4  loss_dice_2: 3.923  loss_ce_3: 3.468  loss_mask_3: 1.433  loss_dice_3: 3.992  loss_ce_4: 3.205  loss_mask_4: 1.421  loss_dice_4: 4.021  loss_ce_5: 3.21  loss_mask_5: 1.633  loss_dice_5: 4.01  loss_ce_6: 3.082  loss_mask_6: 1.698  loss_dice_6: 3.991  loss_ce_7: 3.421  loss_mask_7: 1.529  loss_dice_7: 3.973  loss_ce_8: 3.508  loss_mask_8: 1.462  loss_dice_8: 3.832  loss_mars: 0.708    time: 5.7008  last_time: 5.4286  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 02:11:32] d2.utils.events INFO:  eta: 20:51:47  iter: 2359  total_loss: 89.73  loss_ce: 2.952  loss_mask: 1.326  loss_dice: 4.145  loss_ce_0: 3.947  loss_mask_0: 1.56  loss_dice_0: 4.22  loss_ce_1: 2.992  loss_mask_1: 1.498  loss_dice_1: 4.018  loss_ce_2: 3.049  loss_mask_2: 1.416  loss_dice_2: 4.123  loss_ce_3: 3.048  loss_mask_3: 1.35  loss_dice_3: 4.114  loss_ce_4: 3.046  loss_mask_4: 1.521  loss_dice_4: 4.285  loss_ce_5: 3.083  loss_mask_5: 1.631  loss_dice_5: 4.249  loss_ce_6: 3.162  loss_mask_6: 1.517  loss_dice_6: 4.299  loss_ce_7: 3.029  loss_mask_7: 1.362  loss_dice_7: 4.131  loss_ce_8: 3.045  loss_mask_8: 1.612  loss_dice_8: 4.041  loss_mars: 0.9438    time: 5.7040  last_time: 6.2262  data_time: 0.0024  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 02:13:27] d2.utils.events INFO:  eta: 20:49:51  iter: 2379  total_loss: 91.97  loss_ce: 3.868  loss_mask: 1.626  loss_dice: 4.3  loss_ce_0: 4.102  loss_mask_0: 1.295  loss_dice_0: 4.208  loss_ce_1: 3.454  loss_mask_1: 1.381  loss_dice_1: 4.057  loss_ce_2: 3.484  loss_mask_2: 1.279  loss_dice_2: 4.093  loss_ce_3: 3.681  loss_mask_3: 1.373  loss_dice_3: 4.031  loss_ce_4: 3.584  loss_mask_4: 1.326  loss_dice_4: 4.031  loss_ce_5: 3.568  loss_mask_5: 1.401  loss_dice_5: 4.152  loss_ce_6: 3.45  loss_mask_6: 1.378  loss_dice_6: 4.147  loss_ce_7: 3.539  loss_mask_7: 1.369  loss_dice_7: 3.943  loss_ce_8: 3.647  loss_mask_8: 1.625  loss_dice_8: 4.232  loss_mars: 0.7183    time: 5.7055  last_time: 5.3657  data_time: 0.0025  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 02:15:25] d2.utils.events INFO:  eta: 20:48:26  iter: 2399  total_loss: 95.98  loss_ce: 4.121  loss_mask: 1.797  loss_dice: 4.176  loss_ce_0: 4.264  loss_mask_0: 1.614  loss_dice_0: 4.073  loss_ce_1: 3.722  loss_mask_1: 1.491  loss_dice_1: 4.151  loss_ce_2: 3.639  loss_mask_2: 1.333  loss_dice_2: 4.053  loss_ce_3: 3.537  loss_mask_3: 1.513  loss_dice_3: 4.183  loss_ce_4: 3.714  loss_mask_4: 1.516  loss_dice_4: 4.162  loss_ce_5: 3.732  loss_mask_5: 1.524  loss_dice_5: 4.179  loss_ce_6: 3.746  loss_mask_6: 1.492  loss_dice_6: 4.186  loss_ce_7: 3.817  loss_mask_7: 1.824  loss_dice_7: 4.204  loss_ce_8: 4.117  loss_mask_8: 1.585  loss_dice_8: 4.297  loss_mars: 0.7471    time: 5.7098  last_time: 6.0133  data_time: 0.0027  last_data_time: 0.0048   lr: 0.0001  max_mem: 0M
[10/19 02:17:20] d2.utils.events INFO:  eta: 20:46:33  iter: 2419  total_loss: 90.8  loss_ce: 3.021  loss_mask: 1.41  loss_dice: 4.294  loss_ce_0: 3.624  loss_mask_0: 1.335  loss_dice_0: 4.279  loss_ce_1: 3.046  loss_mask_1: 1.347  loss_dice_1: 4.149  loss_ce_2: 2.807  loss_mask_2: 1.255  loss_dice_2: 4.376  loss_ce_3: 2.76  loss_mask_3: 1.39  loss_dice_3: 4.36  loss_ce_4: 2.837  loss_mask_4: 1.335  loss_dice_4: 4.419  loss_ce_5: 2.925  loss_mask_5: 1.378  loss_dice_5: 4.291  loss_ce_6: 3.006  loss_mask_6: 1.427  loss_dice_6: 4.286  loss_ce_7: 3.023  loss_mask_7: 1.395  loss_dice_7: 4.334  loss_ce_8: 3.018  loss_mask_8: 1.437  loss_dice_8: 4.326  loss_mars: 0.5552    time: 5.7108  last_time: 5.9513  data_time: 0.0024  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 02:19:14] d2.utils.events INFO:  eta: 20:44:14  iter: 2439  total_loss: 84.87  loss_ce: 3.72  loss_mask: 1.396  loss_dice: 3.886  loss_ce_0: 4.052  loss_mask_0: 1.374  loss_dice_0: 3.947  loss_ce_1: 3.162  loss_mask_1: 1.376  loss_dice_1: 3.893  loss_ce_2: 3.533  loss_mask_2: 1.432  loss_dice_2: 3.995  loss_ce_3: 3.459  loss_mask_3: 1.447  loss_dice_3: 3.934  loss_ce_4: 3.447  loss_mask_4: 1.286  loss_dice_4: 3.867  loss_ce_5: 3.381  loss_mask_5: 1.269  loss_dice_5: 3.97  loss_ce_6: 3.569  loss_mask_6: 1.412  loss_dice_6: 3.921  loss_ce_7: 3.744  loss_mask_7: 1.35  loss_dice_7: 3.83  loss_ce_8: 3.727  loss_mask_8: 1.501  loss_dice_8: 4.026  loss_mars: 0.7329    time: 5.7114  last_time: 6.1054  data_time: 0.0022  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 02:21:10] d2.utils.events INFO:  eta: 20:42:18  iter: 2459  total_loss: 91.58  loss_ce: 3.044  loss_mask: 0.8616  loss_dice: 4.451  loss_ce_0: 3.466  loss_mask_0: 0.8191  loss_dice_0: 4.47  loss_ce_1: 2.925  loss_mask_1: 0.8355  loss_dice_1: 4.458  loss_ce_2: 2.811  loss_mask_2: 0.8589  loss_dice_2: 4.449  loss_ce_3: 2.788  loss_mask_3: 0.8994  loss_dice_3: 4.4  loss_ce_4: 2.671  loss_mask_4: 0.8694  loss_dice_4: 4.469  loss_ce_5: 2.743  loss_mask_5: 0.8973  loss_dice_5: 4.455  loss_ce_6: 2.634  loss_mask_6: 0.8491  loss_dice_6: 4.462  loss_ce_7: 2.935  loss_mask_7: 0.8945  loss_dice_7: 4.421  loss_ce_8: 3.24  loss_mask_8: 0.804  loss_dice_8: 4.378  loss_mars: 0.5527    time: 5.7131  last_time: 4.4046  data_time: 0.0026  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 02:23:06] d2.utils.events INFO:  eta: 20:40:25  iter: 2479  total_loss: 92.13  loss_ce: 3.566  loss_mask: 1.139  loss_dice: 4.215  loss_ce_0: 3.923  loss_mask_0: 1.298  loss_dice_0: 4.21  loss_ce_1: 3.224  loss_mask_1: 1.04  loss_dice_1: 4.263  loss_ce_2: 3.182  loss_mask_2: 1.274  loss_dice_2: 4.314  loss_ce_3: 3.076  loss_mask_3: 1.039  loss_dice_3: 4.262  loss_ce_4: 3.014  loss_mask_4: 1.387  loss_dice_4: 4.414  loss_ce_5: 3.106  loss_mask_5: 1.281  loss_dice_5: 4.255  loss_ce_6: 3.081  loss_mask_6: 1.036  loss_dice_6: 4.245  loss_ce_7: 3.325  loss_mask_7: 1.119  loss_dice_7: 4.162  loss_ce_8: 3.49  loss_mask_8: 1.043  loss_dice_8: 4.251  loss_mars: 0.431    time: 5.7146  last_time: 5.3488  data_time: 0.0024  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 02:24:57] d2.utils.events INFO:  eta: 20:38:32  iter: 2499  total_loss: 81.03  loss_ce: 2.573  loss_mask: 1.73  loss_dice: 3.394  loss_ce_0: 2.878  loss_mask_0: 1.671  loss_dice_0: 3.464  loss_ce_1: 2.313  loss_mask_1: 1.742  loss_dice_1: 3.468  loss_ce_2: 2.134  loss_mask_2: 1.74  loss_dice_2: 3.775  loss_ce_3: 2.026  loss_mask_3: 1.73  loss_dice_3: 3.793  loss_ce_4: 2.124  loss_mask_4: 1.883  loss_dice_4: 3.735  loss_ce_5: 2.11  loss_mask_5: 1.738  loss_dice_5: 3.742  loss_ce_6: 2.197  loss_mask_6: 1.749  loss_dice_6: 3.59  loss_ce_7: 2.353  loss_mask_7: 1.707  loss_dice_7: 3.593  loss_ce_8: 2.504  loss_mask_8: 1.732  loss_dice_8: 3.552  loss_mars: 0.6601    time: 5.7115  last_time: 5.8877  data_time: 0.0022  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 02:26:53] d2.utils.events INFO:  eta: 20:37:06  iter: 2519  total_loss: 95.16  loss_ce: 3.847  loss_mask: 0.992  loss_dice: 4.486  loss_ce_0: 4.469  loss_mask_0: 1.082  loss_dice_0: 4.27  loss_ce_1: 3.768  loss_mask_1: 1.07  loss_dice_1: 4.269  loss_ce_2: 3.73  loss_mask_2: 1.035  loss_dice_2: 4.366  loss_ce_3: 3.276  loss_mask_3: 1.235  loss_dice_3: 4.374  loss_ce_4: 3.344  loss_mask_4: 1.101  loss_dice_4: 4.356  loss_ce_5: 3.455  loss_mask_5: 1.148  loss_dice_5: 4.25  loss_ce_6: 3.426  loss_mask_6: 1.18  loss_dice_6: 4.305  loss_ce_7: 3.602  loss_mask_7: 1.159  loss_dice_7: 4.383  loss_ce_8: 3.479  loss_mask_8: 1.085  loss_dice_8: 4.511  loss_mars: 0.5797    time: 5.7127  last_time: 8.3406  data_time: 0.0023  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 02:28:44] d2.utils.events INFO:  eta: 20:34:48  iter: 2539  total_loss: 89.33  loss_ce: 3.192  loss_mask: 1.121  loss_dice: 4.363  loss_ce_0: 4.182  loss_mask_0: 1.244  loss_dice_0: 3.919  loss_ce_1: 3.241  loss_mask_1: 1.158  loss_dice_1: 3.907  loss_ce_2: 3.358  loss_mask_2: 1.164  loss_dice_2: 4.06  loss_ce_3: 3.126  loss_mask_3: 1.194  loss_dice_3: 4.054  loss_ce_4: 3.095  loss_mask_4: 1.325  loss_dice_4: 3.975  loss_ce_5: 3.121  loss_mask_5: 1.29  loss_dice_5: 3.916  loss_ce_6: 3.256  loss_mask_6: 1.385  loss_dice_6: 3.913  loss_ce_7: 3.191  loss_mask_7: 1.298  loss_dice_7: 4.042  loss_ce_8: 3.065  loss_mask_8: 1.168  loss_dice_8: 4.139  loss_mars: 0.7459    time: 5.7101  last_time: 5.4613  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 02:30:38] d2.utils.events INFO:  eta: 20:32:55  iter: 2559  total_loss: 84.56  loss_ce: 2.778  loss_mask: 1.332  loss_dice: 4.26  loss_ce_0: 3.69  loss_mask_0: 1.188  loss_dice_0: 4.042  loss_ce_1: 2.802  loss_mask_1: 1.304  loss_dice_1: 3.911  loss_ce_2: 2.788  loss_mask_2: 1.235  loss_dice_2: 4.191  loss_ce_3: 2.449  loss_mask_3: 1.32  loss_dice_3: 4.199  loss_ce_4: 2.564  loss_mask_4: 1.173  loss_dice_4: 4.202  loss_ce_5: 2.826  loss_mask_5: 1.454  loss_dice_5: 4.273  loss_ce_6: 2.681  loss_mask_6: 1.374  loss_dice_6: 4.223  loss_ce_7: 2.555  loss_mask_7: 1.418  loss_dice_7: 4.102  loss_ce_8: 2.734  loss_mask_8: 1.352  loss_dice_8: 4.306  loss_mars: 0.5109    time: 5.7090  last_time: 5.3659  data_time: 0.0024  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 02:32:31] d2.utils.events INFO:  eta: 20:29:26  iter: 2579  total_loss: 89.78  loss_ce: 3.482  loss_mask: 0.941  loss_dice: 3.957  loss_ce_0: 3.6  loss_mask_0: 0.9517  loss_dice_0: 3.87  loss_ce_1: 3.213  loss_mask_1: 0.9327  loss_dice_1: 4.04  loss_ce_2: 3.241  loss_mask_2: 0.996  loss_dice_2: 3.875  loss_ce_3: 3.164  loss_mask_3: 1.033  loss_dice_3: 4.005  loss_ce_4: 3.162  loss_mask_4: 0.9697  loss_dice_4: 4.068  loss_ce_5: 3.115  loss_mask_5: 0.9503  loss_dice_5: 3.921  loss_ce_6: 3.161  loss_mask_6: 0.9709  loss_dice_6: 3.918  loss_ce_7: 3.16  loss_mask_7: 0.9319  loss_dice_7: 4.009  loss_ce_8: 3.265  loss_mask_8: 1.015  loss_dice_8: 3.879  loss_mars: 0.7018    time: 5.7080  last_time: 5.8855  data_time: 0.0028  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 02:34:24] d2.utils.events INFO:  eta: 20:28:21  iter: 2599  total_loss: 83.16  loss_ce: 2.241  loss_mask: 1.789  loss_dice: 4.128  loss_ce_0: 2.814  loss_mask_0: 1.827  loss_dice_0: 4.07  loss_ce_1: 2.226  loss_mask_1: 1.778  loss_dice_1: 4.071  loss_ce_2: 2.253  loss_mask_2: 1.723  loss_dice_2: 4.076  loss_ce_3: 2.138  loss_mask_3: 1.828  loss_dice_3: 4.132  loss_ce_4: 2.194  loss_mask_4: 1.639  loss_dice_4: 4.095  loss_ce_5: 2.247  loss_mask_5: 1.739  loss_dice_5: 4.041  loss_ce_6: 2.39  loss_mask_6: 1.739  loss_dice_6: 4.087  loss_ce_7: 2.435  loss_mask_7: 1.641  loss_dice_7: 4.065  loss_ce_8: 2.341  loss_mask_8: 1.634  loss_dice_8: 4.046  loss_mars: 0.5921    time: 5.7072  last_time: 5.4895  data_time: 0.0023  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 02:36:19] d2.utils.events INFO:  eta: 20:26:28  iter: 2619  total_loss: 80.51  loss_ce: 2.333  loss_mask: 1.12  loss_dice: 4.344  loss_ce_0: 2.728  loss_mask_0: 0.9447  loss_dice_0: 4.044  loss_ce_1: 2.363  loss_mask_1: 0.8092  loss_dice_1: 3.915  loss_ce_2: 2.214  loss_mask_2: 0.8985  loss_dice_2: 4.076  loss_ce_3: 2.122  loss_mask_3: 0.9907  loss_dice_3: 4.133  loss_ce_4: 2.189  loss_mask_4: 0.9822  loss_dice_4: 4.205  loss_ce_5: 2.253  loss_mask_5: 0.8662  loss_dice_5: 4.246  loss_ce_6: 2.264  loss_mask_6: 0.8808  loss_dice_6: 4.261  loss_ce_7: 2.339  loss_mask_7: 0.9787  loss_dice_7: 4.32  loss_ce_8: 2.286  loss_mask_8: 1.085  loss_dice_8: 4.379  loss_mars: 0.6386    time: 5.7083  last_time: 6.0737  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 02:38:16] d2.utils.events INFO:  eta: 20:25:27  iter: 2639  total_loss: 92.17  loss_ce: 3.443  loss_mask: 1.391  loss_dice: 4.009  loss_ce_0: 4.038  loss_mask_0: 1.537  loss_dice_0: 4.144  loss_ce_1: 3.17  loss_mask_1: 1.386  loss_dice_1: 4.094  loss_ce_2: 3.351  loss_mask_2: 1.43  loss_dice_2: 4.064  loss_ce_3: 3.492  loss_mask_3: 1.496  loss_dice_3: 4.001  loss_ce_4: 3.37  loss_mask_4: 1.598  loss_dice_4: 3.979  loss_ce_5: 3.296  loss_mask_5: 1.51  loss_dice_5: 3.972  loss_ce_6: 3.386  loss_mask_6: 1.565  loss_dice_6: 4.001  loss_ce_7: 3.231  loss_mask_7: 1.461  loss_dice_7: 3.984  loss_ce_8: 3.095  loss_mask_8: 1.559  loss_dice_8: 4.071  loss_mars: 0.9167    time: 5.7103  last_time: 6.0069  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 02:40:09] d2.utils.events INFO:  eta: 20:21:54  iter: 2659  total_loss: 84.35  loss_ce: 3.121  loss_mask: 1.11  loss_dice: 4.048  loss_ce_0: 3.734  loss_mask_0: 0.9956  loss_dice_0: 4.035  loss_ce_1: 2.926  loss_mask_1: 1.04  loss_dice_1: 4.059  loss_ce_2: 2.973  loss_mask_2: 1.264  loss_dice_2: 4.12  loss_ce_3: 2.779  loss_mask_3: 1.263  loss_dice_3: 3.954  loss_ce_4: 2.963  loss_mask_4: 1.005  loss_dice_4: 4.01  loss_ce_5: 2.877  loss_mask_5: 1.01  loss_dice_5: 4.15  loss_ce_6: 3.029  loss_mask_6: 0.981  loss_dice_6: 4.055  loss_ce_7: 2.934  loss_mask_7: 1.024  loss_dice_7: 4.155  loss_ce_8: 2.92  loss_mask_8: 1.058  loss_dice_8: 4.225  loss_mars: 0.7617    time: 5.7093  last_time: 5.1701  data_time: 0.0025  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 02:42:09] d2.utils.events INFO:  eta: 20:21:41  iter: 2679  total_loss: 93.44  loss_ce: 3.905  loss_mask: 1.149  loss_dice: 4.255  loss_ce_0: 5.081  loss_mask_0: 0.9752  loss_dice_0: 4.118  loss_ce_1: 3.666  loss_mask_1: 0.9703  loss_dice_1: 4.243  loss_ce_2: 3.936  loss_mask_2: 1.053  loss_dice_2: 4.132  loss_ce_3: 3.92  loss_mask_3: 1.078  loss_dice_3: 4.031  loss_ce_4: 4.03  loss_mask_4: 1.134  loss_dice_4: 4.087  loss_ce_5: 4.063  loss_mask_5: 1.076  loss_dice_5: 4.023  loss_ce_6: 4.058  loss_mask_6: 1.096  loss_dice_6: 4.037  loss_ce_7: 4.09  loss_mask_7: 1.182  loss_dice_7: 4.108  loss_ce_8: 3.973  loss_mask_8: 1.253  loss_dice_8: 4.093  loss_mars: 0.9161    time: 5.7142  last_time: 6.1030  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 02:44:03] d2.utils.events INFO:  eta: 20:18:08  iter: 2699  total_loss: 88.25  loss_ce: 3.51  loss_mask: 1.412  loss_dice: 4.049  loss_ce_0: 3.89  loss_mask_0: 1.341  loss_dice_0: 4.114  loss_ce_1: 2.878  loss_mask_1: 1.415  loss_dice_1: 4.153  loss_ce_2: 2.867  loss_mask_2: 1.431  loss_dice_2: 4.02  loss_ce_3: 2.937  loss_mask_3: 1.505  loss_dice_3: 4.04  loss_ce_4: 3.023  loss_mask_4: 1.435  loss_dice_4: 4.022  loss_ce_5: 3.165  loss_mask_5: 1.353  loss_dice_5: 4.167  loss_ce_6: 3.152  loss_mask_6: 1.475  loss_dice_6: 4.128  loss_ce_7: 3.168  loss_mask_7: 1.395  loss_dice_7: 4.05  loss_ce_8: 3.103  loss_mask_8: 1.578  loss_dice_8: 4.082  loss_mars: 0.5541    time: 5.7139  last_time: 5.9017  data_time: 0.0025  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 02:45:58] d2.utils.events INFO:  eta: 20:17:42  iter: 2719  total_loss: 94.65  loss_ce: 3.913  loss_mask: 0.8985  loss_dice: 4.185  loss_ce_0: 3.842  loss_mask_0: 0.8506  loss_dice_0: 4.126  loss_ce_1: 3.011  loss_mask_1: 0.9502  loss_dice_1: 4.016  loss_ce_2: 3.057  loss_mask_2: 0.8525  loss_dice_2: 4.278  loss_ce_3: 3.101  loss_mask_3: 0.9168  loss_dice_3: 4.244  loss_ce_4: 3.24  loss_mask_4: 1.03  loss_dice_4: 4.247  loss_ce_5: 3.392  loss_mask_5: 0.9611  loss_dice_5: 4.071  loss_ce_6: 3.508  loss_mask_6: 1.13  loss_dice_6: 4.212  loss_ce_7: 3.481  loss_mask_7: 1.086  loss_dice_7: 4.224  loss_ce_8: 3.516  loss_mask_8: 1.047  loss_dice_8: 4.266  loss_mars: 0.5714    time: 5.7147  last_time: 5.3021  data_time: 0.0024  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 02:47:54] d2.utils.events INFO:  eta: 20:15:54  iter: 2739  total_loss: 100.6  loss_ce: 3.892  loss_mask: 0.9194  loss_dice: 4.388  loss_ce_0: 4.681  loss_mask_0: 0.8388  loss_dice_0: 4.465  loss_ce_1: 4.014  loss_mask_1: 0.888  loss_dice_1: 4.464  loss_ce_2: 3.829  loss_mask_2: 0.8972  loss_dice_2: 4.45  loss_ce_3: 3.858  loss_mask_3: 0.8498  loss_dice_3: 4.526  loss_ce_4: 3.837  loss_mask_4: 0.8676  loss_dice_4: 4.57  loss_ce_5: 3.809  loss_mask_5: 0.8788  loss_dice_5: 4.508  loss_ce_6: 3.867  loss_mask_6: 0.8292  loss_dice_6: 4.434  loss_ce_7: 3.75  loss_mask_7: 0.8507  loss_dice_7: 4.485  loss_ce_8: 3.816  loss_mask_8: 0.9479  loss_dice_8: 4.53  loss_mars: 0.3906    time: 5.7159  last_time: 5.3848  data_time: 0.0029  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 02:49:49] d2.utils.events INFO:  eta: 20:13:16  iter: 2759  total_loss: 83.82  loss_ce: 2.743  loss_mask: 1.314  loss_dice: 4.127  loss_ce_0: 3.582  loss_mask_0: 1.408  loss_dice_0: 4.003  loss_ce_1: 2.833  loss_mask_1: 1.361  loss_dice_1: 4.18  loss_ce_2: 2.547  loss_mask_2: 1.366  loss_dice_2: 4.238  loss_ce_3: 2.645  loss_mask_3: 1.525  loss_dice_3: 4.138  loss_ce_4: 2.606  loss_mask_4: 1.356  loss_dice_4: 4.219  loss_ce_5: 2.682  loss_mask_5: 1.39  loss_dice_5: 4.037  loss_ce_6: 2.509  loss_mask_6: 1.53  loss_dice_6: 4.155  loss_ce_7: 2.622  loss_mask_7: 1.585  loss_dice_7: 4.166  loss_ce_8: 2.531  loss_mask_8: 1.453  loss_dice_8: 4.192  loss_mars: 0.5313    time: 5.7167  last_time: 5.5521  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 02:51:41] d2.utils.events INFO:  eta: 20:12:02  iter: 2779  total_loss: 78.56  loss_ce: 2.386  loss_mask: 1.909  loss_dice: 3.753  loss_ce_0: 3.113  loss_mask_0: 1.703  loss_dice_0: 3.48  loss_ce_1: 2.589  loss_mask_1: 1.739  loss_dice_1: 3.53  loss_ce_2: 2.644  loss_mask_2: 1.662  loss_dice_2: 3.724  loss_ce_3: 2.404  loss_mask_3: 1.866  loss_dice_3: 3.572  loss_ce_4: 2.642  loss_mask_4: 2.035  loss_dice_4: 3.511  loss_ce_5: 2.558  loss_mask_5: 1.837  loss_dice_5: 3.434  loss_ce_6: 2.729  loss_mask_6: 1.907  loss_dice_6: 3.638  loss_ce_7: 2.729  loss_mask_7: 1.96  loss_dice_7: 3.51  loss_ce_8: 2.722  loss_mask_8: 1.792  loss_dice_8: 3.546  loss_mars: 0.7886    time: 5.7147  last_time: 6.1230  data_time: 0.0022  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 02:53:31] d2.utils.events INFO:  eta: 20:08:25  iter: 2799  total_loss: 90.21  loss_ce: 2.89  loss_mask: 2.026  loss_dice: 4.455  loss_ce_0: 3.355  loss_mask_0: 1.631  loss_dice_0: 3.865  loss_ce_1: 3  loss_mask_1: 1.981  loss_dice_1: 3.65  loss_ce_2: 2.784  loss_mask_2: 1.499  loss_dice_2: 4.026  loss_ce_3: 2.828  loss_mask_3: 2.308  loss_dice_3: 4.309  loss_ce_4: 2.786  loss_mask_4: 2.47  loss_dice_4: 4.084  loss_ce_5: 2.752  loss_mask_5: 2.159  loss_dice_5: 4.079  loss_ce_6: 2.49  loss_mask_6: 2.318  loss_dice_6: 4.356  loss_ce_7: 2.576  loss_mask_7: 1.676  loss_dice_7: 4.262  loss_ce_8: 2.496  loss_mask_8: 1.929  loss_dice_8: 4.481  loss_mars: 0.4936    time: 5.7116  last_time: 5.8567  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 02:55:23] d2.utils.events INFO:  eta: 20:00:16  iter: 2819  total_loss: 90.28  loss_ce: 2.99  loss_mask: 1.159  loss_dice: 4.208  loss_ce_0: 3.826  loss_mask_0: 1.009  loss_dice_0: 4.173  loss_ce_1: 3.228  loss_mask_1: 1.141  loss_dice_1: 4.265  loss_ce_2: 3.029  loss_mask_2: 1.054  loss_dice_2: 4.338  loss_ce_3: 3.26  loss_mask_3: 1.191  loss_dice_3: 4.387  loss_ce_4: 3.092  loss_mask_4: 1.221  loss_dice_4: 4.141  loss_ce_5: 3.119  loss_mask_5: 1.179  loss_dice_5: 4.167  loss_ce_6: 3.018  loss_mask_6: 1.309  loss_dice_6: 4.135  loss_ce_7: 2.953  loss_mask_7: 1.123  loss_dice_7: 4.227  loss_ce_8: 2.951  loss_mask_8: 1.213  loss_dice_8: 4.155  loss_mars: 0.4816    time: 5.7093  last_time: 5.0849  data_time: 0.0023  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 02:57:21] d2.utils.events INFO:  eta: 20:03:55  iter: 2839  total_loss: 88.71  loss_ce: 3.025  loss_mask: 0.9908  loss_dice: 4.113  loss_ce_0: 3.687  loss_mask_0: 0.8299  loss_dice_0: 4.113  loss_ce_1: 2.88  loss_mask_1: 0.9138  loss_dice_1: 4.141  loss_ce_2: 3.192  loss_mask_2: 0.8902  loss_dice_2: 3.969  loss_ce_3: 3.071  loss_mask_3: 0.9126  loss_dice_3: 4.05  loss_ce_4: 2.884  loss_mask_4: 0.9502  loss_dice_4: 3.997  loss_ce_5: 2.93  loss_mask_5: 0.9471  loss_dice_5: 4.012  loss_ce_6: 2.802  loss_mask_6: 1.119  loss_dice_6: 4.278  loss_ce_7: 2.755  loss_mask_7: 1.174  loss_dice_7: 4.285  loss_ce_8: 2.815  loss_mask_8: 1.327  loss_dice_8: 4.274  loss_mars: 0.5424    time: 5.7122  last_time: 5.9344  data_time: 0.0025  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 02:59:21] d2.utils.events INFO:  eta: 20:03:50  iter: 2859  total_loss: 93.47  loss_ce: 3.286  loss_mask: 0.8629  loss_dice: 4.17  loss_ce_0: 4.151  loss_mask_0: 0.8766  loss_dice_0: 4.223  loss_ce_1: 3.696  loss_mask_1: 0.8725  loss_dice_1: 4.147  loss_ce_2: 3.194  loss_mask_2: 0.9061  loss_dice_2: 4.281  loss_ce_3: 3.145  loss_mask_3: 0.886  loss_dice_3: 4.238  loss_ce_4: 3.12  loss_mask_4: 0.8779  loss_dice_4: 4.243  loss_ce_5: 3.083  loss_mask_5: 0.9451  loss_dice_5: 4.285  loss_ce_6: 3.32  loss_mask_6: 0.9091  loss_dice_6: 4.354  loss_ce_7: 3.013  loss_mask_7: 0.9077  loss_dice_7: 4.256  loss_ce_8: 3.227  loss_mask_8: 0.9702  loss_dice_8: 4.315  loss_mars: 0.8535    time: 5.7168  last_time: 6.1824  data_time: 0.0024  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 03:01:20] d2.utils.events INFO:  eta: 20:02:48  iter: 2879  total_loss: 93.03  loss_ce: 4.178  loss_mask: 0.9954  loss_dice: 4.059  loss_ce_0: 4.911  loss_mask_0: 1.048  loss_dice_0: 4.094  loss_ce_1: 3.695  loss_mask_1: 1.024  loss_dice_1: 4.073  loss_ce_2: 3.609  loss_mask_2: 1.009  loss_dice_2: 4.227  loss_ce_3: 3.524  loss_mask_3: 0.947  loss_dice_3: 4.06  loss_ce_4: 3.529  loss_mask_4: 0.9956  loss_dice_4: 4.013  loss_ce_5: 3.749  loss_mask_5: 1.011  loss_dice_5: 3.949  loss_ce_6: 3.679  loss_mask_6: 1.04  loss_dice_6: 3.96  loss_ce_7: 3.946  loss_mask_7: 0.9737  loss_dice_7: 4.025  loss_ce_8: 3.956  loss_mask_8: 1.15  loss_dice_8: 3.987  loss_mars: 0.8513    time: 5.7204  last_time: 5.4338  data_time: 0.0030  last_data_time: 0.0034   lr: 0.0001  max_mem: 0M
[10/19 03:03:15] d2.utils.events INFO:  eta: 20:04:40  iter: 2899  total_loss: 98.31  loss_ce: 4.798  loss_mask: 1.193  loss_dice: 4.155  loss_ce_0: 5.095  loss_mask_0: 1.249  loss_dice_0: 4.249  loss_ce_1: 4.446  loss_mask_1: 1.242  loss_dice_1: 4.232  loss_ce_2: 4.185  loss_mask_2: 1.281  loss_dice_2: 4.266  loss_ce_3: 4.106  loss_mask_3: 1.283  loss_dice_3: 4.351  loss_ce_4: 4.241  loss_mask_4: 1.264  loss_dice_4: 4.23  loss_ce_5: 4.322  loss_mask_5: 1.255  loss_dice_5: 4.301  loss_ce_6: 4.174  loss_mask_6: 1.227  loss_dice_6: 4.232  loss_ce_7: 4.36  loss_mask_7: 1.204  loss_dice_7: 4.188  loss_ce_8: 4.4  loss_mask_8: 1.152  loss_dice_8: 4.345  loss_mars: 0.7281    time: 5.7207  last_time: 6.0911  data_time: 0.0025  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 03:05:12] d2.utils.events INFO:  eta: 20:03:54  iter: 2919  total_loss: 94.76  loss_ce: 5.227  loss_mask: 0.6339  loss_dice: 4.374  loss_ce_0: 5.625  loss_mask_0: 0.662  loss_dice_0: 4.375  loss_ce_1: 4.329  loss_mask_1: 0.6685  loss_dice_1: 4.499  loss_ce_2: 4.234  loss_mask_2: 0.6962  loss_dice_2: 4.428  loss_ce_3: 4.245  loss_mask_3: 0.6493  loss_dice_3: 4.445  loss_ce_4: 4.054  loss_mask_4: 0.6466  loss_dice_4: 4.431  loss_ce_5: 4.24  loss_mask_5: 0.6802  loss_dice_5: 4.515  loss_ce_6: 4.026  loss_mask_6: 0.6705  loss_dice_6: 4.512  loss_ce_7: 4.231  loss_mask_7: 0.6742  loss_dice_7: 4.458  loss_ce_8: 4.312  loss_mask_8: 0.6858  loss_dice_8: 4.482  loss_mars: 0.6698    time: 5.7229  last_time: 5.4618  data_time: 0.0031  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 03:07:14] d2.utils.events INFO:  eta: 20:02:40  iter: 2939  total_loss: 82.06  loss_ce: 2.9  loss_mask: 0.8697  loss_dice: 4.012  loss_ce_0: 3.315  loss_mask_0: 1.048  loss_dice_0: 4  loss_ce_1: 2.635  loss_mask_1: 0.9249  loss_dice_1: 4.087  loss_ce_2: 2.627  loss_mask_2: 1.254  loss_dice_2: 4.181  loss_ce_3: 2.498  loss_mask_3: 0.8486  loss_dice_3: 3.972  loss_ce_4: 2.542  loss_mask_4: 0.8593  loss_dice_4: 4.041  loss_ce_5: 2.63  loss_mask_5: 0.9362  loss_dice_5: 4.011  loss_ce_6: 2.602  loss_mask_6: 1.087  loss_dice_6: 3.977  loss_ce_7: 2.632  loss_mask_7: 0.9963  loss_dice_7: 4.084  loss_ce_8: 2.618  loss_mask_8: 0.8942  loss_dice_8: 4.067  loss_mars: 0.7204    time: 5.7281  last_time: 6.1158  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 03:09:06] d2.utils.events INFO:  eta: 20:00:46  iter: 2959  total_loss: 83.46  loss_ce: 3.037  loss_mask: 1.236  loss_dice: 4.024  loss_ce_0: 3.836  loss_mask_0: 1.273  loss_dice_0: 3.972  loss_ce_1: 3.002  loss_mask_1: 1.309  loss_dice_1: 3.977  loss_ce_2: 3.242  loss_mask_2: 1.376  loss_dice_2: 3.929  loss_ce_3: 3.285  loss_mask_3: 1.351  loss_dice_3: 3.964  loss_ce_4: 3.036  loss_mask_4: 1.376  loss_dice_4: 4.024  loss_ce_5: 3.173  loss_mask_5: 1.341  loss_dice_5: 3.961  loss_ce_6: 2.877  loss_mask_6: 1.351  loss_dice_6: 3.947  loss_ce_7: 3.072  loss_mask_7: 1.345  loss_dice_7: 3.911  loss_ce_8: 3.144  loss_mask_8: 1.374  loss_dice_8: 3.962  loss_mars: 0.9523    time: 5.7266  last_time: 5.9802  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 03:10:55] d2.utils.events INFO:  eta: 19:53:45  iter: 2979  total_loss: 93.22  loss_ce: 4.061  loss_mask: 1.199  loss_dice: 4.01  loss_ce_0: 4.072  loss_mask_0: 1.181  loss_dice_0: 4.105  loss_ce_1: 3.629  loss_mask_1: 1.256  loss_dice_1: 4.184  loss_ce_2: 3.927  loss_mask_2: 1.183  loss_dice_2: 4.055  loss_ce_3: 3.778  loss_mask_3: 1.371  loss_dice_3: 4.125  loss_ce_4: 3.904  loss_mask_4: 1.238  loss_dice_4: 4.142  loss_ce_5: 4.053  loss_mask_5: 1.221  loss_dice_5: 4.175  loss_ce_6: 3.995  loss_mask_6: 1.178  loss_dice_6: 4.166  loss_ce_7: 4.051  loss_mask_7: 1.194  loss_dice_7: 4.109  loss_ce_8: 4.009  loss_mask_8: 1.131  loss_dice_8: 4.216  loss_mars: 0.7071    time: 5.7226  last_time: 5.4027  data_time: 0.0023  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 03:12:48] d2.utils.events INFO:  eta: 19:56:58  iter: 2999  total_loss: 86.8  loss_ce: 2.881  loss_mask: 1.161  loss_dice: 3.842  loss_ce_0: 3.348  loss_mask_0: 1.311  loss_dice_0: 3.978  loss_ce_1: 2.412  loss_mask_1: 1.332  loss_dice_1: 3.84  loss_ce_2: 2.65  loss_mask_2: 1.335  loss_dice_2: 4.048  loss_ce_3: 2.595  loss_mask_3: 1.361  loss_dice_3: 3.882  loss_ce_4: 2.498  loss_mask_4: 1.453  loss_dice_4: 4.004  loss_ce_5: 2.561  loss_mask_5: 1.332  loss_dice_5: 4.053  loss_ce_6: 2.611  loss_mask_6: 1.569  loss_dice_6: 4.084  loss_ce_7: 2.652  loss_mask_7: 1.373  loss_dice_7: 3.774  loss_ce_8: 2.758  loss_mask_8: 1.339  loss_dice_8: 3.739  loss_mars: 0.8297    time: 5.7213  last_time: 6.0901  data_time: 0.0023  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 03:14:37] d2.utils.events INFO:  eta: 19:48:05  iter: 3019  total_loss: 89.27  loss_ce: 2.611  loss_mask: 1.807  loss_dice: 3.993  loss_ce_0: 3.217  loss_mask_0: 1.938  loss_dice_0: 3.866  loss_ce_1: 2.507  loss_mask_1: 1.975  loss_dice_1: 3.827  loss_ce_2: 2.496  loss_mask_2: 1.987  loss_dice_2: 3.899  loss_ce_3: 2.653  loss_mask_3: 1.946  loss_dice_3: 3.648  loss_ce_4: 2.489  loss_mask_4: 2  loss_dice_4: 3.864  loss_ce_5: 2.327  loss_mask_5: 2.08  loss_dice_5: 3.969  loss_ce_6: 2.367  loss_mask_6: 1.885  loss_dice_6: 3.928  loss_ce_7: 2.4  loss_mask_7: 1.697  loss_dice_7: 3.962  loss_ce_8: 2.524  loss_mask_8: 1.865  loss_dice_8: 3.899  loss_mars: 0.279    time: 5.7174  last_time: 5.4798  data_time: 0.0024  last_data_time: 0.0037   lr: 0.0001  max_mem: 0M
[10/19 03:16:31] d2.utils.events INFO:  eta: 19:49:24  iter: 3039  total_loss: 96.79  loss_ce: 3.988  loss_mask: 0.9399  loss_dice: 4.335  loss_ce_0: 4.362  loss_mask_0: 0.972  loss_dice_0: 4.374  loss_ce_1: 3.691  loss_mask_1: 0.9493  loss_dice_1: 4.333  loss_ce_2: 3.368  loss_mask_2: 0.9741  loss_dice_2: 4.354  loss_ce_3: 3.479  loss_mask_3: 0.9337  loss_dice_3: 4.477  loss_ce_4: 3.36  loss_mask_4: 0.9407  loss_dice_4: 4.549  loss_ce_5: 3.373  loss_mask_5: 0.9187  loss_dice_5: 4.479  loss_ce_6: 3.366  loss_mask_6: 1.036  loss_dice_6: 4.486  loss_ce_7: 3.28  loss_mask_7: 0.9168  loss_dice_7: 4.414  loss_ce_8: 3.614  loss_mask_8: 0.9815  loss_dice_8: 4.493  loss_mars: 0.4362    time: 5.7178  last_time: 6.0610  data_time: 0.0025  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 03:18:25] d2.utils.events INFO:  eta: 19:45:04  iter: 3059  total_loss: 87.97  loss_ce: 2.502  loss_mask: 1.736  loss_dice: 4.499  loss_ce_0: 3.27  loss_mask_0: 1.466  loss_dice_0: 4.299  loss_ce_1: 2.445  loss_mask_1: 1.487  loss_dice_1: 4.425  loss_ce_2: 2.486  loss_mask_2: 1.488  loss_dice_2: 4.395  loss_ce_3: 2.319  loss_mask_3: 1.377  loss_dice_3: 4.404  loss_ce_4: 2.227  loss_mask_4: 1.57  loss_dice_4: 4.518  loss_ce_5: 2.245  loss_mask_5: 1.307  loss_dice_5: 4.435  loss_ce_6: 2.176  loss_mask_6: 1.425  loss_dice_6: 4.518  loss_ce_7: 2.4  loss_mask_7: 1.435  loss_dice_7: 4.599  loss_ce_8: 2.485  loss_mask_8: 1.502  loss_dice_8: 4.422  loss_mars: 0.1953    time: 5.7174  last_time: 5.9457  data_time: 0.0027  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 03:20:16] d2.utils.events INFO:  eta: 19:43:10  iter: 3079  total_loss: 79.92  loss_ce: 2.776  loss_mask: 1.531  loss_dice: 4.189  loss_ce_0: 3.045  loss_mask_0: 1.232  loss_dice_0: 4.174  loss_ce_1: 2.507  loss_mask_1: 1.315  loss_dice_1: 4.157  loss_ce_2: 2.573  loss_mask_2: 1.282  loss_dice_2: 4.226  loss_ce_3: 2.814  loss_mask_3: 1.386  loss_dice_3: 4.159  loss_ce_4: 2.561  loss_mask_4: 1.382  loss_dice_4: 4.182  loss_ce_5: 2.576  loss_mask_5: 1.377  loss_dice_5: 4.082  loss_ce_6: 2.264  loss_mask_6: 1.318  loss_dice_6: 4.084  loss_ce_7: 2.404  loss_mask_7: 1.358  loss_dice_7: 4.144  loss_ce_8: 2.479  loss_mask_8: 1.282  loss_dice_8: 4.18  loss_mars: 0.5432    time: 5.7148  last_time: 5.4228  data_time: 0.0025  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 03:22:13] d2.utils.events INFO:  eta: 19:45:44  iter: 3099  total_loss: 88.16  loss_ce: 2.671  loss_mask: 1.618  loss_dice: 4.022  loss_ce_0: 3.594  loss_mask_0: 1.628  loss_dice_0: 3.992  loss_ce_1: 2.708  loss_mask_1: 1.565  loss_dice_1: 3.949  loss_ce_2: 2.662  loss_mask_2: 1.541  loss_dice_2: 4.041  loss_ce_3: 2.795  loss_mask_3: 1.813  loss_dice_3: 4.058  loss_ce_4: 2.583  loss_mask_4: 1.565  loss_dice_4: 4.101  loss_ce_5: 2.469  loss_mask_5: 1.705  loss_dice_5: 4.038  loss_ce_6: 2.385  loss_mask_6: 1.628  loss_dice_6: 4.002  loss_ce_7: 2.429  loss_mask_7: 1.568  loss_dice_7: 4.176  loss_ce_8: 2.504  loss_mask_8: 1.63  loss_dice_8: 3.858  loss_mars: 0.4367    time: 5.7171  last_time: 5.9835  data_time: 0.0023  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 03:24:10] d2.utils.events INFO:  eta: 19:43:50  iter: 3119  total_loss: 86.44  loss_ce: 2.984  loss_mask: 1.163  loss_dice: 4.308  loss_ce_0: 3.644  loss_mask_0: 0.9042  loss_dice_0: 4.382  loss_ce_1: 3.07  loss_mask_1: 1.08  loss_dice_1: 4.381  loss_ce_2: 3.132  loss_mask_2: 1.062  loss_dice_2: 4.371  loss_ce_3: 2.931  loss_mask_3: 1.136  loss_dice_3: 4.38  loss_ce_4: 2.835  loss_mask_4: 1.243  loss_dice_4: 4.321  loss_ce_5: 2.796  loss_mask_5: 1.354  loss_dice_5: 4.434  loss_ce_6: 2.606  loss_mask_6: 1.32  loss_dice_6: 4.489  loss_ce_7: 2.7  loss_mask_7: 1.327  loss_dice_7: 4.478  loss_ce_8: 2.888  loss_mask_8: 1.267  loss_dice_8: 4.333  loss_mars: 0.6378    time: 5.7183  last_time: 5.9949  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 03:24:44] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0003125.pth
[10/19 03:24:45] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 2199         |   bicycle    | 78           |      car      | 345          |
|  motorcycle   | 61           |   airplane   | 26           |      bus      | 56           |
|     train     | 36           |    truck     | 63           |     boat      | 81           |
| traffic light | 69           | fire hydrant | 21           |   stop sign   | 14           |
| parking meter | 16           |    bench     | 82           |     bird      | 97           |
|      cat      | 48           |     dog      | 40           |     horse     | 45           |
|     sheep     | 42           |     cow      | 46           |   elephant    | 46           |
|     bear      | 14           |    zebra     | 69           |    giraffe    | 36           |
|   backpack    | 98           |   umbrella   | 39           |    handbag    | 93           |
|      tie      | 75           |   suitcase   | 57           |    frisbee    | 22           |
|     skis      | 44           |  snowboard   | 19           |  sports ball  | 45           |
|     kite      | 47           | baseball bat | 32           | baseball gl.. | 25           |
|  skateboard   | 32           |  surfboard   | 56           | tennis racket | 52           |
|    bottle     | 208          |  wine glass  | 65           |      cup      | 185          |
|     fork      | 43           |    knife     | 65           |     spoon     | 52           |
|     bowl      | 128          |    banana    | 90           |     apple     | 41           |
|   sandwich    | 33           |    orange    | 38           |   broccoli    | 96           |
|    carrot     | 64           |   hot dog    | 33           |     pizza     | 54           |
|     donut     | 51           |     cake     | 29           |     chair     | 391          |
|     couch     | 54           | potted plant | 64           |      bed      | 41           |
| dining table  | 152          |    toilet    | 36           |      tv       | 49           |
|    laptop     | 53           |    mouse     | 21           |    remote     | 57           |
|   keyboard    | 32           |  cell phone  | 58           |   microwave   | 8            |
|     oven      | 28           |   toaster    | 2            |     sink      | 47           |
| refrigerator  | 24           |     book     | 206          |     clock     | 47           |
|     vase      | 53           |   scissors   | 5            |  teddy bear   | 46           |
|  hair drier   | 0            |  toothbrush  | 2            |               |              |
|     total     | 7117         |              |              |               |              |[0m
[10/19 03:24:45] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 03:24:45] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 03:24:46] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 03:24:46] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 03:26:04] d2.utils.events INFO:  eta: 19:39:39  iter: 3139  total_loss: 95.25  loss_ce: 2.801  loss_mask: 1.624  loss_dice: 4.42  loss_ce_0: 3.45  loss_mask_0: 1.279  loss_dice_0: 4.11  loss_ce_1: 2.804  loss_mask_1: 1.404  loss_dice_1: 4.146  loss_ce_2: 2.812  loss_mask_2: 1.748  loss_dice_2: 4.149  loss_ce_3: 3.055  loss_mask_3: 1.946  loss_dice_3: 4.342  loss_ce_4: 2.845  loss_mask_4: 1.989  loss_dice_4: 4.161  loss_ce_5: 2.668  loss_mask_5: 1.931  loss_dice_5: 4.289  loss_ce_6: 2.654  loss_mask_6: 1.886  loss_dice_6: 4.376  loss_ce_7: 2.641  loss_mask_7: 1.617  loss_dice_7: 4.353  loss_ce_8: 2.827  loss_mask_8: 1.718  loss_dice_8: 4.354  loss_mars: 0.3791    time: 5.7173  last_time: 5.4616  data_time: 0.0024  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 03:28:00] d2.utils.events INFO:  eta: 19:37:46  iter: 3159  total_loss: 92.22  loss_ce: 3.057  loss_mask: 1.496  loss_dice: 4.384  loss_ce_0: 3.781  loss_mask_0: 1.463  loss_dice_0: 4.326  loss_ce_1: 2.744  loss_mask_1: 1.414  loss_dice_1: 4.419  loss_ce_2: 2.519  loss_mask_2: 1.5  loss_dice_2: 4.445  loss_ce_3: 2.539  loss_mask_3: 1.447  loss_dice_3: 4.365  loss_ce_4: 2.755  loss_mask_4: 1.668  loss_dice_4: 4.415  loss_ce_5: 2.742  loss_mask_5: 1.633  loss_dice_5: 4.369  loss_ce_6: 2.793  loss_mask_6: 1.578  loss_dice_6: 4.308  loss_ce_7: 2.852  loss_mask_7: 1.604  loss_dice_7: 4.368  loss_ce_8: 2.704  loss_mask_8: 1.571  loss_dice_8: 4.381  loss_mars: 0.7577    time: 5.7182  last_time: 5.6950  data_time: 0.0024  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 03:29:58] d2.utils.events INFO:  eta: 19:40:36  iter: 3179  total_loss: 88.28  loss_ce: 3.254  loss_mask: 1.536  loss_dice: 3.859  loss_ce_0: 3.293  loss_mask_0: 1.133  loss_dice_0: 4.009  loss_ce_1: 2.687  loss_mask_1: 1.384  loss_dice_1: 3.873  loss_ce_2: 2.802  loss_mask_2: 1.431  loss_dice_2: 3.711  loss_ce_3: 3.014  loss_mask_3: 1.38  loss_dice_3: 3.79  loss_ce_4: 2.985  loss_mask_4: 1.571  loss_dice_4: 4.005  loss_ce_5: 3.201  loss_mask_5: 1.669  loss_dice_5: 3.949  loss_ce_6: 3.079  loss_mask_6: 1.274  loss_dice_6: 3.891  loss_ce_7: 3.158  loss_mask_7: 1.451  loss_dice_7: 3.866  loss_ce_8: 3.242  loss_mask_8: 1.418  loss_dice_8: 3.97  loss_mars: 0.8479    time: 5.7206  last_time: 5.9749  data_time: 0.0027  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 03:31:55] d2.utils.events INFO:  eta: 19:39:06  iter: 3199  total_loss: 92.02  loss_ce: 4.505  loss_mask: 0.6287  loss_dice: 4.05  loss_ce_0: 5.387  loss_mask_0: 0.5606  loss_dice_0: 4.193  loss_ce_1: 4.269  loss_mask_1: 0.5424  loss_dice_1: 4.061  loss_ce_2: 4.113  loss_mask_2: 0.5781  loss_dice_2: 4.23  loss_ce_3: 4.213  loss_mask_3: 0.5546  loss_dice_3: 4.141  loss_ce_4: 4.053  loss_mask_4: 0.5831  loss_dice_4: 4.164  loss_ce_5: 4.14  loss_mask_5: 0.5707  loss_dice_5: 4.23  loss_ce_6: 4.041  loss_mask_6: 0.6438  loss_dice_6: 4.143  loss_ce_7: 4.067  loss_mask_7: 0.5762  loss_dice_7: 4.333  loss_ce_8: 4.216  loss_mask_8: 0.5809  loss_dice_8: 4.136  loss_mars: 0.4646    time: 5.7223  last_time: 5.3434  data_time: 0.0026  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 03:33:50] d2.utils.events INFO:  eta: 19:36:07  iter: 3219  total_loss: 84.47  loss_ce: 2.438  loss_mask: 1.308  loss_dice: 4.168  loss_ce_0: 3.067  loss_mask_0: 1.177  loss_dice_0: 4.254  loss_ce_1: 2.466  loss_mask_1: 1.169  loss_dice_1: 4.075  loss_ce_2: 2.466  loss_mask_2: 1.176  loss_dice_2: 4.188  loss_ce_3: 2.334  loss_mask_3: 1.355  loss_dice_3: 4.18  loss_ce_4: 2.322  loss_mask_4: 1.121  loss_dice_4: 4.11  loss_ce_5: 2.317  loss_mask_5: 1.313  loss_dice_5: 4.277  loss_ce_6: 2.442  loss_mask_6: 1.341  loss_dice_6: 4.278  loss_ce_7: 2.34  loss_mask_7: 1.234  loss_dice_7: 4.136  loss_ce_8: 2.443  loss_mask_8: 1.154  loss_dice_8: 4.067  loss_mars: 0.3779    time: 5.7224  last_time: 6.0916  data_time: 0.0023  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 03:35:46] d2.utils.events INFO:  eta: 19:34:54  iter: 3239  total_loss: 95  loss_ce: 3.388  loss_mask: 0.8986  loss_dice: 4.414  loss_ce_0: 3.152  loss_mask_0: 0.9174  loss_dice_0: 4.309  loss_ce_1: 2.787  loss_mask_1: 0.9582  loss_dice_1: 4.302  loss_ce_2: 2.903  loss_mask_2: 0.984  loss_dice_2: 4.399  loss_ce_3: 3.049  loss_mask_3: 1.087  loss_dice_3: 4.284  loss_ce_4: 3.12  loss_mask_4: 0.8455  loss_dice_4: 4.349  loss_ce_5: 3.179  loss_mask_5: 0.8606  loss_dice_5: 4.335  loss_ce_6: 3.571  loss_mask_6: 0.9529  loss_dice_6: 4.323  loss_ce_7: 3.563  loss_mask_7: 0.9187  loss_dice_7: 4.303  loss_ce_8: 3.56  loss_mask_8: 0.861  loss_dice_8: 4.387  loss_mars: 0.787    time: 5.7234  last_time: 6.0612  data_time: 0.0026  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 03:37:42] d2.utils.events INFO:  eta: 19:33:25  iter: 3259  total_loss: 88.94  loss_ce: 3.657  loss_mask: 1.017  loss_dice: 4.289  loss_ce_0: 3.63  loss_mask_0: 1.112  loss_dice_0: 4.404  loss_ce_1: 3.197  loss_mask_1: 1.032  loss_dice_1: 4.26  loss_ce_2: 2.973  loss_mask_2: 1.127  loss_dice_2: 4.308  loss_ce_3: 2.985  loss_mask_3: 0.935  loss_dice_3: 4.314  loss_ce_4: 3.04  loss_mask_4: 1.372  loss_dice_4: 4.33  loss_ce_5: 2.963  loss_mask_5: 1.271  loss_dice_5: 4.391  loss_ce_6: 3.242  loss_mask_6: 1.007  loss_dice_6: 4.425  loss_ce_7: 3.493  loss_mask_7: 1.041  loss_dice_7: 4.352  loss_ce_8: 3.681  loss_mask_8: 0.9844  loss_dice_8: 4.335  loss_mars: 0.6418    time: 5.7245  last_time: 7.1544  data_time: 0.0028  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 03:39:38] d2.utils.events INFO:  eta: 19:33:46  iter: 3279  total_loss: 89.24  loss_ce: 3.579  loss_mask: 0.896  loss_dice: 4.049  loss_ce_0: 3.621  loss_mask_0: 0.9308  loss_dice_0: 4.332  loss_ce_1: 3.186  loss_mask_1: 0.9644  loss_dice_1: 4.472  loss_ce_2: 3.187  loss_mask_2: 0.9538  loss_dice_2: 4.326  loss_ce_3: 3.203  loss_mask_3: 0.907  loss_dice_3: 4.202  loss_ce_4: 3.186  loss_mask_4: 0.9981  loss_dice_4: 4.171  loss_ce_5: 3.535  loss_mask_5: 0.9424  loss_dice_5: 4.148  loss_ce_6: 3.609  loss_mask_6: 0.9525  loss_dice_6: 4.374  loss_ce_7: 3.646  loss_mask_7: 0.8687  loss_dice_7: 4.192  loss_ce_8: 3.668  loss_mask_8: 0.8894  loss_dice_8: 4.081  loss_mars: 0.8563    time: 5.7253  last_time: 5.9174  data_time: 0.0026  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 03:41:34] d2.utils.events INFO:  eta: 19:34:21  iter: 3299  total_loss: 98.77  loss_ce: 4.535  loss_mask: 1.342  loss_dice: 3.801  loss_ce_0: 4.815  loss_mask_0: 1.462  loss_dice_0: 3.725  loss_ce_1: 3.981  loss_mask_1: 1.328  loss_dice_1: 4.001  loss_ce_2: 4.219  loss_mask_2: 1.352  loss_dice_2: 3.688  loss_ce_3: 4.335  loss_mask_3: 1.367  loss_dice_3: 3.516  loss_ce_4: 4.087  loss_mask_4: 1.375  loss_dice_4: 3.635  loss_ce_5: 4.198  loss_mask_5: 1.265  loss_dice_5: 3.755  loss_ce_6: 4.515  loss_mask_6: 1.379  loss_dice_6: 3.924  loss_ce_7: 4.617  loss_mask_7: 1.435  loss_dice_7: 3.971  loss_ce_8: 4.667  loss_mask_8: 1.398  loss_dice_8: 4.012  loss_mars: 0.8658    time: 5.7262  last_time: 6.0187  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 03:43:28] d2.utils.events INFO:  eta: 19:34:12  iter: 3319  total_loss: 97.59  loss_ce: 4.535  loss_mask: 0.6787  loss_dice: 4.471  loss_ce_0: 5.549  loss_mask_0: 0.6746  loss_dice_0: 4.341  loss_ce_1: 4.479  loss_mask_1: 0.7552  loss_dice_1: 4.375  loss_ce_2: 4.479  loss_mask_2: 0.7704  loss_dice_2: 4.292  loss_ce_3: 4.598  loss_mask_3: 0.7784  loss_dice_3: 4.358  loss_ce_4: 4.453  loss_mask_4: 0.7261  loss_dice_4: 4.333  loss_ce_5: 4.61  loss_mask_5: 0.7659  loss_dice_5: 4.301  loss_ce_6: 4.637  loss_mask_6: 0.7404  loss_dice_6: 4.329  loss_ce_7: 4.748  loss_mask_7: 0.7073  loss_dice_7: 4.371  loss_ce_8: 4.509  loss_mask_8: 0.661  loss_dice_8: 4.353  loss_mars: 0.9156    time: 5.7255  last_time: 5.9650  data_time: 0.0027  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 03:45:19] d2.utils.events INFO:  eta: 19:29:37  iter: 3339  total_loss: 91.05  loss_ce: 3.665  loss_mask: 1.026  loss_dice: 4.059  loss_ce_0: 4.429  loss_mask_0: 1.02  loss_dice_0: 4.181  loss_ce_1: 3.476  loss_mask_1: 1.086  loss_dice_1: 4.178  loss_ce_2: 3.902  loss_mask_2: 1.01  loss_dice_2: 4.097  loss_ce_3: 3.877  loss_mask_3: 0.8485  loss_dice_3: 4.198  loss_ce_4: 3.742  loss_mask_4: 0.8933  loss_dice_4: 4.253  loss_ce_5: 3.519  loss_mask_5: 1.048  loss_dice_5: 4.263  loss_ce_6: 3.605  loss_mask_6: 1.006  loss_dice_6: 4.187  loss_ce_7: 3.747  loss_mask_7: 0.9862  loss_dice_7: 4.236  loss_ce_8: 3.765  loss_mask_8: 1.006  loss_dice_8: 4.299  loss_mars: 0.8152    time: 5.7239  last_time: 6.0715  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 03:47:10] d2.utils.events INFO:  eta: 19:24:36  iter: 3359  total_loss: 88.86  loss_ce: 3.111  loss_mask: 1.063  loss_dice: 3.923  loss_ce_0: 3.314  loss_mask_0: 1.154  loss_dice_0: 3.803  loss_ce_1: 3.168  loss_mask_1: 1.086  loss_dice_1: 4.053  loss_ce_2: 3.305  loss_mask_2: 1.18  loss_dice_2: 3.708  loss_ce_3: 3.284  loss_mask_3: 1.14  loss_dice_3: 3.994  loss_ce_4: 3.263  loss_mask_4: 1.221  loss_dice_4: 3.942  loss_ce_5: 3.039  loss_mask_5: 1.111  loss_dice_5: 3.9  loss_ce_6: 2.93  loss_mask_6: 1.081  loss_dice_6: 3.962  loss_ce_7: 3.015  loss_mask_7: 1.162  loss_dice_7: 3.977  loss_ce_8: 2.996  loss_mask_8: 1.055  loss_dice_8: 3.999  loss_mars: 0.9204    time: 5.7215  last_time: 6.9037  data_time: 0.0025  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 03:49:04] d2.utils.events INFO:  eta: 19:25:18  iter: 3379  total_loss: 97.18  loss_ce: 3.214  loss_mask: 1.142  loss_dice: 4.287  loss_ce_0: 3.958  loss_mask_0: 1.183  loss_dice_0: 4.427  loss_ce_1: 3.063  loss_mask_1: 1.455  loss_dice_1: 4.426  loss_ce_2: 3.408  loss_mask_2: 1.173  loss_dice_2: 4.447  loss_ce_3: 3.495  loss_mask_3: 1.129  loss_dice_3: 4.28  loss_ce_4: 3.308  loss_mask_4: 1.271  loss_dice_4: 4.211  loss_ce_5: 3.375  loss_mask_5: 1.183  loss_dice_5: 4.325  loss_ce_6: 3.212  loss_mask_6: 1.096  loss_dice_6: 4.45  loss_ce_7: 3.273  loss_mask_7: 1.166  loss_dice_7: 4.257  loss_ce_8: 3.271  loss_mask_8: 1.026  loss_dice_8: 4.295  loss_mars: 0.855    time: 5.7213  last_time: 5.3093  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 03:50:55] d2.utils.events INFO:  eta: 19:22:03  iter: 3399  total_loss: 87.72  loss_ce: 2.96  loss_mask: 1.224  loss_dice: 4.01  loss_ce_0: 2.95  loss_mask_0: 1.069  loss_dice_0: 3.988  loss_ce_1: 2.466  loss_mask_1: 1.161  loss_dice_1: 4.193  loss_ce_2: 2.606  loss_mask_2: 1.16  loss_dice_2: 4.301  loss_ce_3: 2.929  loss_mask_3: 1.217  loss_dice_3: 4.119  loss_ce_4: 2.919  loss_mask_4: 1.254  loss_dice_4: 3.919  loss_ce_5: 3.068  loss_mask_5: 1.161  loss_dice_5: 3.834  loss_ce_6: 3.006  loss_mask_6: 1.169  loss_dice_6: 4.071  loss_ce_7: 2.894  loss_mask_7: 1.135  loss_dice_7: 4.097  loss_ce_8: 2.864  loss_mask_8: 1.316  loss_dice_8: 4.135  loss_mars: 0.9253    time: 5.7198  last_time: 5.2338  data_time: 0.0023  last_data_time: 0.0037   lr: 0.0001  max_mem: 0M
[10/19 03:52:48] d2.utils.events INFO:  eta: 19:20:09  iter: 3419  total_loss: 93.56  loss_ce: 3.997  loss_mask: 1.025  loss_dice: 4.034  loss_ce_0: 4.346  loss_mask_0: 1.19  loss_dice_0: 3.817  loss_ce_1: 3.835  loss_mask_1: 1.131  loss_dice_1: 3.977  loss_ce_2: 3.728  loss_mask_2: 1.124  loss_dice_2: 3.68  loss_ce_3: 3.894  loss_mask_3: 1.032  loss_dice_3: 3.824  loss_ce_4: 3.761  loss_mask_4: 1.062  loss_dice_4: 3.909  loss_ce_5: 3.805  loss_mask_5: 0.9989  loss_dice_5: 3.977  loss_ce_6: 3.809  loss_mask_6: 1.124  loss_dice_6: 3.848  loss_ce_7: 3.861  loss_mask_7: 1.17  loss_dice_7: 3.888  loss_ce_8: 3.955  loss_mask_8: 1.176  loss_dice_8: 3.896  loss_mars: 0.9215    time: 5.7186  last_time: 5.9559  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 03:54:42] d2.utils.events INFO:  eta: 19:19:36  iter: 3439  total_loss: 93.54  loss_ce: 2.965  loss_mask: 1.193  loss_dice: 4.284  loss_ce_0: 3.062  loss_mask_0: 0.9982  loss_dice_0: 4.087  loss_ce_1: 3.167  loss_mask_1: 1.058  loss_dice_1: 4.321  loss_ce_2: 3.214  loss_mask_2: 1.278  loss_dice_2: 4.408  loss_ce_3: 2.945  loss_mask_3: 1.086  loss_dice_3: 4.18  loss_ce_4: 2.976  loss_mask_4: 1.384  loss_dice_4: 4.424  loss_ce_5: 2.895  loss_mask_5: 1.243  loss_dice_5: 4.516  loss_ce_6: 2.795  loss_mask_6: 1.347  loss_dice_6: 4.431  loss_ce_7: 2.726  loss_mask_7: 1.208  loss_dice_7: 4.335  loss_ce_8: 2.858  loss_mask_8: 1.088  loss_dice_8: 4.307  loss_mars: 0.8373    time: 5.7186  last_time: 5.8157  data_time: 0.0023  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 03:56:40] d2.utils.events INFO:  eta: 19:19:06  iter: 3459  total_loss: 90.32  loss_ce: 3.211  loss_mask: 1.067  loss_dice: 4.489  loss_ce_0: 3.876  loss_mask_0: 0.8541  loss_dice_0: 4.545  loss_ce_1: 3.423  loss_mask_1: 1.055  loss_dice_1: 4.541  loss_ce_2: 3.456  loss_mask_2: 0.9856  loss_dice_2: 4.498  loss_ce_3: 3.355  loss_mask_3: 0.9843  loss_dice_3: 4.531  loss_ce_4: 3.359  loss_mask_4: 1.064  loss_dice_4: 4.473  loss_ce_5: 3.361  loss_mask_5: 1.103  loss_dice_5: 4.535  loss_ce_6: 3.372  loss_mask_6: 0.9674  loss_dice_6: 4.509  loss_ce_7: 3.424  loss_mask_7: 0.9568  loss_dice_7: 4.493  loss_ce_8: 3.362  loss_mask_8: 0.961  loss_dice_8: 4.478  loss_mars: 0.889    time: 5.7207  last_time: 5.3456  data_time: 0.0025  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 03:58:35] d2.utils.events INFO:  eta: 19:19:59  iter: 3479  total_loss: 91.98  loss_ce: 3.752  loss_mask: 1.132  loss_dice: 4.438  loss_ce_0: 3.675  loss_mask_0: 1.136  loss_dice_0: 4.438  loss_ce_1: 3.574  loss_mask_1: 1.204  loss_dice_1: 4.478  loss_ce_2: 3.689  loss_mask_2: 1.14  loss_dice_2: 4.402  loss_ce_3: 3.63  loss_mask_3: 1.144  loss_dice_3: 4.342  loss_ce_4: 3.739  loss_mask_4: 1.083  loss_dice_4: 4.366  loss_ce_5: 3.475  loss_mask_5: 1.194  loss_dice_5: 4.446  loss_ce_6: 3.403  loss_mask_6: 1.32  loss_dice_6: 4.409  loss_ce_7: 3.642  loss_mask_7: 1.172  loss_dice_7: 4.451  loss_ce_8: 3.473  loss_mask_8: 1.236  loss_dice_8: 4.404  loss_mars: 0.7051    time: 5.7209  last_time: 5.9714  data_time: 0.0025  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 04:00:31] d2.utils.events INFO:  eta: 19:20:06  iter: 3499  total_loss: 96.23  loss_ce: 4.022  loss_mask: 1.051  loss_dice: 4.417  loss_ce_0: 4.414  loss_mask_0: 1.01  loss_dice_0: 4.448  loss_ce_1: 3.976  loss_mask_1: 1.044  loss_dice_1: 4.263  loss_ce_2: 4.264  loss_mask_2: 1.006  loss_dice_2: 4.279  loss_ce_3: 4.129  loss_mask_3: 1.038  loss_dice_3: 4.328  loss_ce_4: 4.337  loss_mask_4: 1.038  loss_dice_4: 4.252  loss_ce_5: 4.105  loss_mask_5: 1.033  loss_dice_5: 4.283  loss_ce_6: 3.726  loss_mask_6: 1.122  loss_dice_6: 4.33  loss_ce_7: 3.74  loss_mask_7: 1.105  loss_dice_7: 4.359  loss_ce_8: 3.961  loss_mask_8: 1.073  loss_dice_8: 4.4  loss_mars: 0.8284    time: 5.7215  last_time: 5.0731  data_time: 0.0024  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 04:02:18] d2.utils.events INFO:  eta: 19:13:10  iter: 3519  total_loss: 86.58  loss_ce: 2.648  loss_mask: 1.5  loss_dice: 3.812  loss_ce_0: 2.992  loss_mask_0: 1.291  loss_dice_0: 4.095  loss_ce_1: 2.874  loss_mask_1: 1.57  loss_dice_1: 3.716  loss_ce_2: 3.003  loss_mask_2: 1.348  loss_dice_2: 3.695  loss_ce_3: 2.939  loss_mask_3: 1.405  loss_dice_3: 3.507  loss_ce_4: 2.823  loss_mask_4: 1.423  loss_dice_4: 3.714  loss_ce_5: 2.915  loss_mask_5: 1.449  loss_dice_5: 3.914  loss_ce_6: 2.806  loss_mask_6: 1.632  loss_dice_6: 3.989  loss_ce_7: 2.803  loss_mask_7: 1.724  loss_dice_7: 3.997  loss_ce_8: 2.482  loss_mask_8: 1.352  loss_dice_8: 4.075  loss_mars: 0.9107    time: 5.7177  last_time: 5.3649  data_time: 0.0022  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 04:04:14] d2.utils.events INFO:  eta: 19:17:19  iter: 3539  total_loss: 90.58  loss_ce: 2.797  loss_mask: 1.209  loss_dice: 4.126  loss_ce_0: 3.571  loss_mask_0: 1.152  loss_dice_0: 4.022  loss_ce_1: 2.985  loss_mask_1: 1.154  loss_dice_1: 3.937  loss_ce_2: 2.869  loss_mask_2: 1.197  loss_dice_2: 4.151  loss_ce_3: 2.717  loss_mask_3: 1.092  loss_dice_3: 4  loss_ce_4: 2.678  loss_mask_4: 1.158  loss_dice_4: 4.086  loss_ce_5: 2.961  loss_mask_5: 1.194  loss_dice_5: 4.185  loss_ce_6: 2.729  loss_mask_6: 1.183  loss_dice_6: 4.139  loss_ce_7: 2.72  loss_mask_7: 1.217  loss_dice_7: 4.3  loss_ce_8: 2.529  loss_mask_8: 1.208  loss_dice_8: 4.11  loss_mars: 0.8959    time: 5.7187  last_time: 5.8723  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 04:06:07] d2.utils.events INFO:  eta: 19:21:05  iter: 3559  total_loss: 84.78  loss_ce: 3.024  loss_mask: 1.205  loss_dice: 4.124  loss_ce_0: 3.078  loss_mask_0: 1.152  loss_dice_0: 4.141  loss_ce_1: 3.186  loss_mask_1: 1.235  loss_dice_1: 4.012  loss_ce_2: 3.167  loss_mask_2: 1.215  loss_dice_2: 4.087  loss_ce_3: 3.15  loss_mask_3: 1.224  loss_dice_3: 4.047  loss_ce_4: 3.072  loss_mask_4: 1.21  loss_dice_4: 4.124  loss_ce_5: 3.148  loss_mask_5: 1.214  loss_dice_5: 4.188  loss_ce_6: 2.85  loss_mask_6: 1.31  loss_dice_6: 4.27  loss_ce_7: 2.885  loss_mask_7: 1.316  loss_dice_7: 4.152  loss_ce_8: 2.982  loss_mask_8: 1.284  loss_dice_8: 4.211  loss_mars: 0.7116    time: 5.7181  last_time: 5.8433  data_time: 0.0023  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 04:07:55] d2.utils.events INFO:  eta: 19:17:34  iter: 3579  total_loss: 92.71  loss_ce: 3.097  loss_mask: 1.331  loss_dice: 4.29  loss_ce_0: 3.403  loss_mask_0: 1.226  loss_dice_0: 4.381  loss_ce_1: 3.414  loss_mask_1: 1.236  loss_dice_1: 4.185  loss_ce_2: 3.504  loss_mask_2: 1.333  loss_dice_2: 4.262  loss_ce_3: 3.173  loss_mask_3: 1.256  loss_dice_3: 4.345  loss_ce_4: 3.012  loss_mask_4: 1.294  loss_dice_4: 4.368  loss_ce_5: 2.959  loss_mask_5: 1.226  loss_dice_5: 4.296  loss_ce_6: 2.924  loss_mask_6: 1.307  loss_dice_6: 4.454  loss_ce_7: 2.893  loss_mask_7: 1.227  loss_dice_7: 4.448  loss_ce_8: 3.077  loss_mask_8: 1.166  loss_dice_8: 4.294  loss_mars: 0.5828    time: 5.7149  last_time: 6.1803  data_time: 0.0022  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 04:09:43] d2.utils.events INFO:  eta: 19:11:34  iter: 3599  total_loss: 95.15  loss_ce: 3.128  loss_mask: 1.407  loss_dice: 4.344  loss_ce_0: 3.614  loss_mask_0: 1.31  loss_dice_0: 4.259  loss_ce_1: 3.513  loss_mask_1: 1.398  loss_dice_1: 4.226  loss_ce_2: 3.603  loss_mask_2: 1.289  loss_dice_2: 4.286  loss_ce_3: 3.637  loss_mask_3: 1.327  loss_dice_3: 4.206  loss_ce_4: 3.551  loss_mask_4: 1.321  loss_dice_4: 4.252  loss_ce_5: 3.501  loss_mask_5: 1.236  loss_dice_5: 4.253  loss_ce_6: 3.283  loss_mask_6: 1.365  loss_dice_6: 4.334  loss_ce_7: 3.393  loss_mask_7: 1.293  loss_dice_7: 4.307  loss_ce_8: 3.231  loss_mask_8: 1.373  loss_dice_8: 4.311  loss_mars: 0.7296    time: 5.7116  last_time: 6.3216  data_time: 0.0027  last_data_time: 0.0035   lr: 0.0001  max_mem: 0M
[10/19 04:11:34] d2.utils.events INFO:  eta: 19:04:24  iter: 3619  total_loss: 92.24  loss_ce: 3.108  loss_mask: 1.27  loss_dice: 4.483  loss_ce_0: 3.824  loss_mask_0: 1.014  loss_dice_0: 4.376  loss_ce_1: 3.175  loss_mask_1: 1.124  loss_dice_1: 4.519  loss_ce_2: 2.972  loss_mask_2: 1.083  loss_dice_2: 4.527  loss_ce_3: 2.871  loss_mask_3: 1.149  loss_dice_3: 4.373  loss_ce_4: 3.085  loss_mask_4: 1.257  loss_dice_4: 4.391  loss_ce_5: 3.04  loss_mask_5: 1.154  loss_dice_5: 4.481  loss_ce_6: 2.989  loss_mask_6: 1.136  loss_dice_6: 4.502  loss_ce_7: 2.873  loss_mask_7: 1  loss_dice_7: 4.527  loss_ce_8: 2.835  loss_mask_8: 1.15  loss_dice_8: 4.49  loss_mars: 0.7268    time: 5.7100  last_time: 6.2146  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 04:13:32] d2.utils.events INFO:  eta: 19:02:30  iter: 3639  total_loss: 91.92  loss_ce: 3.408  loss_mask: 0.7668  loss_dice: 4.451  loss_ce_0: 4.177  loss_mask_0: 0.7168  loss_dice_0: 4.475  loss_ce_1: 3.822  loss_mask_1: 0.8536  loss_dice_1: 4.435  loss_ce_2: 3.866  loss_mask_2: 0.8408  loss_dice_2: 4.538  loss_ce_3: 3.598  loss_mask_3: 0.895  loss_dice_3: 4.466  loss_ce_4: 3.478  loss_mask_4: 0.8076  loss_dice_4: 4.579  loss_ce_5: 3.337  loss_mask_5: 0.78  loss_dice_5: 4.549  loss_ce_6: 3.395  loss_mask_6: 0.7346  loss_dice_6: 4.541  loss_ce_7: 3.42  loss_mask_7: 0.7634  loss_dice_7: 4.586  loss_ce_8: 3.061  loss_mask_8: 0.7587  loss_dice_8: 4.438  loss_mars: 0.5748    time: 5.7117  last_time: 5.0349  data_time: 0.0027  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 04:15:22] d2.utils.events INFO:  eta: 19:01:37  iter: 3659  total_loss: 84.43  loss_ce: 2.423  loss_mask: 0.801  loss_dice: 4.594  loss_ce_0: 3.315  loss_mask_0: 0.8002  loss_dice_0: 4.468  loss_ce_1: 3.012  loss_mask_1: 0.7561  loss_dice_1: 4.308  loss_ce_2: 2.738  loss_mask_2: 0.8428  loss_dice_2: 4.304  loss_ce_3: 2.746  loss_mask_3: 0.8091  loss_dice_3: 4.504  loss_ce_4: 2.43  loss_mask_4: 0.9126  loss_dice_4: 4.465  loss_ce_5: 2.403  loss_mask_5: 0.7854  loss_dice_5: 4.557  loss_ce_6: 2.459  loss_mask_6: 0.7758  loss_dice_6: 4.595  loss_ce_7: 2.525  loss_mask_7: 0.768  loss_dice_7: 4.583  loss_ce_8: 2.448  loss_mask_8: 0.7751  loss_dice_8: 4.515  loss_mars: 0.727    time: 5.7097  last_time: 5.8200  data_time: 0.0024  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 04:17:17] d2.utils.events INFO:  eta: 18:59:43  iter: 3679  total_loss: 87.63  loss_ce: 3.25  loss_mask: 1.499  loss_dice: 4.185  loss_ce_0: 2.996  loss_mask_0: 1.534  loss_dice_0: 3.687  loss_ce_1: 2.813  loss_mask_1: 1.512  loss_dice_1: 3.417  loss_ce_2: 2.75  loss_mask_2: 1.529  loss_dice_2: 3.583  loss_ce_3: 2.941  loss_mask_3: 1.638  loss_dice_3: 3.789  loss_ce_4: 2.835  loss_mask_4: 1.529  loss_dice_4: 4.044  loss_ce_5: 3.073  loss_mask_5: 1.466  loss_dice_5: 3.942  loss_ce_6: 2.982  loss_mask_6: 1.513  loss_dice_6: 3.963  loss_ce_7: 3.158  loss_mask_7: 1.611  loss_dice_7: 4.042  loss_ce_8: 2.866  loss_mask_8: 1.609  loss_dice_8: 4.156  loss_mars: 0.5035    time: 5.7101  last_time: 6.9658  data_time: 0.0022  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 04:19:09] d2.utils.events INFO:  eta: 19:02:00  iter: 3699  total_loss: 90.38  loss_ce: 2.912  loss_mask: 1.998  loss_dice: 4.286  loss_ce_0: 2.544  loss_mask_0: 1.556  loss_dice_0: 3.889  loss_ce_1: 2.837  loss_mask_1: 1.454  loss_dice_1: 4.126  loss_ce_2: 2.561  loss_mask_2: 1.53  loss_dice_2: 4.558  loss_ce_3: 2.532  loss_mask_3: 1.492  loss_dice_3: 4.242  loss_ce_4: 2.6  loss_mask_4: 1.332  loss_dice_4: 3.976  loss_ce_5: 2.693  loss_mask_5: 1.885  loss_dice_5: 3.733  loss_ce_6: 2.577  loss_mask_6: 1.769  loss_dice_6: 4.042  loss_ce_7: 2.515  loss_mask_7: 1.925  loss_dice_7: 4.139  loss_ce_8: 2.681  loss_mask_8: 1.91  loss_dice_8: 3.996  loss_mars: 0.5101    time: 5.7092  last_time: 4.7265  data_time: 0.0021  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 04:21:04] d2.utils.events INFO:  eta: 18:57:29  iter: 3719  total_loss: 96.72  loss_ce: 3.849  loss_mask: 1.016  loss_dice: 4.39  loss_ce_0: 4.381  loss_mask_0: 0.9448  loss_dice_0: 4.31  loss_ce_1: 4  loss_mask_1: 0.9457  loss_dice_1: 4.263  loss_ce_2: 4.043  loss_mask_2: 0.9956  loss_dice_2: 4.4  loss_ce_3: 3.798  loss_mask_3: 1.038  loss_dice_3: 4.532  loss_ce_4: 3.925  loss_mask_4: 0.9803  loss_dice_4: 4.449  loss_ce_5: 3.961  loss_mask_5: 0.9459  loss_dice_5: 4.119  loss_ce_6: 4.063  loss_mask_6: 0.9985  loss_dice_6: 4.448  loss_ce_7: 4.204  loss_mask_7: 1.045  loss_dice_7: 4.426  loss_ce_8: 3.725  loss_mask_8: 0.9547  loss_dice_8: 4.429  loss_mars: 0.3097    time: 5.7093  last_time: 5.3894  data_time: 0.0026  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 04:22:58] d2.utils.events INFO:  eta: 18:58:27  iter: 3739  total_loss: 85.5  loss_ce: 2.671  loss_mask: 1.084  loss_dice: 4.46  loss_ce_0: 3.396  loss_mask_0: 1.033  loss_dice_0: 4.231  loss_ce_1: 2.74  loss_mask_1: 1.049  loss_dice_1: 4.283  loss_ce_2: 2.742  loss_mask_2: 1.052  loss_dice_2: 4.228  loss_ce_3: 2.779  loss_mask_3: 1.053  loss_dice_3: 4.249  loss_ce_4: 2.909  loss_mask_4: 1.026  loss_dice_4: 4.385  loss_ce_5: 3.113  loss_mask_5: 1.093  loss_dice_5: 4.308  loss_ce_6: 2.877  loss_mask_6: 1.111  loss_dice_6: 4.274  loss_ce_7: 2.952  loss_mask_7: 1.068  loss_dice_7: 4.368  loss_ce_8: 2.651  loss_mask_8: 0.9891  loss_dice_8: 4.201  loss_mars: 0.6736    time: 5.7092  last_time: 5.9135  data_time: 0.0027  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 04:24:53] d2.utils.events INFO:  eta: 19:00:27  iter: 3759  total_loss: 95.03  loss_ce: 3.306  loss_mask: 0.8405  loss_dice: 4.583  loss_ce_0: 3.816  loss_mask_0: 0.708  loss_dice_0: 4.435  loss_ce_1: 3.492  loss_mask_1: 0.9011  loss_dice_1: 4.403  loss_ce_2: 3.305  loss_mask_2: 0.8259  loss_dice_2: 4.453  loss_ce_3: 3.552  loss_mask_3: 0.811  loss_dice_3: 4.478  loss_ce_4: 3.781  loss_mask_4: 0.8688  loss_dice_4: 4.425  loss_ce_5: 3.729  loss_mask_5: 0.9111  loss_dice_5: 4.412  loss_ce_6: 3.466  loss_mask_6: 0.7466  loss_dice_6: 4.502  loss_ce_7: 3.396  loss_mask_7: 0.7213  loss_dice_7: 4.441  loss_ce_8: 3.182  loss_mask_8: 0.6614  loss_dice_8: 4.612  loss_mars: 0.548    time: 5.7097  last_time: 5.3077  data_time: 0.0022  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 04:26:43] d2.utils.events INFO:  eta: 18:55:45  iter: 3779  total_loss: 91.52  loss_ce: 3.232  loss_mask: 1.336  loss_dice: 4.169  loss_ce_0: 3.368  loss_mask_0: 1.307  loss_dice_0: 4.071  loss_ce_1: 3.05  loss_mask_1: 1.24  loss_dice_1: 4.134  loss_ce_2: 2.921  loss_mask_2: 1.275  loss_dice_2: 4.23  loss_ce_3: 2.792  loss_mask_3: 1.375  loss_dice_3: 4.37  loss_ce_4: 3.286  loss_mask_4: 1.33  loss_dice_4: 4.136  loss_ce_5: 3.303  loss_mask_5: 1.379  loss_dice_5: 4.215  loss_ce_6: 3.168  loss_mask_6: 1.28  loss_dice_6: 4.337  loss_ce_7: 3.271  loss_mask_7: 1.438  loss_dice_7: 4.218  loss_ce_8: 3.16  loss_mask_8: 1.488  loss_dice_8: 4.238  loss_mars: 0.841    time: 5.7079  last_time: 6.0655  data_time: 0.0026  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 04:28:36] d2.utils.events INFO:  eta: 18:55:24  iter: 3799  total_loss: 87.66  loss_ce: 3.198  loss_mask: 1.383  loss_dice: 4.026  loss_ce_0: 3.376  loss_mask_0: 1.371  loss_dice_0: 4.032  loss_ce_1: 3.176  loss_mask_1: 1.393  loss_dice_1: 4.194  loss_ce_2: 2.917  loss_mask_2: 1.415  loss_dice_2: 4.246  loss_ce_3: 3.073  loss_mask_3: 1.411  loss_dice_3: 4.129  loss_ce_4: 3.121  loss_mask_4: 1.404  loss_dice_4: 4.119  loss_ce_5: 3.062  loss_mask_5: 1.483  loss_dice_5: 4.205  loss_ce_6: 3.037  loss_mask_6: 1.379  loss_dice_6: 4.164  loss_ce_7: 3.079  loss_mask_7: 1.431  loss_dice_7: 4.084  loss_ce_8: 3.196  loss_mask_8: 1.434  loss_dice_8: 4.051  loss_mars: 0.8394    time: 5.7073  last_time: 3.7500  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 04:30:28] d2.utils.events INFO:  eta: 18:57:11  iter: 3819  total_loss: 97.43  loss_ce: 3.857  loss_mask: 1.356  loss_dice: 4.197  loss_ce_0: 4.052  loss_mask_0: 1.269  loss_dice_0: 4.042  loss_ce_1: 3.522  loss_mask_1: 1.212  loss_dice_1: 4.186  loss_ce_2: 3.681  loss_mask_2: 1.331  loss_dice_2: 4.266  loss_ce_3: 3.661  loss_mask_3: 1.484  loss_dice_3: 4.408  loss_ce_4: 3.736  loss_mask_4: 1.399  loss_dice_4: 4.433  loss_ce_5: 4.016  loss_mask_5: 1.457  loss_dice_5: 4.256  loss_ce_6: 4.008  loss_mask_6: 1.377  loss_dice_6: 4.25  loss_ce_7: 3.884  loss_mask_7: 1.34  loss_dice_7: 4.223  loss_ce_8: 3.852  loss_mask_8: 1.462  loss_dice_8: 4.116  loss_mars: 0.7559    time: 5.7061  last_time: 6.0536  data_time: 0.0027  last_data_time: 0.0034   lr: 0.0001  max_mem: 0M
[10/19 04:32:18] d2.utils.events INFO:  eta: 18:50:45  iter: 3839  total_loss: 92.8  loss_ce: 3.48  loss_mask: 1.104  loss_dice: 4.304  loss_ce_0: 3.021  loss_mask_0: 1.126  loss_dice_0: 4.154  loss_ce_1: 3.068  loss_mask_1: 1.154  loss_dice_1: 4.335  loss_ce_2: 3.081  loss_mask_2: 1.057  loss_dice_2: 4.47  loss_ce_3: 3.157  loss_mask_3: 1.002  loss_dice_3: 4.487  loss_ce_4: 3.272  loss_mask_4: 1.128  loss_dice_4: 4.537  loss_ce_5: 3.203  loss_mask_5: 1.19  loss_dice_5: 4.432  loss_ce_6: 3.65  loss_mask_6: 1.089  loss_dice_6: 4.35  loss_ce_7: 3.599  loss_mask_7: 1.255  loss_dice_7: 4.21  loss_ce_8: 3.196  loss_mask_8: 1.156  loss_dice_8: 4.169  loss_mars: 0.7658    time: 5.7043  last_time: 6.0082  data_time: 0.0025  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 04:34:08] d2.utils.events INFO:  eta: 18:46:49  iter: 3859  total_loss: 90.06  loss_ce: 3.58  loss_mask: 1.058  loss_dice: 4.251  loss_ce_0: 3.937  loss_mask_0: 1.027  loss_dice_0: 4.172  loss_ce_1: 3.469  loss_mask_1: 1.054  loss_dice_1: 4.147  loss_ce_2: 3.55  loss_mask_2: 0.9658  loss_dice_2: 4.2  loss_ce_3: 3.473  loss_mask_3: 1.019  loss_dice_3: 4.2  loss_ce_4: 3.309  loss_mask_4: 0.9967  loss_dice_4: 4.284  loss_ce_5: 3.4  loss_mask_5: 1.093  loss_dice_5: 4.222  loss_ce_6: 3.319  loss_mask_6: 0.9289  loss_dice_6: 4.527  loss_ce_7: 3.274  loss_mask_7: 1.025  loss_dice_7: 4.451  loss_ce_8: 3.265  loss_mask_8: 1.005  loss_dice_8: 4.328  loss_mars: 0.6699    time: 5.7027  last_time: 5.7745  data_time: 0.0023  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 04:35:59] d2.utils.events INFO:  eta: 18:43:19  iter: 3879  total_loss: 88.56  loss_ce: 3.534  loss_mask: 1.108  loss_dice: 3.829  loss_ce_0: 3.684  loss_mask_0: 1.518  loss_dice_0: 3.859  loss_ce_1: 3.331  loss_mask_1: 1.124  loss_dice_1: 3.827  loss_ce_2: 3.411  loss_mask_2: 1.261  loss_dice_2: 3.893  loss_ce_3: 3.136  loss_mask_3: 1.181  loss_dice_3: 3.896  loss_ce_4: 3.171  loss_mask_4: 1.088  loss_dice_4: 3.972  loss_ce_5: 3.37  loss_mask_5: 1.202  loss_dice_5: 3.931  loss_ce_6: 3.362  loss_mask_6: 1.283  loss_dice_6: 4.215  loss_ce_7: 3.529  loss_mask_7: 1.102  loss_dice_7: 4.176  loss_ce_8: 3.586  loss_mask_8: 1.34  loss_dice_8: 3.841  loss_mars: 0.866    time: 5.7014  last_time: 3.7073  data_time: 0.0023  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 04:37:52] d2.utils.events INFO:  eta: 18:38:15  iter: 3899  total_loss: 83.7  loss_ce: 3.215  loss_mask: 0.7876  loss_dice: 3.867  loss_ce_0: 3.285  loss_mask_0: 0.7929  loss_dice_0: 4.137  loss_ce_1: 2.924  loss_mask_1: 0.8694  loss_dice_1: 3.863  loss_ce_2: 2.699  loss_mask_2: 0.8122  loss_dice_2: 3.979  loss_ce_3: 2.735  loss_mask_3: 0.7775  loss_dice_3: 3.994  loss_ce_4: 2.657  loss_mask_4: 0.8436  loss_dice_4: 4.063  loss_ce_5: 3.139  loss_mask_5: 0.7671  loss_dice_5: 4.101  loss_ce_6: 2.968  loss_mask_6: 0.7895  loss_dice_6: 4.146  loss_ce_7: 2.979  loss_mask_7: 0.8527  loss_dice_7: 4.139  loss_ce_8: 3.159  loss_mask_8: 0.7963  loss_dice_8: 4.029  loss_mars: 0.9001    time: 5.7008  last_time: 5.0225  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 04:39:34] d2.utils.events INFO:  eta: 18:33:56  iter: 3919  total_loss: 90.14  loss_ce: 3.613  loss_mask: 1.932  loss_dice: 3.991  loss_ce_0: 3.663  loss_mask_0: 1.213  loss_dice_0: 4.28  loss_ce_1: 3.869  loss_mask_1: 1.342  loss_dice_1: 4.154  loss_ce_2: 3.474  loss_mask_2: 1.168  loss_dice_2: 4.224  loss_ce_3: 3.639  loss_mask_3: 1.369  loss_dice_3: 4.14  loss_ce_4: 3.463  loss_mask_4: 1.191  loss_dice_4: 4.263  loss_ce_5: 3.631  loss_mask_5: 1.462  loss_dice_5: 4.18  loss_ce_6: 3.704  loss_mask_6: 1.391  loss_dice_6: 4.43  loss_ce_7: 3.849  loss_mask_7: 1.386  loss_dice_7: 4.197  loss_ce_8: 3.753  loss_mask_8: 1.688  loss_dice_8: 4.115  loss_mars: 0.7144    time: 5.6957  last_time: 5.5190  data_time: 0.0023  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 04:41:30] d2.utils.events INFO:  eta: 18:30:42  iter: 3939  total_loss: 80.9  loss_ce: 2.717  loss_mask: 1.156  loss_dice: 4.153  loss_ce_0: 3.31  loss_mask_0: 1.147  loss_dice_0: 4.253  loss_ce_1: 2.923  loss_mask_1: 1.234  loss_dice_1: 4.111  loss_ce_2: 2.772  loss_mask_2: 1.039  loss_dice_2: 4.03  loss_ce_3: 2.745  loss_mask_3: 1.082  loss_dice_3: 4.105  loss_ce_4: 2.919  loss_mask_4: 1.112  loss_dice_4: 4.22  loss_ce_5: 2.91  loss_mask_5: 1.054  loss_dice_5: 3.931  loss_ce_6: 2.552  loss_mask_6: 0.9553  loss_dice_6: 4.235  loss_ce_7: 2.812  loss_mask_7: 1.19  loss_dice_7: 4.181  loss_ce_8: 2.689  loss_mask_8: 1.326  loss_dice_8: 4.178  loss_mars: 0.812    time: 5.6966  last_time: 6.0006  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 04:43:21] d2.utils.events INFO:  eta: 18:28:48  iter: 3959  total_loss: 86.15  loss_ce: 2.761  loss_mask: 1.295  loss_dice: 4.198  loss_ce_0: 3.068  loss_mask_0: 1.054  loss_dice_0: 3.828  loss_ce_1: 2.764  loss_mask_1: 1.067  loss_dice_1: 3.964  loss_ce_2: 2.742  loss_mask_2: 1.078  loss_dice_2: 3.85  loss_ce_3: 3.064  loss_mask_3: 1.241  loss_dice_3: 3.949  loss_ce_4: 3.075  loss_mask_4: 1.266  loss_dice_4: 3.997  loss_ce_5: 3.001  loss_mask_5: 1.391  loss_dice_5: 3.939  loss_ce_6: 2.976  loss_mask_6: 1.359  loss_dice_6: 4.106  loss_ce_7: 2.984  loss_mask_7: 1.13  loss_dice_7: 4.072  loss_ce_8: 2.949  loss_mask_8: 1.301  loss_dice_8: 4.225  loss_mars: 0.9177    time: 5.6954  last_time: 5.2360  data_time: 0.0022  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 04:45:13] d2.utils.events INFO:  eta: 18:28:00  iter: 3979  total_loss: 88.76  loss_ce: 2.777  loss_mask: 1.586  loss_dice: 3.818  loss_ce_0: 3.265  loss_mask_0: 1.745  loss_dice_0: 3.764  loss_ce_1: 2.741  loss_mask_1: 1.58  loss_dice_1: 3.837  loss_ce_2: 2.849  loss_mask_2: 1.72  loss_dice_2: 3.843  loss_ce_3: 3.036  loss_mask_3: 1.39  loss_dice_3: 3.808  loss_ce_4: 2.983  loss_mask_4: 1.611  loss_dice_4: 3.824  loss_ce_5: 2.962  loss_mask_5: 1.715  loss_dice_5: 3.731  loss_ce_6: 2.854  loss_mask_6: 1.504  loss_dice_6: 3.922  loss_ce_7: 2.713  loss_mask_7: 1.681  loss_dice_7: 3.831  loss_ce_8: 3  loss_mask_8: 1.501  loss_dice_8: 3.831  loss_mars: 0.877    time: 5.6947  last_time: 5.2711  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 04:47:04] d2.utils.events INFO:  eta: 18:23:11  iter: 3999  total_loss: 79.92  loss_ce: 2.708  loss_mask: 1.209  loss_dice: 3.869  loss_ce_0: 3.04  loss_mask_0: 1.043  loss_dice_0: 4.044  loss_ce_1: 2.746  loss_mask_1: 0.9294  loss_dice_1: 4.028  loss_ce_2: 2.726  loss_mask_2: 0.9247  loss_dice_2: 4.135  loss_ce_3: 2.827  loss_mask_3: 0.9384  loss_dice_3: 4.149  loss_ce_4: 3.01  loss_mask_4: 0.8805  loss_dice_4: 4.072  loss_ce_5: 3.067  loss_mask_5: 0.9767  loss_dice_5: 4.034  loss_ce_6: 2.682  loss_mask_6: 1.044  loss_dice_6: 3.999  loss_ce_7: 2.624  loss_mask_7: 1.131  loss_dice_7: 4.014  loss_ce_8: 2.615  loss_mask_8: 1.174  loss_dice_8: 4.034  loss_mars: 0.8106    time: 5.6933  last_time: 3.7439  data_time: 0.0023  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 04:48:57] d2.utils.events INFO:  eta: 18:24:30  iter: 4019  total_loss: 87.48  loss_ce: 2.561  loss_mask: 1.333  loss_dice: 4.333  loss_ce_0: 3.212  loss_mask_0: 0.9369  loss_dice_0: 4.192  loss_ce_1: 2.755  loss_mask_1: 0.96  loss_dice_1: 4.107  loss_ce_2: 2.915  loss_mask_2: 0.9133  loss_dice_2: 4.188  loss_ce_3: 2.917  loss_mask_3: 0.9672  loss_dice_3: 3.994  loss_ce_4: 2.793  loss_mask_4: 0.9646  loss_dice_4: 4.06  loss_ce_5: 2.829  loss_mask_5: 0.9533  loss_dice_5: 4.241  loss_ce_6: 2.652  loss_mask_6: 1.349  loss_dice_6: 4.32  loss_ce_7: 2.635  loss_mask_7: 1.299  loss_dice_7: 4.384  loss_ce_8: 2.671  loss_mask_8: 1.15  loss_dice_8: 4.141  loss_mars: 0.7328    time: 5.6929  last_time: 5.9851  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 04:50:50] d2.utils.events INFO:  eta: 18:19:55  iter: 4039  total_loss: 90.28  loss_ce: 3.031  loss_mask: 1.603  loss_dice: 4.161  loss_ce_0: 4.125  loss_mask_0: 1.359  loss_dice_0: 4.16  loss_ce_1: 3.36  loss_mask_1: 1.366  loss_dice_1: 4.195  loss_ce_2: 3.478  loss_mask_2: 1.42  loss_dice_2: 4.133  loss_ce_3: 3.505  loss_mask_3: 1.388  loss_dice_3: 4.164  loss_ce_4: 3.426  loss_mask_4: 1.361  loss_dice_4: 4.163  loss_ce_5: 3.354  loss_mask_5: 1.381  loss_dice_5: 4.163  loss_ce_6: 3.385  loss_mask_6: 1.459  loss_dice_6: 4.245  loss_ce_7: 3.315  loss_mask_7: 1.549  loss_dice_7: 4.177  loss_ce_8: 3.172  loss_mask_8: 1.543  loss_dice_8: 4.172  loss_mars: 0.7627    time: 5.6927  last_time: 3.7580  data_time: 0.0026  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 04:52:41] d2.utils.events INFO:  eta: 18:18:01  iter: 4059  total_loss: 91.9  loss_ce: 3.41  loss_mask: 0.9297  loss_dice: 4.316  loss_ce_0: 3.309  loss_mask_0: 0.9457  loss_dice_0: 4.294  loss_ce_1: 3.342  loss_mask_1: 0.9645  loss_dice_1: 4.163  loss_ce_2: 3.582  loss_mask_2: 0.9642  loss_dice_2: 4.317  loss_ce_3: 3.384  loss_mask_3: 0.9551  loss_dice_3: 4.364  loss_ce_4: 3.413  loss_mask_4: 0.9365  loss_dice_4: 4.339  loss_ce_5: 3.408  loss_mask_5: 0.9928  loss_dice_5: 4.264  loss_ce_6: 3.828  loss_mask_6: 0.9574  loss_dice_6: 4.343  loss_ce_7: 3.46  loss_mask_7: 0.978  loss_dice_7: 4.26  loss_ce_8: 3.517  loss_mask_8: 0.9631  loss_dice_8: 4.262  loss_mars: 0.5871    time: 5.6914  last_time: 5.9421  data_time: 0.0024  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 04:54:31] d2.utils.events INFO:  eta: 18:18:30  iter: 4079  total_loss: 87.48  loss_ce: 2.484  loss_mask: 1.535  loss_dice: 3.921  loss_ce_0: 2.611  loss_mask_0: 1.326  loss_dice_0: 4.132  loss_ce_1: 2.688  loss_mask_1: 1.353  loss_dice_1: 3.905  loss_ce_2: 2.639  loss_mask_2: 1.383  loss_dice_2: 3.996  loss_ce_3: 2.621  loss_mask_3: 1.458  loss_dice_3: 3.907  loss_ce_4: 2.413  loss_mask_4: 1.358  loss_dice_4: 3.969  loss_ce_5: 2.434  loss_mask_5: 1.488  loss_dice_5: 4.08  loss_ce_6: 2.401  loss_mask_6: 1.379  loss_dice_6: 4.236  loss_ce_7: 2.347  loss_mask_7: 1.463  loss_dice_7: 4.266  loss_ce_8: 2.379  loss_mask_8: 1.414  loss_dice_8: 4.125  loss_mars: 0.7234    time: 5.6899  last_time: 6.1025  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 04:56:25] d2.utils.events INFO:  eta: 18:14:14  iter: 4099  total_loss: 89.03  loss_ce: 3.146  loss_mask: 1.41  loss_dice: 3.82  loss_ce_0: 3.399  loss_mask_0: 1.282  loss_dice_0: 3.967  loss_ce_1: 3.261  loss_mask_1: 1.261  loss_dice_1: 3.917  loss_ce_2: 3.317  loss_mask_2: 1.304  loss_dice_2: 3.968  loss_ce_3: 3.366  loss_mask_3: 1.503  loss_dice_3: 3.877  loss_ce_4: 3.255  loss_mask_4: 1.369  loss_dice_4: 3.838  loss_ce_5: 3.258  loss_mask_5: 1.512  loss_dice_5: 4.204  loss_ce_6: 3.213  loss_mask_6: 1.375  loss_dice_6: 3.783  loss_ce_7: 3.394  loss_mask_7: 1.705  loss_dice_7: 3.883  loss_ce_8: 3.305  loss_mask_8: 1.612  loss_dice_8: 3.856  loss_mars: 0.7456    time: 5.6903  last_time: 5.4588  data_time: 0.0022  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 04:58:20] d2.utils.events INFO:  eta: 18:11:48  iter: 4119  total_loss: 82.31  loss_ce: 2.577  loss_mask: 1.626  loss_dice: 3.815  loss_ce_0: 2.82  loss_mask_0: 1.684  loss_dice_0: 3.509  loss_ce_1: 2.734  loss_mask_1: 1.639  loss_dice_1: 3.41  loss_ce_2: 2.8  loss_mask_2: 1.613  loss_dice_2: 3.524  loss_ce_3: 2.69  loss_mask_3: 1.546  loss_dice_3: 3.208  loss_ce_4: 2.651  loss_mask_4: 1.696  loss_dice_4: 3.286  loss_ce_5: 2.61  loss_mask_5: 1.703  loss_dice_5: 3.255  loss_ce_6: 2.514  loss_mask_6: 1.83  loss_dice_6: 3.637  loss_ce_7: 2.51  loss_mask_7: 1.666  loss_dice_7: 3.505  loss_ce_8: 2.572  loss_mask_8: 1.779  loss_dice_8: 3.681  loss_mars: 0.7467    time: 5.6904  last_time: 6.3039  data_time: 0.0023  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 05:00:12] d2.utils.events INFO:  eta: 18:10:26  iter: 4139  total_loss: 82.28  loss_ce: 2.377  loss_mask: 1.448  loss_dice: 4.034  loss_ce_0: 2.553  loss_mask_0: 1.361  loss_dice_0: 4.08  loss_ce_1: 2.453  loss_mask_1: 1.283  loss_dice_1: 4.026  loss_ce_2: 2.447  loss_mask_2: 1.271  loss_dice_2: 4.028  loss_ce_3: 2.292  loss_mask_3: 1.445  loss_dice_3: 3.98  loss_ce_4: 2.53  loss_mask_4: 1.353  loss_dice_4: 3.969  loss_ce_5: 2.235  loss_mask_5: 1.453  loss_dice_5: 4.058  loss_ce_6: 2.305  loss_mask_6: 1.722  loss_dice_6: 4.396  loss_ce_7: 2.376  loss_mask_7: 1.433  loss_dice_7: 4.148  loss_ce_8: 2.524  loss_mask_8: 1.496  loss_dice_8: 3.989  loss_mars: 0.7471    time: 5.6900  last_time: 5.7358  data_time: 0.0024  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 05:02:04] d2.utils.events INFO:  eta: 18:08:19  iter: 4159  total_loss: 96.35  loss_ce: 4.326  loss_mask: 0.8326  loss_dice: 4.412  loss_ce_0: 5.147  loss_mask_0: 0.8637  loss_dice_0: 4.554  loss_ce_1: 4.204  loss_mask_1: 0.8628  loss_dice_1: 4.355  loss_ce_2: 4.374  loss_mask_2: 0.8453  loss_dice_2: 4.471  loss_ce_3: 4.234  loss_mask_3: 0.9088  loss_dice_3: 4.334  loss_ce_4: 4.243  loss_mask_4: 0.8297  loss_dice_4: 4.353  loss_ce_5: 4.286  loss_mask_5: 0.867  loss_dice_5: 4.38  loss_ce_6: 4.039  loss_mask_6: 1.004  loss_dice_6: 4.407  loss_ce_7: 4.295  loss_mask_7: 0.8835  loss_dice_7: 4.387  loss_ce_8: 4.288  loss_mask_8: 0.8075  loss_dice_8: 4.33  loss_mars: 0.6778    time: 5.6892  last_time: 5.8061  data_time: 0.0027  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 05:03:58] d2.utils.events INFO:  eta: 18:05:39  iter: 4179  total_loss: 96.27  loss_ce: 4.215  loss_mask: 0.7004  loss_dice: 4.618  loss_ce_0: 4.71  loss_mask_0: 0.7974  loss_dice_0: 4.639  loss_ce_1: 4.232  loss_mask_1: 0.7109  loss_dice_1: 4.538  loss_ce_2: 4.281  loss_mask_2: 0.7265  loss_dice_2: 4.561  loss_ce_3: 4.136  loss_mask_3: 0.8022  loss_dice_3: 4.591  loss_ce_4: 4.089  loss_mask_4: 0.7148  loss_dice_4: 4.566  loss_ce_5: 4.219  loss_mask_5: 0.7749  loss_dice_5: 4.566  loss_ce_6: 4.162  loss_mask_6: 0.7279  loss_dice_6: 4.555  loss_ce_7: 4.18  loss_mask_7: 0.6925  loss_dice_7: 4.537  loss_ce_8: 4.444  loss_mask_8: 0.7483  loss_dice_8: 4.511  loss_mars: 0.4613    time: 5.6891  last_time: 5.8675  data_time: 0.0026  last_data_time: 0.0038   lr: 0.0001  max_mem: 0M
[10/19 05:05:51] d2.utils.events INFO:  eta: 18:05:27  iter: 4199  total_loss: 99.33  loss_ce: 3.656  loss_mask: 1.064  loss_dice: 4.553  loss_ce_0: 4.071  loss_mask_0: 1.179  loss_dice_0: 4.422  loss_ce_1: 3.724  loss_mask_1: 1.063  loss_dice_1: 4.36  loss_ce_2: 3.751  loss_mask_2: 1.227  loss_dice_2: 4.456  loss_ce_3: 3.941  loss_mask_3: 1.158  loss_dice_3: 4.564  loss_ce_4: 3.974  loss_mask_4: 1.227  loss_dice_4: 4.489  loss_ce_5: 3.879  loss_mask_5: 1.155  loss_dice_5: 4.435  loss_ce_6: 3.513  loss_mask_6: 1.123  loss_dice_6: 4.617  loss_ce_7: 3.571  loss_mask_7: 1.164  loss_dice_7: 4.492  loss_ce_8: 3.663  loss_mask_8: 1.193  loss_dice_8: 4.503  loss_mars: 0.3809    time: 5.6889  last_time: 5.9464  data_time: 0.0025  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 05:07:45] d2.utils.events INFO:  eta: 18:05:46  iter: 4219  total_loss: 95.49  loss_ce: 3.514  loss_mask: 0.9601  loss_dice: 4.529  loss_ce_0: 3.71  loss_mask_0: 1.028  loss_dice_0: 4.389  loss_ce_1: 3.408  loss_mask_1: 0.9385  loss_dice_1: 4.454  loss_ce_2: 3.618  loss_mask_2: 1.082  loss_dice_2: 4.351  loss_ce_3: 3.74  loss_mask_3: 1.024  loss_dice_3: 4.185  loss_ce_4: 3.719  loss_mask_4: 1.029  loss_dice_4: 4.352  loss_ce_5: 3.545  loss_mask_5: 1.015  loss_dice_5: 4.372  loss_ce_6: 3.422  loss_mask_6: 0.9954  loss_dice_6: 4.519  loss_ce_7: 3.494  loss_mask_7: 0.9964  loss_dice_7: 4.505  loss_ce_8: 3.518  loss_mask_8: 1.007  loss_dice_8: 4.541  loss_mars: 0.5987    time: 5.6889  last_time: 5.8849  data_time: 0.0023  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 05:09:35] d2.utils.events INFO:  eta: 18:03:52  iter: 4239  total_loss: 95.46  loss_ce: 3.451  loss_mask: 1.605  loss_dice: 4.149  loss_ce_0: 3.705  loss_mask_0: 1.383  loss_dice_0: 4.142  loss_ce_1: 3.693  loss_mask_1: 1.555  loss_dice_1: 4.189  loss_ce_2: 3.782  loss_mask_2: 1.434  loss_dice_2: 4.061  loss_ce_3: 3.696  loss_mask_3: 1.44  loss_dice_3: 4.205  loss_ce_4: 3.865  loss_mask_4: 1.384  loss_dice_4: 4.344  loss_ce_5: 3.656  loss_mask_5: 1.376  loss_dice_5: 4.299  loss_ce_6: 3.429  loss_mask_6: 1.439  loss_dice_6: 4.225  loss_ce_7: 3.333  loss_mask_7: 1.481  loss_dice_7: 4.164  loss_ce_8: 3.261  loss_mask_8: 1.727  loss_dice_8: 4.285  loss_mars: 0.7352    time: 5.6877  last_time: 4.8683  data_time: 0.0024  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 05:11:31] d2.utils.events INFO:  eta: 18:02:29  iter: 4259  total_loss: 90.57  loss_ce: 2.781  loss_mask: 1.255  loss_dice: 4.231  loss_ce_0: 2.778  loss_mask_0: 1.037  loss_dice_0: 4.03  loss_ce_1: 3.005  loss_mask_1: 1.059  loss_dice_1: 4.159  loss_ce_2: 3.167  loss_mask_2: 1.069  loss_dice_2: 3.824  loss_ce_3: 3.43  loss_mask_3: 1.206  loss_dice_3: 3.82  loss_ce_4: 3.395  loss_mask_4: 1.033  loss_dice_4: 3.979  loss_ce_5: 3.771  loss_mask_5: 1.162  loss_dice_5: 4.109  loss_ce_6: 3.294  loss_mask_6: 1.376  loss_dice_6: 4.337  loss_ce_7: 2.894  loss_mask_7: 1.351  loss_dice_7: 4.5  loss_ce_8: 2.878  loss_mask_8: 1.267  loss_dice_8: 4.168  loss_mars: 0.3525    time: 5.6884  last_time: 7.9548  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 05:13:21] d2.utils.events INFO:  eta: 17:59:40  iter: 4279  total_loss: 83.59  loss_ce: 2.171  loss_mask: 1.392  loss_dice: 4.331  loss_ce_0: 2.709  loss_mask_0: 1.315  loss_dice_0: 4.304  loss_ce_1: 2.402  loss_mask_1: 1.345  loss_dice_1: 4.332  loss_ce_2: 2.344  loss_mask_2: 1.293  loss_dice_2: 4.305  loss_ce_3: 2.501  loss_mask_3: 1.338  loss_dice_3: 4.202  loss_ce_4: 2.455  loss_mask_4: 1.364  loss_dice_4: 4.286  loss_ce_5: 2.837  loss_mask_5: 1.245  loss_dice_5: 4.205  loss_ce_6: 2.581  loss_mask_6: 1.492  loss_dice_6: 4.203  loss_ce_7: 2.274  loss_mask_7: 1.527  loss_dice_7: 4.228  loss_ce_8: 2.218  loss_mask_8: 1.38  loss_dice_8: 4.397  loss_mars: 0.4482    time: 5.6870  last_time: 5.1764  data_time: 0.0022  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 05:15:06] d2.utils.events INFO:  eta: 17:53:59  iter: 4299  total_loss: 81.28  loss_ce: 2.193  loss_mask: 1.55  loss_dice: 4.05  loss_ce_0: 2.487  loss_mask_0: 1.557  loss_dice_0: 3.978  loss_ce_1: 2.274  loss_mask_1: 1.347  loss_dice_1: 3.879  loss_ce_2: 2.403  loss_mask_2: 1.253  loss_dice_2: 3.902  loss_ce_3: 2.553  loss_mask_3: 1.23  loss_dice_3: 3.81  loss_ce_4: 2.467  loss_mask_4: 1.654  loss_dice_4: 3.965  loss_ce_5: 2.398  loss_mask_5: 1.508  loss_dice_5: 3.863  loss_ce_6: 2.191  loss_mask_6: 1.711  loss_dice_6: 3.833  loss_ce_7: 2.192  loss_mask_7: 1.546  loss_dice_7: 3.945  loss_ce_8: 2.199  loss_mask_8: 1.536  loss_dice_8: 3.902  loss_mars: 0.5744    time: 5.6839  last_time: 5.2429  data_time: 0.0023  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 05:16:55] d2.utils.events INFO:  eta: 17:51:33  iter: 4319  total_loss: 89.04  loss_ce: 3.173  loss_mask: 1.234  loss_dice: 4.081  loss_ce_0: 3.527  loss_mask_0: 1.196  loss_dice_0: 4.193  loss_ce_1: 3.456  loss_mask_1: 1.199  loss_dice_1: 4.081  loss_ce_2: 3.507  loss_mask_2: 1.225  loss_dice_2: 4.094  loss_ce_3: 3.485  loss_mask_3: 1.197  loss_dice_3: 4.192  loss_ce_4: 3.298  loss_mask_4: 1.187  loss_dice_4: 4.09  loss_ce_5: 3.526  loss_mask_5: 1.226  loss_dice_5: 4.254  loss_ce_6: 3.356  loss_mask_6: 1.219  loss_dice_6: 4.155  loss_ce_7: 3.153  loss_mask_7: 1.175  loss_dice_7: 4.276  loss_ce_8: 3.173  loss_mask_8: 1.226  loss_dice_8: 4.374  loss_mars: 0.4515    time: 5.6820  last_time: 4.7007  data_time: 0.0023  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 05:18:43] d2.utils.events INFO:  eta: 17:48:16  iter: 4339  total_loss: 86.36  loss_ce: 2.918  loss_mask: 1.074  loss_dice: 3.922  loss_ce_0: 3.064  loss_mask_0: 1.016  loss_dice_0: 3.887  loss_ce_1: 2.991  loss_mask_1: 1.081  loss_dice_1: 3.796  loss_ce_2: 3.353  loss_mask_2: 1.065  loss_dice_2: 3.765  loss_ce_3: 3.278  loss_mask_3: 1.043  loss_dice_3: 3.929  loss_ce_4: 3.12  loss_mask_4: 1.057  loss_dice_4: 3.9  loss_ce_5: 3.219  loss_mask_5: 1.121  loss_dice_5: 3.92  loss_ce_6: 2.898  loss_mask_6: 1.176  loss_dice_6: 3.943  loss_ce_7: 2.98  loss_mask_7: 1.07  loss_dice_7: 4.144  loss_ce_8: 2.866  loss_mask_8: 1.036  loss_dice_8: 3.82  loss_mars: 0.7475    time: 5.6799  last_time: 5.6444  data_time: 0.0022  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 05:20:37] d2.utils.events INFO:  eta: 17:48:18  iter: 4359  total_loss: 94.68  loss_ce: 3.052  loss_mask: 1.317  loss_dice: 4.214  loss_ce_0: 3.665  loss_mask_0: 1.374  loss_dice_0: 4.339  loss_ce_1: 3.382  loss_mask_1: 1.393  loss_dice_1: 4.261  loss_ce_2: 3.608  loss_mask_2: 1.451  loss_dice_2: 4.254  loss_ce_3: 3.406  loss_mask_3: 1.581  loss_dice_3: 4.148  loss_ce_4: 3.22  loss_mask_4: 1.509  loss_dice_4: 4.201  loss_ce_5: 3.475  loss_mask_5: 1.506  loss_dice_5: 4.188  loss_ce_6: 2.957  loss_mask_6: 1.483  loss_dice_6: 4.384  loss_ce_7: 3.041  loss_mask_7: 1.466  loss_dice_7: 4.307  loss_ce_8: 3.044  loss_mask_8: 1.475  loss_dice_8: 4.172  loss_mars: 0.8704    time: 5.6801  last_time: 6.2176  data_time: 0.0023  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 05:22:29] d2.utils.events INFO:  eta: 17:41:07  iter: 4379  total_loss: 97.41  loss_ce: 4.16  loss_mask: 1.099  loss_dice: 4.233  loss_ce_0: 5.187  loss_mask_0: 0.8841  loss_dice_0: 4.21  loss_ce_1: 4.363  loss_mask_1: 0.8567  loss_dice_1: 4.271  loss_ce_2: 4.587  loss_mask_2: 0.9046  loss_dice_2: 4.231  loss_ce_3: 4.445  loss_mask_3: 0.8438  loss_dice_3: 4.16  loss_ce_4: 4.382  loss_mask_4: 0.9067  loss_dice_4: 4.268  loss_ce_5: 4.258  loss_mask_5: 1.03  loss_dice_5: 4.169  loss_ce_6: 3.986  loss_mask_6: 1.183  loss_dice_6: 4.245  loss_ce_7: 4.134  loss_mask_7: 1.092  loss_dice_7: 4.205  loss_ce_8: 4.315  loss_mask_8: 0.8951  loss_dice_8: 4.261  loss_mars: 0.8564    time: 5.6796  last_time: 5.2662  data_time: 0.0026  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 05:24:24] d2.utils.events INFO:  eta: 17:39:14  iter: 4399  total_loss: 98.71  loss_ce: 3.967  loss_mask: 1.099  loss_dice: 4.382  loss_ce_0: 4.514  loss_mask_0: 1.058  loss_dice_0: 4.459  loss_ce_1: 4.077  loss_mask_1: 1.141  loss_dice_1: 4.334  loss_ce_2: 4.421  loss_mask_2: 1.156  loss_dice_2: 4.352  loss_ce_3: 4.19  loss_mask_3: 1.175  loss_dice_3: 4.398  loss_ce_4: 4.119  loss_mask_4: 1.182  loss_dice_4: 4.29  loss_ce_5: 3.995  loss_mask_5: 1.216  loss_dice_5: 4.427  loss_ce_6: 3.911  loss_mask_6: 1.131  loss_dice_6: 4.411  loss_ce_7: 4.079  loss_mask_7: 1.115  loss_dice_7: 4.434  loss_ce_8: 4.061  loss_mask_8: 1.178  loss_dice_8: 4.442  loss_mars: 0.9102    time: 5.6800  last_time: 6.0885  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 05:26:20] d2.utils.events INFO:  eta: 17:41:17  iter: 4419  total_loss: 79.92  loss_ce: 2.838  loss_mask: 1.325  loss_dice: 3.951  loss_ce_0: 2.964  loss_mask_0: 1.38  loss_dice_0: 3.937  loss_ce_1: 2.776  loss_mask_1: 1.534  loss_dice_1: 4.185  loss_ce_2: 3.01  loss_mask_2: 1.438  loss_dice_2: 4.067  loss_ce_3: 2.813  loss_mask_3: 1.461  loss_dice_3: 3.972  loss_ce_4: 3.018  loss_mask_4: 1.377  loss_dice_4: 4.044  loss_ce_5: 2.968  loss_mask_5: 1.399  loss_dice_5: 3.978  loss_ce_6: 2.796  loss_mask_6: 1.332  loss_dice_6: 4.019  loss_ce_7: 2.969  loss_mask_7: 1.337  loss_dice_7: 3.924  loss_ce_8: 3.146  loss_mask_8: 1.395  loss_dice_8: 3.992  loss_mars: 0.709    time: 5.6811  last_time: 6.0937  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 05:28:11] d2.utils.events INFO:  eta: 17:31:58  iter: 4439  total_loss: 95.34  loss_ce: 3.464  loss_mask: 1.17  loss_dice: 4.11  loss_ce_0: 3.592  loss_mask_0: 1.606  loss_dice_0: 4.369  loss_ce_1: 3.1  loss_mask_1: 1.513  loss_dice_1: 4.217  loss_ce_2: 3.15  loss_mask_2: 1.269  loss_dice_2: 4.189  loss_ce_3: 3.192  loss_mask_3: 1.36  loss_dice_3: 4.472  loss_ce_4: 3.264  loss_mask_4: 1.506  loss_dice_4: 4.368  loss_ce_5: 3.49  loss_mask_5: 1.627  loss_dice_5: 4.361  loss_ce_6: 3.385  loss_mask_6: 1.283  loss_dice_6: 4.166  loss_ce_7: 3.415  loss_mask_7: 1.305  loss_dice_7: 4.148  loss_ce_8: 3.421  loss_mask_8: 1.324  loss_dice_8: 4.112  loss_mars: 0.5179    time: 5.6801  last_time: 5.8781  data_time: 0.0027  last_data_time: 0.0042   lr: 0.0001  max_mem: 0M
[10/19 05:30:01] d2.utils.events INFO:  eta: 17:26:08  iter: 4459  total_loss: 90.58  loss_ce: 3.692  loss_mask: 1.063  loss_dice: 3.894  loss_ce_0: 3.358  loss_mask_0: 0.8482  loss_dice_0: 4.165  loss_ce_1: 3.121  loss_mask_1: 0.9175  loss_dice_1: 4.292  loss_ce_2: 3.345  loss_mask_2: 0.9727  loss_dice_2: 3.972  loss_ce_3: 3.346  loss_mask_3: 0.996  loss_dice_3: 4.367  loss_ce_4: 3.502  loss_mask_4: 1.059  loss_dice_4: 3.953  loss_ce_5: 3.653  loss_mask_5: 1.085  loss_dice_5: 3.87  loss_ce_6: 3.694  loss_mask_6: 1.001  loss_dice_6: 3.916  loss_ce_7: 3.606  loss_mask_7: 0.9402  loss_dice_7: 3.906  loss_ce_8: 3.544  loss_mask_8: 1.035  loss_dice_8: 3.778  loss_mars: 0.7348    time: 5.6787  last_time: 7.2129  data_time: 0.0023  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 05:31:53] d2.utils.events INFO:  eta: 17:19:04  iter: 4479  total_loss: 88.77  loss_ce: 3.515  loss_mask: 1.5  loss_dice: 3.975  loss_ce_0: 3.73  loss_mask_0: 1.706  loss_dice_0: 4.054  loss_ce_1: 3.128  loss_mask_1: 1.446  loss_dice_1: 3.974  loss_ce_2: 3.196  loss_mask_2: 1.555  loss_dice_2: 4.136  loss_ce_3: 3.024  loss_mask_3: 1.815  loss_dice_3: 4.151  loss_ce_4: 3.171  loss_mask_4: 1.556  loss_dice_4: 4.032  loss_ce_5: 3.361  loss_mask_5: 1.53  loss_dice_5: 4.012  loss_ce_6: 3.406  loss_mask_6: 1.568  loss_dice_6: 3.928  loss_ce_7: 3.396  loss_mask_7: 1.534  loss_dice_7: 3.918  loss_ce_8: 3.466  loss_mask_8: 1.626  loss_dice_8: 3.921  loss_mars: 0.7587    time: 5.6782  last_time: 6.0725  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 05:33:49] d2.utils.events INFO:  eta: 17:21:02  iter: 4499  total_loss: 89.97  loss_ce: 3.206  loss_mask: 1.264  loss_dice: 3.996  loss_ce_0: 3.672  loss_mask_0: 1.239  loss_dice_0: 4.061  loss_ce_1: 2.975  loss_mask_1: 1.235  loss_dice_1: 4.121  loss_ce_2: 2.953  loss_mask_2: 1.26  loss_dice_2: 4.014  loss_ce_3: 2.804  loss_mask_3: 1.151  loss_dice_3: 4.242  loss_ce_4: 3.14  loss_mask_4: 1.162  loss_dice_4: 3.981  loss_ce_5: 3.329  loss_mask_5: 1.348  loss_dice_5: 3.944  loss_ce_6: 3.171  loss_mask_6: 1.2  loss_dice_6: 3.993  loss_ce_7: 3.271  loss_mask_7: 1.246  loss_dice_7: 3.951  loss_ce_8: 3.295  loss_mask_8: 1.188  loss_dice_8: 3.993  loss_mars: 0.7656    time: 5.6790  last_time: 5.3846  data_time: 0.0027  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/19 05:35:41] d2.utils.events INFO:  eta: 17:21:21  iter: 4519  total_loss: 98.16  loss_ce: 3.836  loss_mask: 0.995  loss_dice: 4.301  loss_ce_0: 5.06  loss_mask_0: 0.8068  loss_dice_0: 4.351  loss_ce_1: 3.554  loss_mask_1: 0.8495  loss_dice_1: 4.435  loss_ce_2: 3.715  loss_mask_2: 1.099  loss_dice_2: 4.379  loss_ce_3: 3.585  loss_mask_3: 1.096  loss_dice_3: 4.425  loss_ce_4: 3.752  loss_mask_4: 1.072  loss_dice_4: 4.303  loss_ce_5: 4.079  loss_mask_5: 0.9015  loss_dice_5: 4.31  loss_ce_6: 3.955  loss_mask_6: 0.9791  loss_dice_6: 4.268  loss_ce_7: 4.071  loss_mask_7: 0.9623  loss_dice_7: 4.455  loss_ce_8: 4.007  loss_mask_8: 0.9451  loss_dice_8: 4.379  loss_mars: 0.2718    time: 5.6784  last_time: 5.2160  data_time: 0.0026  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 05:37:39] d2.utils.events INFO:  eta: 17:15:26  iter: 4539  total_loss: 84.11  loss_ce: 2.899  loss_mask: 1.793  loss_dice: 3.855  loss_ce_0: 2.583  loss_mask_0: 1.822  loss_dice_0: 3.906  loss_ce_1: 2.521  loss_mask_1: 1.705  loss_dice_1: 3.82  loss_ce_2: 2.683  loss_mask_2: 1.901  loss_dice_2: 3.851  loss_ce_3: 2.486  loss_mask_3: 1.63  loss_dice_3: 3.634  loss_ce_4: 2.482  loss_mask_4: 1.62  loss_dice_4: 3.762  loss_ce_5: 2.767  loss_mask_5: 1.668  loss_dice_5: 3.703  loss_ce_6: 2.664  loss_mask_6: 1.729  loss_dice_6: 4.032  loss_ce_7: 2.855  loss_mask_7: 1.729  loss_dice_7: 3.987  loss_ce_8: 2.943  loss_mask_8: 1.614  loss_dice_8: 4.038  loss_mars: 0.7116    time: 5.6799  last_time: 5.3033  data_time: 0.0022  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 05:39:35] d2.utils.events INFO:  eta: 17:13:34  iter: 4559  total_loss: 94.62  loss_ce: 3.603  loss_mask: 1.159  loss_dice: 4.369  loss_ce_0: 3.499  loss_mask_0: 1.077  loss_dice_0: 4.223  loss_ce_1: 3.65  loss_mask_1: 1.068  loss_dice_1: 4.274  loss_ce_2: 3.63  loss_mask_2: 1.093  loss_dice_2: 4.171  loss_ce_3: 3.358  loss_mask_3: 1.111  loss_dice_3: 4.401  loss_ce_4: 3.551  loss_mask_4: 1.124  loss_dice_4: 4.462  loss_ce_5: 3.669  loss_mask_5: 1.09  loss_dice_5: 4.361  loss_ce_6: 3.697  loss_mask_6: 1.364  loss_dice_6: 4.214  loss_ce_7: 3.618  loss_mask_7: 1.173  loss_dice_7: 4.302  loss_ce_8: 3.671  loss_mask_8: 1.175  loss_dice_8: 4.417  loss_mars: 0.6765    time: 5.6806  last_time: 6.2527  data_time: 0.0022  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 05:41:28] d2.utils.events INFO:  eta: 17:14:53  iter: 4579  total_loss: 97.03  loss_ce: 3.816  loss_mask: 1.272  loss_dice: 4.257  loss_ce_0: 4.076  loss_mask_0: 1.285  loss_dice_0: 4.197  loss_ce_1: 3.694  loss_mask_1: 1.279  loss_dice_1: 4.196  loss_ce_2: 3.876  loss_mask_2: 1.227  loss_dice_2: 4.05  loss_ce_3: 3.761  loss_mask_3: 1.224  loss_dice_3: 4.171  loss_ce_4: 4.016  loss_mask_4: 1.25  loss_dice_4: 4.131  loss_ce_5: 4.074  loss_mask_5: 1.273  loss_dice_5: 4.121  loss_ce_6: 3.745  loss_mask_6: 1.566  loss_dice_6: 4.158  loss_ce_7: 3.644  loss_mask_7: 1.306  loss_dice_7: 4.145  loss_ce_8: 3.772  loss_mask_8: 1.268  loss_dice_8: 4.219  loss_mars: 0.9224    time: 5.6806  last_time: 5.2898  data_time: 0.0029  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 05:43:21] d2.utils.events INFO:  eta: 17:15:33  iter: 4599  total_loss: 82.95  loss_ce: 3.16  loss_mask: 1.257  loss_dice: 4.066  loss_ce_0: 3.575  loss_mask_0: 1.162  loss_dice_0: 4.097  loss_ce_1: 2.974  loss_mask_1: 1.224  loss_dice_1: 4.134  loss_ce_2: 2.819  loss_mask_2: 1.147  loss_dice_2: 4.123  loss_ce_3: 2.765  loss_mask_3: 1.263  loss_dice_3: 4.184  loss_ce_4: 2.97  loss_mask_4: 1.281  loss_dice_4: 4.111  loss_ce_5: 3.108  loss_mask_5: 1.313  loss_dice_5: 4.177  loss_ce_6: 2.898  loss_mask_6: 1.259  loss_dice_6: 4.109  loss_ce_7: 2.968  loss_mask_7: 1.243  loss_dice_7: 4.165  loss_ce_8: 3.2  loss_mask_8: 1.222  loss_dice_8: 4.066  loss_mars: 0.9484    time: 5.6804  last_time: 5.8356  data_time: 0.0023  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 05:45:15] d2.utils.events INFO:  eta: 17:20:19  iter: 4619  total_loss: 92.14  loss_ce: 3.508  loss_mask: 1.772  loss_dice: 3.748  loss_ce_0: 3.046  loss_mask_0: 1.757  loss_dice_0: 3.846  loss_ce_1: 3.097  loss_mask_1: 1.643  loss_dice_1: 3.752  loss_ce_2: 3.032  loss_mask_2: 1.609  loss_dice_2: 3.605  loss_ce_3: 2.996  loss_mask_3: 1.593  loss_dice_3: 3.663  loss_ce_4: 3.055  loss_mask_4: 1.852  loss_dice_4: 3.644  loss_ce_5: 3.339  loss_mask_5: 2.029  loss_dice_5: 3.539  loss_ce_6: 3.156  loss_mask_6: 1.715  loss_dice_6: 3.777  loss_ce_7: 3.373  loss_mask_7: 1.771  loss_dice_7: 3.825  loss_ce_8: 3.358  loss_mask_8: 1.757  loss_dice_8: 3.693  loss_mars: 0.9237    time: 5.6806  last_time: 5.9975  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 05:47:09] d2.utils.events INFO:  eta: 17:13:10  iter: 4639  total_loss: 83.25  loss_ce: 3.272  loss_mask: 1.009  loss_dice: 3.4  loss_ce_0: 3.406  loss_mask_0: 0.8831  loss_dice_0: 3.579  loss_ce_1: 3.101  loss_mask_1: 1.021  loss_dice_1: 3.384  loss_ce_2: 3.126  loss_mask_2: 1.17  loss_dice_2: 3.661  loss_ce_3: 2.943  loss_mask_3: 1.104  loss_dice_3: 3.438  loss_ce_4: 3.155  loss_mask_4: 1.047  loss_dice_4: 3.576  loss_ce_5: 3.147  loss_mask_5: 0.8926  loss_dice_5: 3.66  loss_ce_6: 2.897  loss_mask_6: 0.9047  loss_dice_6: 3.546  loss_ce_7: 3.109  loss_mask_7: 0.8743  loss_dice_7: 3.432  loss_ce_8: 3.123  loss_mask_8: 0.8476  loss_dice_8: 3.454  loss_mars: 0.8061    time: 5.6805  last_time: 6.2664  data_time: 0.0021  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 05:49:06] d2.utils.events INFO:  eta: 17:18:33  iter: 4659  total_loss: 98.72  loss_ce: 4.236  loss_mask: 1.128  loss_dice: 4.185  loss_ce_0: 4.473  loss_mask_0: 1.163  loss_dice_0: 4.154  loss_ce_1: 4.077  loss_mask_1: 1.318  loss_dice_1: 4.236  loss_ce_2: 3.826  loss_mask_2: 1.354  loss_dice_2: 4.285  loss_ce_3: 3.703  loss_mask_3: 1.366  loss_dice_3: 4.326  loss_ce_4: 3.936  loss_mask_4: 1.173  loss_dice_4: 4.295  loss_ce_5: 4.133  loss_mask_5: 1.26  loss_dice_5: 4.319  loss_ce_6: 3.904  loss_mask_6: 1.29  loss_dice_6: 4.224  loss_ce_7: 4.339  loss_mask_7: 1.157  loss_dice_7: 4.096  loss_ce_8: 4.221  loss_mask_8: 1.221  loss_dice_8: 4.304  loss_mars: 0.4376    time: 5.6817  last_time: 6.0831  data_time: 0.0027  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 05:50:55] d2.utils.events INFO:  eta: 17:08:38  iter: 4679  total_loss: 91.37  loss_ce: 4.188  loss_mask: 1.052  loss_dice: 4.008  loss_ce_0: 4.285  loss_mask_0: 1.114  loss_dice_0: 4.162  loss_ce_1: 3.886  loss_mask_1: 1.013  loss_dice_1: 4.076  loss_ce_2: 3.833  loss_mask_2: 1.146  loss_dice_2: 4.114  loss_ce_3: 3.697  loss_mask_3: 1.071  loss_dice_3: 4.051  loss_ce_4: 3.709  loss_mask_4: 1.144  loss_dice_4: 4.128  loss_ce_5: 3.771  loss_mask_5: 1.082  loss_dice_5: 4.038  loss_ce_6: 3.549  loss_mask_6: 1.242  loss_dice_6: 4.042  loss_ce_7: 4.204  loss_mask_7: 1.062  loss_dice_7: 3.988  loss_ce_8: 4.183  loss_mask_8: 1.003  loss_dice_8: 4.15  loss_mars: 0.7179    time: 5.6801  last_time: 5.0295  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 05:51:42] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0004688.pth
[10/19 05:51:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 05:51:42] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 05:51:43] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 05:51:43] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 05:52:46] d2.utils.events INFO:  eta: 17:06:09  iter: 4699  total_loss: 88.44  loss_ce: 2.903  loss_mask: 1.537  loss_dice: 4.168  loss_ce_0: 3.194  loss_mask_0: 1.47  loss_dice_0: 4.088  loss_ce_1: 3.348  loss_mask_1: 1.546  loss_dice_1: 4.14  loss_ce_2: 3.142  loss_mask_2: 1.608  loss_dice_2: 3.9  loss_ce_3: 2.887  loss_mask_3: 1.55  loss_dice_3: 3.962  loss_ce_4: 2.908  loss_mask_4: 1.671  loss_dice_4: 4.151  loss_ce_5: 2.927  loss_mask_5: 1.74  loss_dice_5: 4.148  loss_ce_6: 2.864  loss_mask_6: 1.679  loss_dice_6: 4.149  loss_ce_7: 3.147  loss_mask_7: 1.477  loss_dice_7: 4.117  loss_ce_8: 3.036  loss_mask_8: 1.733  loss_dice_8: 4.244  loss_mars: 0.482    time: 5.6788  last_time: 5.2546  data_time: 0.0022  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 05:54:39] d2.utils.events INFO:  eta: 17:04:17  iter: 4719  total_loss: 85.53  loss_ce: 3.036  loss_mask: 1.668  loss_dice: 3.999  loss_ce_0: 3.496  loss_mask_0: 1.362  loss_dice_0: 3.997  loss_ce_1: 3.065  loss_mask_1: 1.439  loss_dice_1: 4.042  loss_ce_2: 2.911  loss_mask_2: 1.432  loss_dice_2: 3.968  loss_ce_3: 2.883  loss_mask_3: 1.324  loss_dice_3: 4.14  loss_ce_4: 2.904  loss_mask_4: 1.365  loss_dice_4: 4.176  loss_ce_5: 2.836  loss_mask_5: 1.701  loss_dice_5: 3.996  loss_ce_6: 2.531  loss_mask_6: 1.818  loss_dice_6: 4.198  loss_ce_7: 2.615  loss_mask_7: 1.422  loss_dice_7: 4.04  loss_ce_8: 2.839  loss_mask_8: 1.356  loss_dice_8: 3.961  loss_mars: 0.6821    time: 5.6786  last_time: 7.3639  data_time: 0.0023  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 05:56:28] d2.utils.events INFO:  eta: 16:59:10  iter: 4739  total_loss: 84.43  loss_ce: 2.856  loss_mask: 1.808  loss_dice: 4.322  loss_ce_0: 2.799  loss_mask_0: 1.472  loss_dice_0: 4.103  loss_ce_1: 2.762  loss_mask_1: 1.468  loss_dice_1: 4.182  loss_ce_2: 2.463  loss_mask_2: 1.427  loss_dice_2: 4.15  loss_ce_3: 2.295  loss_mask_3: 1.634  loss_dice_3: 3.983  loss_ce_4: 2.394  loss_mask_4: 1.591  loss_dice_4: 4.239  loss_ce_5: 2.365  loss_mask_5: 1.503  loss_dice_5: 4.281  loss_ce_6: 2.434  loss_mask_6: 1.645  loss_dice_6: 4.187  loss_ce_7: 2.463  loss_mask_7: 1.905  loss_dice_7: 4.203  loss_ce_8: 2.985  loss_mask_8: 1.48  loss_dice_8: 4.093  loss_mars: 0.3275    time: 5.6774  last_time: 5.3786  data_time: 0.0023  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 05:58:23] d2.utils.events INFO:  eta: 16:56:46  iter: 4759  total_loss: 86.2  loss_ce: 3.721  loss_mask: 1.887  loss_dice: 4.201  loss_ce_0: 3.246  loss_mask_0: 1.097  loss_dice_0: 3.996  loss_ce_1: 3.078  loss_mask_1: 1.284  loss_dice_1: 3.857  loss_ce_2: 3.037  loss_mask_2: 1.296  loss_dice_2: 3.867  loss_ce_3: 2.86  loss_mask_3: 1.277  loss_dice_3: 3.862  loss_ce_4: 3.041  loss_mask_4: 1.295  loss_dice_4: 3.86  loss_ce_5: 2.958  loss_mask_5: 1.227  loss_dice_5: 3.871  loss_ce_6: 2.971  loss_mask_6: 1.194  loss_dice_6: 4.147  loss_ce_7: 2.943  loss_mask_7: 1.237  loss_dice_7: 4.014  loss_ce_8: 3.522  loss_mask_8: 1.124  loss_dice_8: 3.876  loss_mars: 0.558    time: 5.6776  last_time: 5.1865  data_time: 0.0024  last_data_time: 0.0043   lr: 0.0001  max_mem: 0M
[10/19 06:00:11] d2.utils.events INFO:  eta: 16:51:06  iter: 4779  total_loss: 85.27  loss_ce: 3.105  loss_mask: 1.424  loss_dice: 3.898  loss_ce_0: 3.603  loss_mask_0: 1.273  loss_dice_0: 3.924  loss_ce_1: 3.183  loss_mask_1: 1.265  loss_dice_1: 3.991  loss_ce_2: 3.298  loss_mask_2: 1.355  loss_dice_2: 4.006  loss_ce_3: 3.371  loss_mask_3: 1.146  loss_dice_3: 3.942  loss_ce_4: 3.107  loss_mask_4: 1.291  loss_dice_4: 3.991  loss_ce_5: 3.305  loss_mask_5: 1.22  loss_dice_5: 4.122  loss_ce_6: 3.127  loss_mask_6: 1.209  loss_dice_6: 3.976  loss_ce_7: 3.187  loss_mask_7: 1.333  loss_dice_7: 3.958  loss_ce_8: 3.78  loss_mask_8: 1.135  loss_dice_8: 4.055  loss_mars: 0.5203    time: 5.6759  last_time: 3.9395  data_time: 0.0023  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 06:02:12] d2.utils.events INFO:  eta: 16:49:30  iter: 4799  total_loss: 88.13  loss_ce: 2.79  loss_mask: 0.9181  loss_dice: 4.445  loss_ce_0: 3.092  loss_mask_0: 0.9966  loss_dice_0: 4.039  loss_ce_1: 2.807  loss_mask_1: 0.9961  loss_dice_1: 4.079  loss_ce_2: 2.677  loss_mask_2: 0.9309  loss_dice_2: 4.15  loss_ce_3: 2.75  loss_mask_3: 1.065  loss_dice_3: 4.28  loss_ce_4: 2.701  loss_mask_4: 1.802  loss_dice_4: 4.569  loss_ce_5: 2.801  loss_mask_5: 1.186  loss_dice_5: 4.401  loss_ce_6: 2.757  loss_mask_6: 1.154  loss_dice_6: 4.234  loss_ce_7: 2.956  loss_mask_7: 1.002  loss_dice_7: 4.178  loss_ce_8: 2.771  loss_mask_8: 1.144  loss_dice_8: 4.164  loss_mars: 0.396    time: 5.6784  last_time: 5.3977  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 06:04:06] d2.utils.events INFO:  eta: 16:54:53  iter: 4819  total_loss: 90.57  loss_ce: 3.433  loss_mask: 1.003  loss_dice: 4.411  loss_ce_0: 3.872  loss_mask_0: 0.9352  loss_dice_0: 4.333  loss_ce_1: 3.426  loss_mask_1: 0.9968  loss_dice_1: 4.259  loss_ce_2: 3.545  loss_mask_2: 1.042  loss_dice_2: 4.181  loss_ce_3: 3.386  loss_mask_3: 0.9781  loss_dice_3: 4.336  loss_ce_4: 3.479  loss_mask_4: 1.513  loss_dice_4: 4.658  loss_ce_5: 3.357  loss_mask_5: 1.09  loss_dice_5: 4.277  loss_ce_6: 3.483  loss_mask_6: 1.168  loss_dice_6: 4.502  loss_ce_7: 3.538  loss_mask_7: 1.023  loss_dice_7: 4.475  loss_ce_8: 3.735  loss_mask_8: 0.9686  loss_dice_8: 4.487  loss_mars: 0.4319    time: 5.6786  last_time: 3.8334  data_time: 0.0027  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 06:06:00] d2.utils.events INFO:  eta: 16:53:53  iter: 4839  total_loss: 88.99  loss_ce: 3.265  loss_mask: 1.183  loss_dice: 4.148  loss_ce_0: 2.964  loss_mask_0: 1.191  loss_dice_0: 4.199  loss_ce_1: 2.959  loss_mask_1: 1.096  loss_dice_1: 4.123  loss_ce_2: 3.025  loss_mask_2: 1.225  loss_dice_2: 4.311  loss_ce_3: 3.044  loss_mask_3: 1.026  loss_dice_3: 4.199  loss_ce_4: 2.874  loss_mask_4: 1.16  loss_dice_4: 4.684  loss_ce_5: 3.041  loss_mask_5: 1.111  loss_dice_5: 4.086  loss_ce_6: 3.235  loss_mask_6: 1.046  loss_dice_6: 4.055  loss_ce_7: 3.143  loss_mask_7: 1.113  loss_dice_7: 4.395  loss_ce_8: 3.159  loss_mask_8: 1.077  loss_dice_8: 4.239  loss_mars: 0.3944    time: 5.6786  last_time: 5.2951  data_time: 0.0021  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 06:07:52] d2.utils.events INFO:  eta: 16:53:04  iter: 4859  total_loss: 85.48  loss_ce: 2.537  loss_mask: 1.307  loss_dice: 4.226  loss_ce_0: 3.035  loss_mask_0: 1.359  loss_dice_0: 4.25  loss_ce_1: 2.643  loss_mask_1: 1.328  loss_dice_1: 4.358  loss_ce_2: 2.57  loss_mask_2: 1.306  loss_dice_2: 4.299  loss_ce_3: 2.529  loss_mask_3: 1.345  loss_dice_3: 4.298  loss_ce_4: 2.729  loss_mask_4: 1.509  loss_dice_4: 4.493  loss_ce_5: 2.594  loss_mask_5: 1.253  loss_dice_5: 4.255  loss_ce_6: 2.641  loss_mask_6: 1.242  loss_dice_6: 4.223  loss_ce_7: 2.551  loss_mask_7: 1.332  loss_dice_7: 4.256  loss_ce_8: 2.402  loss_mask_8: 1.37  loss_dice_8: 4.193  loss_mars: 0.6234    time: 5.6781  last_time: 5.5115  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 06:09:42] d2.utils.events INFO:  eta: 16:49:36  iter: 4879  total_loss: 89.95  loss_ce: 3.344  loss_mask: 1.565  loss_dice: 4.312  loss_ce_0: 3.423  loss_mask_0: 1.599  loss_dice_0: 3.973  loss_ce_1: 3.088  loss_mask_1: 1.666  loss_dice_1: 4.18  loss_ce_2: 3.133  loss_mask_2: 1.518  loss_dice_2: 4.122  loss_ce_3: 3.253  loss_mask_3: 1.581  loss_dice_3: 4.167  loss_ce_4: 3.096  loss_mask_4: 1.571  loss_dice_4: 4.174  loss_ce_5: 3.24  loss_mask_5: 1.732  loss_dice_5: 4.041  loss_ce_6: 3.316  loss_mask_6: 1.574  loss_dice_6: 4.157  loss_ce_7: 3.07  loss_mask_7: 1.588  loss_dice_7: 4.165  loss_ce_8: 3.186  loss_mask_8: 1.538  loss_dice_8: 4.101  loss_mars: 0.3711    time: 5.6770  last_time: 5.2870  data_time: 0.0022  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 06:11:35] d2.utils.events INFO:  eta: 16:47:43  iter: 4899  total_loss: 95.88  loss_ce: 3.694  loss_mask: 1.003  loss_dice: 4.297  loss_ce_0: 4.04  loss_mask_0: 1.03  loss_dice_0: 4.348  loss_ce_1: 3.322  loss_mask_1: 1.202  loss_dice_1: 4.328  loss_ce_2: 3.686  loss_mask_2: 1.046  loss_dice_2: 4.299  loss_ce_3: 3.752  loss_mask_3: 1.032  loss_dice_3: 4.358  loss_ce_4: 3.967  loss_mask_4: 1.147  loss_dice_4: 4.397  loss_ce_5: 3.866  loss_mask_5: 1.175  loss_dice_5: 4.32  loss_ce_6: 3.763  loss_mask_6: 1.019  loss_dice_6: 4.323  loss_ce_7: 3.711  loss_mask_7: 1.083  loss_dice_7: 4.292  loss_ce_8: 3.577  loss_mask_8: 0.9895  loss_dice_8: 4.403  loss_mars: 0.5401    time: 5.6769  last_time: 6.2068  data_time: 0.0024  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 06:13:29] d2.utils.events INFO:  eta: 16:53:55  iter: 4919  total_loss: 97.88  loss_ce: 3.842  loss_mask: 1.156  loss_dice: 4.255  loss_ce_0: 3.481  loss_mask_0: 1.046  loss_dice_0: 4.234  loss_ce_1: 3.273  loss_mask_1: 1.13  loss_dice_1: 4.291  loss_ce_2: 3.337  loss_mask_2: 1.067  loss_dice_2: 4.133  loss_ce_3: 3.631  loss_mask_3: 1.144  loss_dice_3: 4.058  loss_ce_4: 3.678  loss_mask_4: 1.36  loss_dice_4: 4.101  loss_ce_5: 3.721  loss_mask_5: 1.591  loss_dice_5: 4.462  loss_ce_6: 3.75  loss_mask_6: 1.25  loss_dice_6: 4.022  loss_ce_7: 3.575  loss_mask_7: 1.325  loss_dice_7: 4.323  loss_ce_8: 3.875  loss_mask_8: 1.236  loss_dice_8: 4.295  loss_mars: 0.5904    time: 5.6769  last_time: 5.5024  data_time: 0.0024  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 06:15:25] d2.utils.events INFO:  eta: 16:52:02  iter: 4939  total_loss: 93.59  loss_ce: 3.724  loss_mask: 1.269  loss_dice: 4.059  loss_ce_0: 3.721  loss_mask_0: 1.229  loss_dice_0: 4.154  loss_ce_1: 3.485  loss_mask_1: 1.128  loss_dice_1: 4.095  loss_ce_2: 3.66  loss_mask_2: 1.128  loss_dice_2: 4.059  loss_ce_3: 3.712  loss_mask_3: 1.267  loss_dice_3: 4.076  loss_ce_4: 3.737  loss_mask_4: 1.113  loss_dice_4: 4.082  loss_ce_5: 3.753  loss_mask_5: 1.316  loss_dice_5: 4.137  loss_ce_6: 3.999  loss_mask_6: 1.197  loss_dice_6: 4.055  loss_ce_7: 3.685  loss_mask_7: 1.159  loss_dice_7: 4.158  loss_ce_8: 3.877  loss_mask_8: 1.263  loss_dice_8: 4.109  loss_mars: 0.7487    time: 5.6776  last_time: 5.4470  data_time: 0.0024  last_data_time: 0.0038   lr: 0.0001  max_mem: 0M
[10/19 06:17:18] d2.utils.events INFO:  eta: 16:50:08  iter: 4959  total_loss: 90.76  loss_ce: 2.65  loss_mask: 0.8209  loss_dice: 4.132  loss_ce_0: 3.267  loss_mask_0: 0.7506  loss_dice_0: 4.197  loss_ce_1: 2.855  loss_mask_1: 0.8564  loss_dice_1: 4.208  loss_ce_2: 2.806  loss_mask_2: 0.8303  loss_dice_2: 4.225  loss_ce_3: 2.644  loss_mask_3: 0.7653  loss_dice_3: 4.162  loss_ce_4: 2.665  loss_mask_4: 0.9513  loss_dice_4: 4.242  loss_ce_5: 2.315  loss_mask_5: 0.9872  loss_dice_5: 4.452  loss_ce_6: 2.583  loss_mask_6: 1.199  loss_dice_6: 4.353  loss_ce_7: 2.831  loss_mask_7: 0.8802  loss_dice_7: 4.251  loss_ce_8: 2.775  loss_mask_8: 1.007  loss_dice_8: 4.254  loss_mars: 0.7026    time: 5.6774  last_time: 5.2111  data_time: 0.0024  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 06:19:08] d2.utils.events INFO:  eta: 16:50:22  iter: 4979  total_loss: 86.05  loss_ce: 3.196  loss_mask: 1.175  loss_dice: 3.659  loss_ce_0: 2.84  loss_mask_0: 1.168  loss_dice_0: 3.829  loss_ce_1: 2.936  loss_mask_1: 1.275  loss_dice_1: 3.8  loss_ce_2: 2.776  loss_mask_2: 1.315  loss_dice_2: 3.892  loss_ce_3: 2.738  loss_mask_3: 1.37  loss_dice_3: 4  loss_ce_4: 2.848  loss_mask_4: 1.24  loss_dice_4: 3.725  loss_ce_5: 2.834  loss_mask_5: 1.35  loss_dice_5: 3.654  loss_ce_6: 2.529  loss_mask_6: 1.242  loss_dice_6: 3.718  loss_ce_7: 2.623  loss_mask_7: 1.183  loss_dice_7: 3.953  loss_ce_8: 2.675  loss_mask_8: 1.194  loss_dice_8: 3.87  loss_mars: 0.7611    time: 5.6764  last_time: 3.9955  data_time: 0.0023  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 06:21:01] d2.utils.events INFO:  eta: 16:50:04  iter: 4999  total_loss: 92.79  loss_ce: 3.56  loss_mask: 0.8332  loss_dice: 4.316  loss_ce_0: 4.912  loss_mask_0: 0.8916  loss_dice_0: 4.279  loss_ce_1: 3.613  loss_mask_1: 0.853  loss_dice_1: 4.26  loss_ce_2: 3.769  loss_mask_2: 0.8149  loss_dice_2: 4.263  loss_ce_3: 3.782  loss_mask_3: 0.9403  loss_dice_3: 4.284  loss_ce_4: 3.565  loss_mask_4: 0.893  loss_dice_4: 4.254  loss_ce_5: 3.458  loss_mask_5: 0.9971  loss_dice_5: 4.29  loss_ce_6: 3.562  loss_mask_6: 1.013  loss_dice_6: 4.32  loss_ce_7: 3.482  loss_mask_7: 0.9747  loss_dice_7: 4.225  loss_ce_8: 3.606  loss_mask_8: 0.8784  loss_dice_8: 4.232  loss_mars: 0.7729    time: 5.6761  last_time: 6.2734  data_time: 0.0026  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 06:22:52] d2.utils.events INFO:  eta: 16:49:09  iter: 5019  total_loss: 84.67  loss_ce: 2.503  loss_mask: 1.043  loss_dice: 4.152  loss_ce_0: 2.687  loss_mask_0: 1.092  loss_dice_0: 4.329  loss_ce_1: 2.685  loss_mask_1: 1.039  loss_dice_1: 3.777  loss_ce_2: 2.648  loss_mask_2: 0.9988  loss_dice_2: 3.873  loss_ce_3: 2.547  loss_mask_3: 1.196  loss_dice_3: 3.963  loss_ce_4: 2.419  loss_mask_4: 1.057  loss_dice_4: 4.08  loss_ce_5: 2.5  loss_mask_5: 1.077  loss_dice_5: 4.355  loss_ce_6: 2.238  loss_mask_6: 1.228  loss_dice_6: 4.303  loss_ce_7: 2.2  loss_mask_7: 1.207  loss_dice_7: 4.105  loss_ce_8: 2.301  loss_mask_8: 1.047  loss_dice_8: 3.88  loss_mars: 0.6735    time: 5.6755  last_time: 5.3948  data_time: 0.0026  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 06:24:44] d2.utils.events INFO:  eta: 16:47:15  iter: 5039  total_loss: 85.74  loss_ce: 3.108  loss_mask: 0.7752  loss_dice: 4.249  loss_ce_0: 3.559  loss_mask_0: 0.8084  loss_dice_0: 4.254  loss_ce_1: 3.121  loss_mask_1: 0.8354  loss_dice_1: 4.266  loss_ce_2: 3.361  loss_mask_2: 0.7641  loss_dice_2: 4.372  loss_ce_3: 3.108  loss_mask_3: 0.9008  loss_dice_3: 4.242  loss_ce_4: 3.045  loss_mask_4: 0.961  loss_dice_4: 4.44  loss_ce_5: 2.973  loss_mask_5: 0.8985  loss_dice_5: 4.407  loss_ce_6: 2.881  loss_mask_6: 0.9125  loss_dice_6: 4.39  loss_ce_7: 2.855  loss_mask_7: 0.8194  loss_dice_7: 4.331  loss_ce_8: 3.038  loss_mask_8: 0.8471  loss_dice_8: 4.286  loss_mars: 0.5982    time: 5.6750  last_time: 5.9435  data_time: 0.0024  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 06:26:40] d2.utils.events INFO:  eta: 16:47:32  iter: 5059  total_loss: 90.79  loss_ce: 3.7  loss_mask: 1.203  loss_dice: 4.128  loss_ce_0: 4.144  loss_mask_0: 1.159  loss_dice_0: 4.218  loss_ce_1: 3.409  loss_mask_1: 1.001  loss_dice_1: 4.134  loss_ce_2: 3.382  loss_mask_2: 1.086  loss_dice_2: 4.182  loss_ce_3: 3.425  loss_mask_3: 1.236  loss_dice_3: 4.137  loss_ce_4: 3.134  loss_mask_4: 1.098  loss_dice_4: 4.239  loss_ce_5: 3.285  loss_mask_5: 1.275  loss_dice_5: 4.118  loss_ce_6: 3.293  loss_mask_6: 1.534  loss_dice_6: 4.187  loss_ce_7: 3.238  loss_mask_7: 1.165  loss_dice_7: 4.117  loss_ce_8: 3.289  loss_mask_8: 1.291  loss_dice_8: 4.112  loss_mars: 0.5337    time: 5.6759  last_time: 5.8536  data_time: 0.0024  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 06:28:32] d2.utils.events INFO:  eta: 16:46:18  iter: 5079  total_loss: 90.88  loss_ce: 4.089  loss_mask: 0.6499  loss_dice: 4.584  loss_ce_0: 4.873  loss_mask_0: 0.7371  loss_dice_0: 4.521  loss_ce_1: 3.77  loss_mask_1: 0.6808  loss_dice_1: 4.609  loss_ce_2: 3.633  loss_mask_2: 0.6576  loss_dice_2: 4.512  loss_ce_3: 3.696  loss_mask_3: 0.7524  loss_dice_3: 4.474  loss_ce_4: 3.595  loss_mask_4: 0.7038  loss_dice_4: 4.55  loss_ce_5: 3.746  loss_mask_5: 0.6737  loss_dice_5: 4.501  loss_ce_6: 3.498  loss_mask_6: 0.9762  loss_dice_6: 4.365  loss_ce_7: 3.35  loss_mask_7: 0.8565  loss_dice_7: 4.549  loss_ce_8: 3.651  loss_mask_8: 0.742  loss_dice_8: 4.546  loss_mars: 0.5773    time: 5.6754  last_time: 5.0762  data_time: 0.0026  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 06:30:31] d2.utils.events INFO:  eta: 16:47:01  iter: 5099  total_loss: 92  loss_ce: 3.079  loss_mask: 1.407  loss_dice: 3.969  loss_ce_0: 2.968  loss_mask_0: 1.498  loss_dice_0: 4.064  loss_ce_1: 2.944  loss_mask_1: 1.392  loss_dice_1: 4.048  loss_ce_2: 2.879  loss_mask_2: 1.344  loss_dice_2: 3.983  loss_ce_3: 2.77  loss_mask_3: 1.396  loss_dice_3: 4.188  loss_ce_4: 2.997  loss_mask_4: 1.417  loss_dice_4: 4.336  loss_ce_5: 3.001  loss_mask_5: 1.535  loss_dice_5: 4.126  loss_ce_6: 2.438  loss_mask_6: 1.651  loss_dice_6: 4.326  loss_ce_7: 2.547  loss_mask_7: 1.489  loss_dice_7: 4.307  loss_ce_8: 2.575  loss_mask_8: 1.571  loss_dice_8: 4.22  loss_mars: 0.4894    time: 5.6768  last_time: 5.8152  data_time: 0.0026  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 06:32:21] d2.utils.events INFO:  eta: 16:44:42  iter: 5119  total_loss: 88.59  loss_ce: 2.918  loss_mask: 1.213  loss_dice: 4.14  loss_ce_0: 3.613  loss_mask_0: 1.11  loss_dice_0: 4.076  loss_ce_1: 2.682  loss_mask_1: 1.234  loss_dice_1: 4.21  loss_ce_2: 2.773  loss_mask_2: 1.149  loss_dice_2: 4.21  loss_ce_3: 3.171  loss_mask_3: 1.149  loss_dice_3: 4.253  loss_ce_4: 3.041  loss_mask_4: 1.316  loss_dice_4: 4.315  loss_ce_5: 3.258  loss_mask_5: 1.228  loss_dice_5: 4.258  loss_ce_6: 3.338  loss_mask_6: 1.122  loss_dice_6: 4.19  loss_ce_7: 3.045  loss_mask_7: 1.07  loss_dice_7: 4.294  loss_ce_8: 2.913  loss_mask_8: 1.313  loss_dice_8: 4.383  loss_mars: 0.3252    time: 5.6760  last_time: 6.6429  data_time: 0.0025  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 06:34:15] d2.utils.events INFO:  eta: 16:44:11  iter: 5139  total_loss: 84.12  loss_ce: 2.67  loss_mask: 1.644  loss_dice: 4.243  loss_ce_0: 2.153  loss_mask_0: 1.593  loss_dice_0: 3.776  loss_ce_1: 2.304  loss_mask_1: 1.495  loss_dice_1: 4.091  loss_ce_2: 2.363  loss_mask_2: 1.628  loss_dice_2: 3.875  loss_ce_3: 2.545  loss_mask_3: 1.646  loss_dice_3: 4.146  loss_ce_4: 2.572  loss_mask_4: 1.554  loss_dice_4: 3.806  loss_ce_5: 2.578  loss_mask_5: 1.613  loss_dice_5: 3.848  loss_ce_6: 2.202  loss_mask_6: 1.985  loss_dice_6: 4.298  loss_ce_7: 2.7  loss_mask_7: 1.952  loss_dice_7: 3.983  loss_ce_8: 2.71  loss_mask_8: 1.559  loss_dice_8: 3.936  loss_mars: 0.4524    time: 5.6760  last_time: 5.9718  data_time: 0.0021  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 06:36:06] d2.utils.events INFO:  eta: 16:42:59  iter: 5159  total_loss: 99.55  loss_ce: 3.727  loss_mask: 1.678  loss_dice: 4.003  loss_ce_0: 3.039  loss_mask_0: 1.589  loss_dice_0: 4.087  loss_ce_1: 3.007  loss_mask_1: 1.52  loss_dice_1: 4.313  loss_ce_2: 3.074  loss_mask_2: 1.608  loss_dice_2: 4.008  loss_ce_3: 3.161  loss_mask_3: 1.678  loss_dice_3: 4.271  loss_ce_4: 3.328  loss_mask_4: 1.928  loss_dice_4: 4.168  loss_ce_5: 3.266  loss_mask_5: 2.283  loss_dice_5: 4.226  loss_ce_6: 3.346  loss_mask_6: 2.317  loss_dice_6: 4.286  loss_ce_7: 3.523  loss_mask_7: 2.201  loss_dice_7: 4.347  loss_ce_8: 3.582  loss_mask_8: 1.816  loss_dice_8: 4.055  loss_mars: 0.6437    time: 5.6755  last_time: 5.1916  data_time: 0.0025  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 06:38:02] d2.utils.events INFO:  eta: 16:41:04  iter: 5179  total_loss: 90.85  loss_ce: 3.592  loss_mask: 1.036  loss_dice: 4.213  loss_ce_0: 4.257  loss_mask_0: 1.079  loss_dice_0: 4.219  loss_ce_1: 3.429  loss_mask_1: 0.9395  loss_dice_1: 4.226  loss_ce_2: 3.343  loss_mask_2: 1.113  loss_dice_2: 4.191  loss_ce_3: 3.244  loss_mask_3: 1.193  loss_dice_3: 4.223  loss_ce_4: 3.421  loss_mask_4: 1.193  loss_dice_4: 4.147  loss_ce_5: 3.528  loss_mask_5: 1.034  loss_dice_5: 4.26  loss_ce_6: 3.822  loss_mask_6: 1.085  loss_dice_6: 4.273  loss_ce_7: 3.739  loss_mask_7: 1.039  loss_dice_7: 4.181  loss_ce_8: 3.609  loss_mask_8: 1.287  loss_dice_8: 4.316  loss_mars: 0.4875    time: 5.6761  last_time: 5.8252  data_time: 0.0027  last_data_time: 0.0049   lr: 0.0001  max_mem: 0M
[10/19 06:39:55] d2.utils.events INFO:  eta: 16:37:26  iter: 5199  total_loss: 89.15  loss_ce: 3.267  loss_mask: 1.173  loss_dice: 4.507  loss_ce_0: 3.57  loss_mask_0: 1.206  loss_dice_0: 4.221  loss_ce_1: 2.824  loss_mask_1: 1.402  loss_dice_1: 4.255  loss_ce_2: 2.939  loss_mask_2: 1.085  loss_dice_2: 4.157  loss_ce_3: 2.995  loss_mask_3: 1.119  loss_dice_3: 3.987  loss_ce_4: 3.25  loss_mask_4: 1.109  loss_dice_4: 4.067  loss_ce_5: 3.436  loss_mask_5: 1.191  loss_dice_5: 3.974  loss_ce_6: 3.36  loss_mask_6: 1.135  loss_dice_6: 4.468  loss_ce_7: 3.396  loss_mask_7: 1.092  loss_dice_7: 4.469  loss_ce_8: 3.441  loss_mask_8: 1.268  loss_dice_8: 4.387  loss_mars: 0.6301    time: 5.6758  last_time: 5.2447  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 06:41:48] d2.utils.events INFO:  eta: 16:33:15  iter: 5219  total_loss: 94.78  loss_ce: 3.674  loss_mask: 1.275  loss_dice: 4.462  loss_ce_0: 4.004  loss_mask_0: 1.077  loss_dice_0: 4.386  loss_ce_1: 3.167  loss_mask_1: 1.011  loss_dice_1: 4.261  loss_ce_2: 3.113  loss_mask_2: 1.413  loss_dice_2: 4.299  loss_ce_3: 3.678  loss_mask_3: 1.205  loss_dice_3: 4.447  loss_ce_4: 4.053  loss_mask_4: 1.332  loss_dice_4: 4.448  loss_ce_5: 3.865  loss_mask_5: 1.18  loss_dice_5: 4.301  loss_ce_6: 3.938  loss_mask_6: 1.251  loss_dice_6: 4.536  loss_ce_7: 3.864  loss_mask_7: 1.268  loss_dice_7: 4.58  loss_ce_8: 3.711  loss_mask_8: 1.427  loss_dice_8: 4.501  loss_mars: 0.6171    time: 5.6757  last_time: 6.0816  data_time: 0.0023  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 06:43:42] d2.utils.events INFO:  eta: 16:27:57  iter: 5239  total_loss: 102.6  loss_ce: 4.168  loss_mask: 1.352  loss_dice: 4.477  loss_ce_0: 4.774  loss_mask_0: 1.088  loss_dice_0: 4.244  loss_ce_1: 3.849  loss_mask_1: 1.212  loss_dice_1: 4.093  loss_ce_2: 3.781  loss_mask_2: 1.268  loss_dice_2: 4.235  loss_ce_3: 4.426  loss_mask_3: 1.257  loss_dice_3: 4.365  loss_ce_4: 4.408  loss_mask_4: 1.223  loss_dice_4: 4.259  loss_ce_5: 4.382  loss_mask_5: 1.298  loss_dice_5: 3.994  loss_ce_6: 3.934  loss_mask_6: 1.344  loss_dice_6: 4.549  loss_ce_7: 4.003  loss_mask_7: 1.229  loss_dice_7: 4.614  loss_ce_8: 3.799  loss_mask_8: 1.253  loss_dice_8: 4.553  loss_mars: 0.398    time: 5.6759  last_time: 6.3046  data_time: 0.0024  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 06:45:31] d2.utils.events INFO:  eta: 16:24:32  iter: 5259  total_loss: 92.48  loss_ce: 3.397  loss_mask: 1.314  loss_dice: 4.32  loss_ce_0: 3.598  loss_mask_0: 1.361  loss_dice_0: 4.353  loss_ce_1: 3.145  loss_mask_1: 1.268  loss_dice_1: 4.356  loss_ce_2: 3.234  loss_mask_2: 1.397  loss_dice_2: 4.365  loss_ce_3: 3.144  loss_mask_3: 1.415  loss_dice_3: 4.334  loss_ce_4: 3.612  loss_mask_4: 1.372  loss_dice_4: 4.355  loss_ce_5: 3.442  loss_mask_5: 1.503  loss_dice_5: 4.422  loss_ce_6: 3.655  loss_mask_6: 1.482  loss_dice_6: 4.436  loss_ce_7: 3.559  loss_mask_7: 1.333  loss_dice_7: 4.409  loss_ce_8: 3.254  loss_mask_8: 1.39  loss_dice_8: 4.438  loss_mars: 0.5751    time: 5.6747  last_time: 4.0865  data_time: 0.0024  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 06:47:24] d2.utils.events INFO:  eta: 16:21:47  iter: 5279  total_loss: 91.34  loss_ce: 3.521  loss_mask: 1.109  loss_dice: 4.465  loss_ce_0: 3.266  loss_mask_0: 1.084  loss_dice_0: 4.465  loss_ce_1: 3.227  loss_mask_1: 1.086  loss_dice_1: 4.446  loss_ce_2: 3.244  loss_mask_2: 1.094  loss_dice_2: 4.452  loss_ce_3: 3.275  loss_mask_3: 1.042  loss_dice_3: 4.443  loss_ce_4: 3.521  loss_mask_4: 1.102  loss_dice_4: 4.406  loss_ce_5: 3.506  loss_mask_5: 1.101  loss_dice_5: 4.509  loss_ce_6: 3.474  loss_mask_6: 1.247  loss_dice_6: 4.487  loss_ce_7: 3.614  loss_mask_7: 1.188  loss_dice_7: 4.484  loss_ce_8: 3.481  loss_mask_8: 1.061  loss_dice_8: 4.458  loss_mars: 0.4821    time: 5.6745  last_time: 5.9771  data_time: 0.0024  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 06:49:12] d2.utils.events INFO:  eta: 16:21:33  iter: 5299  total_loss: 87.67  loss_ce: 3.147  loss_mask: 1.102  loss_dice: 4.338  loss_ce_0: 3.308  loss_mask_0: 1.086  loss_dice_0: 3.958  loss_ce_1: 3.071  loss_mask_1: 1.023  loss_dice_1: 4.011  loss_ce_2: 3.084  loss_mask_2: 0.9864  loss_dice_2: 3.985  loss_ce_3: 3.226  loss_mask_3: 1.022  loss_dice_3: 4.207  loss_ce_4: 3.179  loss_mask_4: 1.099  loss_dice_4: 4.121  loss_ce_5: 3.212  loss_mask_5: 1.081  loss_dice_5: 4.19  loss_ce_6: 3.193  loss_mask_6: 1.116  loss_dice_6: 4.121  loss_ce_7: 3.032  loss_mask_7: 1.138  loss_dice_7: 4.247  loss_ce_8: 2.966  loss_mask_8: 1.082  loss_dice_8: 4.353  loss_mars: 0.4553    time: 5.6729  last_time: 5.7580  data_time: 0.0023  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 06:51:13] d2.utils.events INFO:  eta: 16:24:33  iter: 5319  total_loss: 92.89  loss_ce: 3.698  loss_mask: 0.8608  loss_dice: 4.361  loss_ce_0: 4.147  loss_mask_0: 0.8787  loss_dice_0: 4.132  loss_ce_1: 3.75  loss_mask_1: 1.001  loss_dice_1: 4.211  loss_ce_2: 3.711  loss_mask_2: 1.073  loss_dice_2: 4.394  loss_ce_3: 3.795  loss_mask_3: 0.9832  loss_dice_3: 4.091  loss_ce_4: 3.753  loss_mask_4: 0.978  loss_dice_4: 4.358  loss_ce_5: 3.613  loss_mask_5: 0.9237  loss_dice_5: 4.572  loss_ce_6: 3.622  loss_mask_6: 0.955  loss_dice_6: 4.43  loss_ce_7: 3.758  loss_mask_7: 0.926  loss_dice_7: 4.428  loss_ce_8: 3.851  loss_mask_8: 1.078  loss_dice_8: 4.543  loss_mars: 0.4274    time: 5.6751  last_time: 5.1775  data_time: 0.0028  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 06:53:06] d2.utils.events INFO:  eta: 16:23:51  iter: 5339  total_loss: 94.63  loss_ce: 3.455  loss_mask: 1.215  loss_dice: 4.434  loss_ce_0: 3.574  loss_mask_0: 1.163  loss_dice_0: 4.171  loss_ce_1: 3.325  loss_mask_1: 1.27  loss_dice_1: 4.149  loss_ce_2: 3.488  loss_mask_2: 1.175  loss_dice_2: 4.336  loss_ce_3: 3.531  loss_mask_3: 1.305  loss_dice_3: 4.163  loss_ce_4: 3.283  loss_mask_4: 1.194  loss_dice_4: 4.261  loss_ce_5: 3.112  loss_mask_5: 1.239  loss_dice_5: 4.437  loss_ce_6: 3.356  loss_mask_6: 1.25  loss_dice_6: 4.344  loss_ce_7: 3.331  loss_mask_7: 1.247  loss_dice_7: 4.423  loss_ce_8: 3.487  loss_mask_8: 1.258  loss_dice_8: 4.314  loss_mars: 0.7481    time: 5.6748  last_time: 5.2498  data_time: 0.0024  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 06:54:56] d2.utils.events INFO:  eta: 16:20:34  iter: 5359  total_loss: 95.02  loss_ce: 4.331  loss_mask: 1.341  loss_dice: 4.386  loss_ce_0: 3.502  loss_mask_0: 1.264  loss_dice_0: 4.285  loss_ce_1: 3.591  loss_mask_1: 1.332  loss_dice_1: 4.339  loss_ce_2: 3.7  loss_mask_2: 1.387  loss_dice_2: 4.373  loss_ce_3: 3.81  loss_mask_3: 1.26  loss_dice_3: 4.274  loss_ce_4: 3.94  loss_mask_4: 1.356  loss_dice_4: 4.335  loss_ce_5: 4.264  loss_mask_5: 1.343  loss_dice_5: 4.302  loss_ce_6: 3.943  loss_mask_6: 1.329  loss_dice_6: 4.304  loss_ce_7: 4.071  loss_mask_7: 1.295  loss_dice_7: 4.282  loss_ce_8: 4.331  loss_mask_8: 1.307  loss_dice_8: 4.251  loss_mars: 0.6947    time: 5.6738  last_time: 5.9138  data_time: 0.0024  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 06:56:43] d2.utils.events INFO:  eta: 16:17:59  iter: 5379  total_loss: 96.26  loss_ce: 3.802  loss_mask: 1.318  loss_dice: 4.41  loss_ce_0: 3.346  loss_mask_0: 1.304  loss_dice_0: 4.303  loss_ce_1: 3.42  loss_mask_1: 1.301  loss_dice_1: 4.445  loss_ce_2: 3.476  loss_mask_2: 1.26  loss_dice_2: 4.214  loss_ce_3: 3.764  loss_mask_3: 1.278  loss_dice_3: 4.348  loss_ce_4: 3.585  loss_mask_4: 1.309  loss_dice_4: 4.382  loss_ce_5: 3.792  loss_mask_5: 1.234  loss_dice_5: 4.245  loss_ce_6: 3.885  loss_mask_6: 1.239  loss_dice_6: 4.324  loss_ce_7: 3.823  loss_mask_7: 1.224  loss_dice_7: 4.15  loss_ce_8: 3.73  loss_mask_8: 1.447  loss_dice_8: 4.419  loss_mars: 0.4863    time: 5.6723  last_time: 6.3839  data_time: 0.0028  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 06:58:38] d2.utils.events INFO:  eta: 16:17:24  iter: 5399  total_loss: 97.2  loss_ce: 3.686  loss_mask: 1.898  loss_dice: 3.958  loss_ce_0: 3.458  loss_mask_0: 2.042  loss_dice_0: 4.074  loss_ce_1: 3.063  loss_mask_1: 1.919  loss_dice_1: 4.025  loss_ce_2: 3.117  loss_mask_2: 2.055  loss_dice_2: 3.909  loss_ce_3: 3.301  loss_mask_3: 2.058  loss_dice_3: 3.806  loss_ce_4: 3.017  loss_mask_4: 1.959  loss_dice_4: 3.755  loss_ce_5: 3.371  loss_mask_5: 1.839  loss_dice_5: 3.821  loss_ce_6: 3.103  loss_mask_6: 1.779  loss_dice_6: 3.969  loss_ce_7: 3.538  loss_mask_7: 2.033  loss_dice_7: 3.911  loss_ce_8: 3.763  loss_mask_8: 2.066  loss_dice_8: 3.984  loss_mars: 0.7532    time: 5.6726  last_time: 6.2272  data_time: 0.0027  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/19 07:00:33] d2.utils.events INFO:  eta: 16:13:30  iter: 5419  total_loss: 92.96  loss_ce: 2.956  loss_mask: 1.215  loss_dice: 4.784  loss_ce_0: 2.655  loss_mask_0: 1.165  loss_dice_0: 4.311  loss_ce_1: 2.802  loss_mask_1: 1.087  loss_dice_1: 4.31  loss_ce_2: 3.039  loss_mask_2: 1.058  loss_dice_2: 4.29  loss_ce_3: 2.667  loss_mask_3: 1.075  loss_dice_3: 4.218  loss_ce_4: 2.73  loss_mask_4: 1.116  loss_dice_4: 4.345  loss_ce_5: 3.012  loss_mask_5: 1.119  loss_dice_5: 4.354  loss_ce_6: 3.011  loss_mask_6: 1.014  loss_dice_6: 4.322  loss_ce_7: 2.943  loss_mask_7: 1.039  loss_dice_7: 4.586  loss_ce_8: 2.892  loss_mask_8: 1.439  loss_dice_8: 4.535  loss_mars: 0.3838    time: 5.6730  last_time: 7.8124  data_time: 0.0023  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 07:02:20] d2.utils.events INFO:  eta: 16:11:36  iter: 5439  total_loss: 90.84  loss_ce: 3.429  loss_mask: 1.329  loss_dice: 4.307  loss_ce_0: 3.487  loss_mask_0: 1.214  loss_dice_0: 3.926  loss_ce_1: 3.415  loss_mask_1: 1.34  loss_dice_1: 4.081  loss_ce_2: 3.562  loss_mask_2: 1.427  loss_dice_2: 4.019  loss_ce_3: 3.306  loss_mask_3: 1.291  loss_dice_3: 3.931  loss_ce_4: 3.073  loss_mask_4: 1.264  loss_dice_4: 3.934  loss_ce_5: 3.434  loss_mask_5: 1.265  loss_dice_5: 4.086  loss_ce_6: 3.461  loss_mask_6: 1.585  loss_dice_6: 3.981  loss_ce_7: 3.388  loss_mask_7: 1.391  loss_dice_7: 4.337  loss_ce_8: 3.293  loss_mask_8: 1.521  loss_dice_8: 4.167  loss_mars: 0.6046    time: 5.6714  last_time: 5.0976  data_time: 0.0022  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 07:04:12] d2.utils.events INFO:  eta: 16:07:01  iter: 5459  total_loss: 85.79  loss_ce: 3.193  loss_mask: 1.417  loss_dice: 3.998  loss_ce_0: 3.426  loss_mask_0: 1.545  loss_dice_0: 3.89  loss_ce_1: 3.149  loss_mask_1: 1.521  loss_dice_1: 3.821  loss_ce_2: 3.081  loss_mask_2: 1.349  loss_dice_2: 4.047  loss_ce_3: 3.197  loss_mask_3: 1.564  loss_dice_3: 3.918  loss_ce_4: 3.207  loss_mask_4: 1.59  loss_dice_4: 3.932  loss_ce_5: 3.408  loss_mask_5: 1.616  loss_dice_5: 3.955  loss_ce_6: 3.145  loss_mask_6: 1.506  loss_dice_6: 4.02  loss_ce_7: 3.176  loss_mask_7: 1.513  loss_dice_7: 4.204  loss_ce_8: 2.842  loss_mask_8: 1.527  loss_dice_8: 3.94  loss_mars: 0.8173    time: 5.6709  last_time: 5.4436  data_time: 0.0026  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 07:06:05] d2.utils.events INFO:  eta: 16:05:07  iter: 5479  total_loss: 97.95  loss_ce: 3.924  loss_mask: 1.248  loss_dice: 4.49  loss_ce_0: 3.851  loss_mask_0: 1.091  loss_dice_0: 4.346  loss_ce_1: 3.896  loss_mask_1: 0.9757  loss_dice_1: 4.213  loss_ce_2: 3.812  loss_mask_2: 0.9435  loss_dice_2: 4.233  loss_ce_3: 3.691  loss_mask_3: 1.095  loss_dice_3: 4.041  loss_ce_4: 3.584  loss_mask_4: 0.9513  loss_dice_4: 4.17  loss_ce_5: 3.957  loss_mask_5: 1.119  loss_dice_5: 4.233  loss_ce_6: 3.903  loss_mask_6: 1.498  loss_dice_6: 4.396  loss_ce_7: 4.09  loss_mask_7: 1.444  loss_dice_7: 4.427  loss_ce_8: 3.885  loss_mask_8: 1.546  loss_dice_8: 4.272  loss_mars: 0.3183    time: 5.6708  last_time: 5.3049  data_time: 0.0025  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 07:08:00] d2.utils.events INFO:  eta: 16:03:13  iter: 5499  total_loss: 81.98  loss_ce: 3.311  loss_mask: 1.29  loss_dice: 4.138  loss_ce_0: 2.782  loss_mask_0: 1.386  loss_dice_0: 4.252  loss_ce_1: 3.169  loss_mask_1: 1.333  loss_dice_1: 4.156  loss_ce_2: 3.091  loss_mask_2: 1.301  loss_dice_2: 4.181  loss_ce_3: 2.834  loss_mask_3: 1.288  loss_dice_3: 4.246  loss_ce_4: 2.721  loss_mask_4: 1.405  loss_dice_4: 4.265  loss_ce_5: 3.04  loss_mask_5: 1.442  loss_dice_5: 4.171  loss_ce_6: 3.169  loss_mask_6: 1.641  loss_dice_6: 4.371  loss_ce_7: 3.456  loss_mask_7: 1.544  loss_dice_7: 4.348  loss_ce_8: 2.976  loss_mask_8: 1.708  loss_dice_8: 4.421  loss_mars: 0.6109    time: 5.6712  last_time: 3.8953  data_time: 0.0021  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 07:09:56] d2.utils.events INFO:  eta: 16:02:48  iter: 5519  total_loss: 95.36  loss_ce: 3.497  loss_mask: 1.405  loss_dice: 4.045  loss_ce_0: 3.382  loss_mask_0: 1.376  loss_dice_0: 3.94  loss_ce_1: 3.362  loss_mask_1: 1.362  loss_dice_1: 4.002  loss_ce_2: 3.315  loss_mask_2: 1.337  loss_dice_2: 3.891  loss_ce_3: 3.124  loss_mask_3: 1.338  loss_dice_3: 3.93  loss_ce_4: 2.996  loss_mask_4: 1.359  loss_dice_4: 3.971  loss_ce_5: 3.718  loss_mask_5: 1.318  loss_dice_5: 3.918  loss_ce_6: 3.646  loss_mask_6: 1.328  loss_dice_6: 3.994  loss_ce_7: 3.59  loss_mask_7: 1.443  loss_dice_7: 3.926  loss_ce_8: 3.654  loss_mask_8: 1.332  loss_dice_8: 4.078  loss_mars: 0.7012    time: 5.6718  last_time: 5.9106  data_time: 0.0028  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 07:11:44] d2.utils.events INFO:  eta: 15:55:05  iter: 5539  total_loss: 85.45  loss_ce: 3.328  loss_mask: 1.116  loss_dice: 4.267  loss_ce_0: 3.597  loss_mask_0: 1.264  loss_dice_0: 4.165  loss_ce_1: 3.48  loss_mask_1: 1.031  loss_dice_1: 4.195  loss_ce_2: 3.121  loss_mask_2: 1.209  loss_dice_2: 4.18  loss_ce_3: 2.826  loss_mask_3: 1.083  loss_dice_3: 4.265  loss_ce_4: 2.7  loss_mask_4: 1.24  loss_dice_4: 4.352  loss_ce_5: 2.946  loss_mask_5: 1.004  loss_dice_5: 4.279  loss_ce_6: 3.413  loss_mask_6: 1.133  loss_dice_6: 4.102  loss_ce_7: 3.392  loss_mask_7: 1.283  loss_dice_7: 4.376  loss_ce_8: 3.21  loss_mask_8: 1.038  loss_dice_8: 4.269  loss_mars: 0.5007    time: 5.6705  last_time: 5.2681  data_time: 0.0024  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 07:13:39] d2.utils.events INFO:  eta: 15:53:12  iter: 5559  total_loss: 88.34  loss_ce: 2.462  loss_mask: 1.603  loss_dice: 4.187  loss_ce_0: 2.868  loss_mask_0: 1.483  loss_dice_0: 4.025  loss_ce_1: 2.875  loss_mask_1: 1.38  loss_dice_1: 4.157  loss_ce_2: 2.669  loss_mask_2: 1.354  loss_dice_2: 4.152  loss_ce_3: 2.688  loss_mask_3: 1.394  loss_dice_3: 4.191  loss_ce_4: 2.186  loss_mask_4: 1.6  loss_dice_4: 4.23  loss_ce_5: 2.544  loss_mask_5: 1.49  loss_dice_5: 4.093  loss_ce_6: 2.858  loss_mask_6: 1.416  loss_dice_6: 4.033  loss_ce_7: 2.668  loss_mask_7: 1.345  loss_dice_7: 4.087  loss_ce_8: 2.662  loss_mask_8: 1.373  loss_dice_8: 4.235  loss_mars: 0.5563    time: 5.6709  last_time: 6.0332  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 07:15:29] d2.utils.events INFO:  eta: 15:51:18  iter: 5579  total_loss: 93.28  loss_ce: 3.424  loss_mask: 1.819  loss_dice: 3.964  loss_ce_0: 3.683  loss_mask_0: 1.311  loss_dice_0: 3.696  loss_ce_1: 3.484  loss_mask_1: 1.318  loss_dice_1: 3.8  loss_ce_2: 3.607  loss_mask_2: 1.301  loss_dice_2: 3.99  loss_ce_3: 3.681  loss_mask_3: 1.229  loss_dice_3: 3.811  loss_ce_4: 3.087  loss_mask_4: 2.289  loss_dice_4: 4.722  loss_ce_5: 3.653  loss_mask_5: 1.559  loss_dice_5: 4.295  loss_ce_6: 3.505  loss_mask_6: 1.326  loss_dice_6: 4.035  loss_ce_7: 3.452  loss_mask_7: 1.61  loss_dice_7: 3.888  loss_ce_8: 3.281  loss_mask_8: 1.522  loss_dice_8: 3.902  loss_mars: 0.5338    time: 5.6702  last_time: 6.8593  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 07:17:24] d2.utils.events INFO:  eta: 15:53:02  iter: 5599  total_loss: 95.53  loss_ce: 3.862  loss_mask: 0.9055  loss_dice: 4.551  loss_ce_0: 3.8  loss_mask_0: 0.9279  loss_dice_0: 4.401  loss_ce_1: 3.828  loss_mask_1: 0.9554  loss_dice_1: 4.066  loss_ce_2: 3.779  loss_mask_2: 0.9411  loss_dice_2: 4.35  loss_ce_3: 3.627  loss_mask_3: 0.9025  loss_dice_3: 4.218  loss_ce_4: 3.622  loss_mask_4: 0.9512  loss_dice_4: 4.637  loss_ce_5: 3.77  loss_mask_5: 0.9704  loss_dice_5: 4.591  loss_ce_6: 3.676  loss_mask_6: 0.9975  loss_dice_6: 4.509  loss_ce_7: 3.765  loss_mask_7: 0.902  loss_dice_7: 4.367  loss_ce_8: 3.512  loss_mask_8: 0.9439  loss_dice_8: 4.481  loss_mars: 0.5685    time: 5.6706  last_time: 5.3011  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 07:19:23] d2.utils.events INFO:  eta: 15:52:40  iter: 5619  total_loss: 92.89  loss_ce: 3.989  loss_mask: 0.9308  loss_dice: 3.981  loss_ce_0: 3.916  loss_mask_0: 0.7505  loss_dice_0: 4.373  loss_ce_1: 3.801  loss_mask_1: 0.8327  loss_dice_1: 4.089  loss_ce_2: 3.615  loss_mask_2: 0.7829  loss_dice_2: 4.062  loss_ce_3: 3.861  loss_mask_3: 0.881  loss_dice_3: 4.13  loss_ce_4: 3.693  loss_mask_4: 0.7489  loss_dice_4: 4.52  loss_ce_5: 3.803  loss_mask_5: 0.9975  loss_dice_5: 4.505  loss_ce_6: 3.513  loss_mask_6: 0.8858  loss_dice_6: 4.133  loss_ce_7: 3.51  loss_mask_7: 0.8009  loss_dice_7: 3.997  loss_ce_8: 3.533  loss_mask_8: 0.936  loss_dice_8: 4.146  loss_mars: 0.8442    time: 5.6719  last_time: 5.3468  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 07:21:17] d2.utils.events INFO:  eta: 15:53:36  iter: 5639  total_loss: 91.32  loss_ce: 3.58  loss_mask: 0.967  loss_dice: 4.281  loss_ce_0: 4.436  loss_mask_0: 0.9459  loss_dice_0: 4.275  loss_ce_1: 3.411  loss_mask_1: 0.982  loss_dice_1: 4.044  loss_ce_2: 3.173  loss_mask_2: 0.918  loss_dice_2: 4.203  loss_ce_3: 3.242  loss_mask_3: 0.9108  loss_dice_3: 4.268  loss_ce_4: 3.314  loss_mask_4: 1.048  loss_dice_4: 4.312  loss_ce_5: 3.067  loss_mask_5: 0.9828  loss_dice_5: 4.407  loss_ce_6: 3.159  loss_mask_6: 0.943  loss_dice_6: 4.323  loss_ce_7: 3.338  loss_mask_7: 1.064  loss_dice_7: 4.309  loss_ce_8: 3.435  loss_mask_8: 0.939  loss_dice_8: 4.344  loss_mars: 0.7024    time: 5.6719  last_time: 5.4607  data_time: 0.0030  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 07:23:11] d2.utils.events INFO:  eta: 15:49:38  iter: 5659  total_loss: 90.46  loss_ce: 3.487  loss_mask: 1.047  loss_dice: 4.161  loss_ce_0: 3.096  loss_mask_0: 1.051  loss_dice_0: 4.199  loss_ce_1: 3.214  loss_mask_1: 1.028  loss_dice_1: 4.089  loss_ce_2: 3.39  loss_mask_2: 1.032  loss_dice_2: 4.152  loss_ce_3: 3.077  loss_mask_3: 1.142  loss_dice_3: 4.095  loss_ce_4: 3.31  loss_mask_4: 1.161  loss_dice_4: 4.32  loss_ce_5: 3.468  loss_mask_5: 1.095  loss_dice_5: 4.132  loss_ce_6: 3.398  loss_mask_6: 1.114  loss_dice_6: 4.29  loss_ce_7: 3.58  loss_mask_7: 1.178  loss_dice_7: 4.089  loss_ce_8: 3.343  loss_mask_8: 1.01  loss_dice_8: 4.337  loss_mars: 0.643    time: 5.6721  last_time: 5.2063  data_time: 0.0022  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 07:25:01] d2.utils.events INFO:  eta: 15:50:16  iter: 5679  total_loss: 86.19  loss_ce: 2.755  loss_mask: 1.288  loss_dice: 4.29  loss_ce_0: 2.863  loss_mask_0: 1.227  loss_dice_0: 4.374  loss_ce_1: 2.739  loss_mask_1: 1.314  loss_dice_1: 4.236  loss_ce_2: 2.85  loss_mask_2: 1.379  loss_dice_2: 4.164  loss_ce_3: 2.912  loss_mask_3: 1.415  loss_dice_3: 4.299  loss_ce_4: 2.87  loss_mask_4: 1.39  loss_dice_4: 4.222  loss_ce_5: 2.836  loss_mask_5: 1.336  loss_dice_5: 4.164  loss_ce_6: 2.968  loss_mask_6: 1.339  loss_dice_6: 4.155  loss_ce_7: 2.719  loss_mask_7: 1.487  loss_dice_7: 4.281  loss_ce_8: 3.045  loss_mask_8: 1.446  loss_dice_8: 4.225  loss_mars: 0.7792    time: 5.6713  last_time: 5.2902  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 07:26:57] d2.utils.events INFO:  eta: 15:49:25  iter: 5699  total_loss: 88.62  loss_ce: 3.027  loss_mask: 1.325  loss_dice: 4.004  loss_ce_0: 2.445  loss_mask_0: 1.348  loss_dice_0: 4.087  loss_ce_1: 2.603  loss_mask_1: 1.14  loss_dice_1: 3.984  loss_ce_2: 2.528  loss_mask_2: 1.129  loss_dice_2: 4.074  loss_ce_3: 2.394  loss_mask_3: 1.207  loss_dice_3: 4.191  loss_ce_4: 2.2  loss_mask_4: 1.354  loss_dice_4: 4.355  loss_ce_5: 2.343  loss_mask_5: 1.167  loss_dice_5: 4.298  loss_ce_6: 2.502  loss_mask_6: 1.373  loss_dice_6: 3.965  loss_ce_7: 2.984  loss_mask_7: 1.25  loss_dice_7: 4.032  loss_ce_8: 2.95  loss_mask_8: 1.074  loss_dice_8: 3.858  loss_mars: 0.5983    time: 5.6720  last_time: 5.2788  data_time: 0.0025  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 07:29:20] d2.utils.events INFO:  eta: 15:49:42  iter: 5719  total_loss: 87.66  loss_ce: 2.94  loss_mask: 1.468  loss_dice: 4.051  loss_ce_0: 2.966  loss_mask_0: 1.395  loss_dice_0: 4.03  loss_ce_1: 2.648  loss_mask_1: 1.75  loss_dice_1: 4.035  loss_ce_2: 2.71  loss_mask_2: 1.559  loss_dice_2: 4.097  loss_ce_3: 2.504  loss_mask_3: 1.813  loss_dice_3: 4.084  loss_ce_4: 2.608  loss_mask_4: 1.595  loss_dice_4: 4.038  loss_ce_5: 2.566  loss_mask_5: 1.705  loss_dice_5: 4.049  loss_ce_6: 2.692  loss_mask_6: 1.574  loss_dice_6: 4.149  loss_ce_7: 3.049  loss_mask_7: 1.614  loss_dice_7: 3.966  loss_ce_8: 2.664  loss_mask_8: 1.847  loss_dice_8: 4.152  loss_mars: 0.4289    time: 5.6790  last_time: 6.5889  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 07:31:23] d2.utils.events INFO:  eta: 15:49:01  iter: 5739  total_loss: 87.25  loss_ce: 2.534  loss_mask: 2.011  loss_dice: 3.582  loss_ce_0: 3.041  loss_mask_0: 1.952  loss_dice_0: 3.688  loss_ce_1: 2.491  loss_mask_1: 2.175  loss_dice_1: 3.663  loss_ce_2: 2.238  loss_mask_2: 2.116  loss_dice_2: 3.694  loss_ce_3: 2.393  loss_mask_3: 2.402  loss_dice_3: 3.692  loss_ce_4: 2.107  loss_mask_4: 2.189  loss_dice_4: 3.765  loss_ce_5: 2.055  loss_mask_5: 1.979  loss_dice_5: 3.745  loss_ce_6: 2.322  loss_mask_6: 1.949  loss_dice_6: 3.761  loss_ce_7: 2.596  loss_mask_7: 1.969  loss_dice_7: 3.653  loss_ce_8: 2.276  loss_mask_8: 2.431  loss_dice_8: 3.867  loss_mars: 0.6485    time: 5.6813  last_time: 6.2043  data_time: 0.0023  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 07:33:13] d2.utils.events INFO:  eta: 15:45:52  iter: 5759  total_loss: 90.43  loss_ce: 3.096  loss_mask: 1.314  loss_dice: 4.131  loss_ce_0: 3.986  loss_mask_0: 1.146  loss_dice_0: 4.096  loss_ce_1: 3.156  loss_mask_1: 1.33  loss_dice_1: 4.109  loss_ce_2: 2.986  loss_mask_2: 1.413  loss_dice_2: 4.112  loss_ce_3: 2.886  loss_mask_3: 1.215  loss_dice_3: 4.096  loss_ce_4: 2.974  loss_mask_4: 1.658  loss_dice_4: 4.372  loss_ce_5: 2.892  loss_mask_5: 1.551  loss_dice_5: 4.294  loss_ce_6: 3.175  loss_mask_6: 1.285  loss_dice_6: 3.977  loss_ce_7: 3.052  loss_mask_7: 1.365  loss_dice_7: 4.242  loss_ce_8: 2.717  loss_mask_8: 1.428  loss_dice_8: 4.26  loss_mars: 0.6864    time: 5.6804  last_time: 5.7501  data_time: 0.0023  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 07:35:08] d2.utils.events INFO:  eta: 15:46:41  iter: 5779  total_loss: 84.33  loss_ce: 3.645  loss_mask: 1.541  loss_dice: 3.78  loss_ce_0: 3.185  loss_mask_0: 1.344  loss_dice_0: 3.928  loss_ce_1: 3.061  loss_mask_1: 1.436  loss_dice_1: 3.856  loss_ce_2: 3.13  loss_mask_2: 1.594  loss_dice_2: 3.809  loss_ce_3: 3.033  loss_mask_3: 1.496  loss_dice_3: 3.789  loss_ce_4: 2.918  loss_mask_4: 1.769  loss_dice_4: 4.235  loss_ce_5: 2.889  loss_mask_5: 1.702  loss_dice_5: 4.061  loss_ce_6: 3.19  loss_mask_6: 1.505  loss_dice_6: 3.785  loss_ce_7: 3.175  loss_mask_7: 1.528  loss_dice_7: 3.833  loss_ce_8: 3.187  loss_mask_8: 1.51  loss_dice_8: 3.934  loss_mars: 0.7683    time: 5.6808  last_time: 5.3028  data_time: 0.0024  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 07:37:01] d2.utils.events INFO:  eta: 15:43:15  iter: 5799  total_loss: 95.76  loss_ce: 3.948  loss_mask: 1.321  loss_dice: 4.246  loss_ce_0: 4.135  loss_mask_0: 1.887  loss_dice_0: 4.565  loss_ce_1: 3.86  loss_mask_1: 1.365  loss_dice_1: 4.35  loss_ce_2: 3.775  loss_mask_2: 1.339  loss_dice_2: 4.271  loss_ce_3: 3.547  loss_mask_3: 1.531  loss_dice_3: 4.193  loss_ce_4: 3.488  loss_mask_4: 1.22  loss_dice_4: 4.42  loss_ce_5: 3.475  loss_mask_5: 1.477  loss_dice_5: 4.38  loss_ce_6: 3.731  loss_mask_6: 1.365  loss_dice_6: 4.437  loss_ce_7: 3.673  loss_mask_7: 1.454  loss_dice_7: 4.534  loss_ce_8: 3.648  loss_mask_8: 1.583  loss_dice_8: 4.262  loss_mars: 0.7367    time: 5.6805  last_time: 5.4819  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 07:38:57] d2.utils.events INFO:  eta: 15:39:46  iter: 5819  total_loss: 102.8  loss_ce: 3.863  loss_mask: 0.9285  loss_dice: 4.161  loss_ce_0: 5.171  loss_mask_0: 0.8888  loss_dice_0: 4.404  loss_ce_1: 4.009  loss_mask_1: 0.8867  loss_dice_1: 4.366  loss_ce_2: 3.789  loss_mask_2: 0.8954  loss_dice_2: 4.276  loss_ce_3: 3.86  loss_mask_3: 0.9157  loss_dice_3: 4.386  loss_ce_4: 3.721  loss_mask_4: 0.9218  loss_dice_4: 4.4  loss_ce_5: 3.741  loss_mask_5: 1.024  loss_dice_5: 4.258  loss_ce_6: 3.903  loss_mask_6: 0.8643  loss_dice_6: 4.277  loss_ce_7: 3.757  loss_mask_7: 1.14  loss_dice_7: 4.348  loss_ce_8: 3.865  loss_mask_8: 0.9238  loss_dice_8: 4.25  loss_mars: 0.8444    time: 5.6810  last_time: 5.2157  data_time: 0.0028  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 07:40:55] d2.utils.events INFO:  eta: 15:38:12  iter: 5839  total_loss: 89.26  loss_ce: 3.693  loss_mask: 1.072  loss_dice: 4.448  loss_ce_0: 4.592  loss_mask_0: 1.005  loss_dice_0: 4.371  loss_ce_1: 3.534  loss_mask_1: 1.032  loss_dice_1: 4.511  loss_ce_2: 3.412  loss_mask_2: 0.9747  loss_dice_2: 4.449  loss_ce_3: 3.253  loss_mask_3: 1.06  loss_dice_3: 4.386  loss_ce_4: 3.22  loss_mask_4: 0.8527  loss_dice_4: 4.308  loss_ce_5: 3.296  loss_mask_5: 0.8902  loss_dice_5: 4.392  loss_ce_6: 3.326  loss_mask_6: 0.9776  loss_dice_6: 4.417  loss_ce_7: 3.364  loss_mask_7: 1.135  loss_dice_7: 4.483  loss_ce_8: 3.657  loss_mask_8: 1.142  loss_dice_8: 4.496  loss_mars: 0.7731    time: 5.6822  last_time: 5.2876  data_time: 0.0028  last_data_time: 0.0041   lr: 0.0001  max_mem: 0M
[10/19 07:42:55] d2.utils.events INFO:  eta: 15:36:17  iter: 5859  total_loss: 92.72  loss_ce: 3.52  loss_mask: 1.264  loss_dice: 4.326  loss_ce_0: 4.19  loss_mask_0: 1.078  loss_dice_0: 4.222  loss_ce_1: 3.342  loss_mask_1: 1.125  loss_dice_1: 4.291  loss_ce_2: 3.398  loss_mask_2: 1.273  loss_dice_2: 4.246  loss_ce_3: 3.464  loss_mask_3: 1.123  loss_dice_3: 4.276  loss_ce_4: 3.33  loss_mask_4: 1.203  loss_dice_4: 4.275  loss_ce_5: 3.272  loss_mask_5: 1.156  loss_dice_5: 4.262  loss_ce_6: 3.372  loss_mask_6: 1.163  loss_dice_6: 4.298  loss_ce_7: 3.269  loss_mask_7: 1.156  loss_dice_7: 4.164  loss_ce_8: 3.822  loss_mask_8: 1.183  loss_dice_8: 4.268  loss_mars: 0.7436    time: 5.6837  last_time: 5.9519  data_time: 0.0026  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 07:44:49] d2.utils.events INFO:  eta: 15:33:54  iter: 5879  total_loss: 83.58  loss_ce: 2.91  loss_mask: 1.09  loss_dice: 4.061  loss_ce_0: 3.157  loss_mask_0: 1.34  loss_dice_0: 4.099  loss_ce_1: 2.776  loss_mask_1: 1.226  loss_dice_1: 4.043  loss_ce_2: 2.898  loss_mask_2: 1.116  loss_dice_2: 4.052  loss_ce_3: 2.916  loss_mask_3: 1.268  loss_dice_3: 4.012  loss_ce_4: 2.994  loss_mask_4: 1.294  loss_dice_4: 4.091  loss_ce_5: 2.841  loss_mask_5: 1.368  loss_dice_5: 4.092  loss_ce_6: 2.959  loss_mask_6: 1.263  loss_dice_6: 4.173  loss_ce_7: 2.938  loss_mask_7: 1.209  loss_dice_7: 4.081  loss_ce_8: 2.866  loss_mask_8: 1.196  loss_dice_8: 4.076  loss_mars: 0.6433    time: 5.6836  last_time: 5.2724  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 07:46:40] d2.utils.events INFO:  eta: 15:31:27  iter: 5899  total_loss: 98.76  loss_ce: 4.07  loss_mask: 1.303  loss_dice: 4.355  loss_ce_0: 4.63  loss_mask_0: 1.384  loss_dice_0: 4.169  loss_ce_1: 3.953  loss_mask_1: 1.162  loss_dice_1: 4.462  loss_ce_2: 3.744  loss_mask_2: 1.103  loss_dice_2: 4.335  loss_ce_3: 3.875  loss_mask_3: 1.439  loss_dice_3: 4.198  loss_ce_4: 3.948  loss_mask_4: 1.317  loss_dice_4: 3.976  loss_ce_5: 3.879  loss_mask_5: 1.337  loss_dice_5: 4.111  loss_ce_6: 3.839  loss_mask_6: 1.216  loss_dice_6: 4.252  loss_ce_7: 3.818  loss_mask_7: 1.362  loss_dice_7: 4.343  loss_ce_8: 3.805  loss_mask_8: 1.273  loss_dice_8: 4.241  loss_mars: 0.6993    time: 5.6832  last_time: 5.0877  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 07:48:31] d2.utils.events INFO:  eta: 15:29:32  iter: 5919  total_loss: 92.43  loss_ce: 3.102  loss_mask: 1.149  loss_dice: 4.405  loss_ce_0: 3.501  loss_mask_0: 0.9673  loss_dice_0: 4.286  loss_ce_1: 3.076  loss_mask_1: 1.168  loss_dice_1: 4.304  loss_ce_2: 3.177  loss_mask_2: 1.186  loss_dice_2: 4.334  loss_ce_3: 3.221  loss_mask_3: 1.128  loss_dice_3: 4.313  loss_ce_4: 3.016  loss_mask_4: 1.167  loss_dice_4: 4.26  loss_ce_5: 3.192  loss_mask_5: 1.185  loss_dice_5: 4.256  loss_ce_6: 3.139  loss_mask_6: 1.168  loss_dice_6: 4.355  loss_ce_7: 3.202  loss_mask_7: 1.057  loss_dice_7: 4.466  loss_ce_8: 3.163  loss_mask_8: 1.157  loss_dice_8: 4.4  loss_mars: 0.6021    time: 5.6824  last_time: 5.6879  data_time: 0.0022  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 07:50:25] d2.utils.events INFO:  eta: 15:26:28  iter: 5939  total_loss: 92.44  loss_ce: 3.266  loss_mask: 1.072  loss_dice: 4.44  loss_ce_0: 4.363  loss_mask_0: 0.9699  loss_dice_0: 4.427  loss_ce_1: 3.435  loss_mask_1: 0.9907  loss_dice_1: 4.426  loss_ce_2: 3.339  loss_mask_2: 1.006  loss_dice_2: 4.406  loss_ce_3: 3.059  loss_mask_3: 1.018  loss_dice_3: 4.413  loss_ce_4: 2.86  loss_mask_4: 0.9987  loss_dice_4: 4.497  loss_ce_5: 3.051  loss_mask_5: 1.013  loss_dice_5: 4.472  loss_ce_6: 3.277  loss_mask_6: 0.9542  loss_dice_6: 4.336  loss_ce_7: 3.4  loss_mask_7: 1.003  loss_dice_7: 4.394  loss_ce_8: 3.111  loss_mask_8: 1.191  loss_dice_8: 4.666  loss_mars: 0.6871    time: 5.6824  last_time: 5.3443  data_time: 0.0025  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 07:52:16] d2.utils.events INFO:  eta: 15:23:52  iter: 5959  total_loss: 91.01  loss_ce: 3.118  loss_mask: 1.159  loss_dice: 4.344  loss_ce_0: 3.212  loss_mask_0: 1.204  loss_dice_0: 3.989  loss_ce_1: 2.828  loss_mask_1: 1.252  loss_dice_1: 4.108  loss_ce_2: 2.872  loss_mask_2: 1.156  loss_dice_2: 4.049  loss_ce_3: 3.018  loss_mask_3: 1.174  loss_dice_3: 4.142  loss_ce_4: 2.847  loss_mask_4: 1.437  loss_dice_4: 4.395  loss_ce_5: 2.829  loss_mask_5: 1.251  loss_dice_5: 4.069  loss_ce_6: 3.206  loss_mask_6: 1.145  loss_dice_6: 4.043  loss_ce_7: 2.832  loss_mask_7: 1.103  loss_dice_7: 4.299  loss_ce_8: 3.185  loss_mask_8: 1.235  loss_dice_8: 4.268  loss_mars: 0.3028    time: 5.6819  last_time: 8.0890  data_time: 0.0023  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 07:54:04] d2.utils.events INFO:  eta: 15:17:01  iter: 5979  total_loss: 97.08  loss_ce: 3.874  loss_mask: 1.146  loss_dice: 4.437  loss_ce_0: 4.243  loss_mask_0: 0.9802  loss_dice_0: 4.321  loss_ce_1: 3.907  loss_mask_1: 1.074  loss_dice_1: 4.417  loss_ce_2: 3.832  loss_mask_2: 1.112  loss_dice_2: 4.399  loss_ce_3: 3.997  loss_mask_3: 1.445  loss_dice_3: 4.499  loss_ce_4: 3.981  loss_mask_4: 1.334  loss_dice_4: 4.558  loss_ce_5: 4.214  loss_mask_5: 1.085  loss_dice_5: 4.423  loss_ce_6: 4.186  loss_mask_6: 1.025  loss_dice_6: 4.405  loss_ce_7: 4.081  loss_mask_7: 1.153  loss_dice_7: 4.496  loss_ce_8: 4.095  loss_mask_8: 1.072  loss_dice_8: 4.513  loss_mars: 0.7152    time: 5.6806  last_time: 5.9558  data_time: 0.0023  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 07:55:53] d2.utils.events INFO:  eta: 15:10:11  iter: 5999  total_loss: 89.86  loss_ce: 3.149  loss_mask: 1.099  loss_dice: 4.239  loss_ce_0: 3.218  loss_mask_0: 1.184  loss_dice_0: 4.235  loss_ce_1: 3.358  loss_mask_1: 1.062  loss_dice_1: 4.276  loss_ce_2: 3.193  loss_mask_2: 1.172  loss_dice_2: 4.315  loss_ce_3: 3.314  loss_mask_3: 1.17  loss_dice_3: 4.225  loss_ce_4: 3.423  loss_mask_4: 1.022  loss_dice_4: 4.583  loss_ce_5: 3.464  loss_mask_5: 1.071  loss_dice_5: 4.196  loss_ce_6: 3.407  loss_mask_6: 1.247  loss_dice_6: 4.245  loss_ce_7: 3.414  loss_mask_7: 1.203  loss_dice_7: 4.347  loss_ce_8: 3.162  loss_mask_8: 1.39  loss_dice_8: 4.447  loss_mars: 0.5989    time: 5.6795  last_time: 6.5388  data_time: 0.0024  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 07:57:49] d2.utils.events INFO:  eta: 15:08:18  iter: 6019  total_loss: 85.71  loss_ce: 2.719  loss_mask: 1.306  loss_dice: 4.122  loss_ce_0: 3.386  loss_mask_0: 1.142  loss_dice_0: 3.907  loss_ce_1: 3.111  loss_mask_1: 1.191  loss_dice_1: 4.122  loss_ce_2: 3.051  loss_mask_2: 1.151  loss_dice_2: 4.04  loss_ce_3: 2.712  loss_mask_3: 1.154  loss_dice_3: 4.052  loss_ce_4: 2.966  loss_mask_4: 1.166  loss_dice_4: 4.393  loss_ce_5: 3.124  loss_mask_5: 1.189  loss_dice_5: 4.09  loss_ce_6: 3.187  loss_mask_6: 1.207  loss_dice_6: 3.888  loss_ce_7: 3.122  loss_mask_7: 1.163  loss_dice_7: 4.1  loss_ce_8: 2.512  loss_mask_8: 1.313  loss_dice_8: 3.883  loss_mars: 0.7897    time: 5.6800  last_time: 7.0863  data_time: 0.0027  last_data_time: 0.0036   lr: 0.0001  max_mem: 0M
[10/19 07:59:40] d2.utils.events INFO:  eta: 15:05:01  iter: 6039  total_loss: 99.99  loss_ce: 3.459  loss_mask: 1.154  loss_dice: 4.294  loss_ce_0: 3.317  loss_mask_0: 0.7789  loss_dice_0: 4.506  loss_ce_1: 3.392  loss_mask_1: 0.7774  loss_dice_1: 4.459  loss_ce_2: 3.11  loss_mask_2: 0.9123  loss_dice_2: 4.522  loss_ce_3: 3.151  loss_mask_3: 0.8352  loss_dice_3: 4.402  loss_ce_4: 2.795  loss_mask_4: 1.044  loss_dice_4: 4.539  loss_ce_5: 3.115  loss_mask_5: 0.8795  loss_dice_5: 4.325  loss_ce_6: 3.317  loss_mask_6: 0.7878  loss_dice_6: 4.478  loss_ce_7: 3.491  loss_mask_7: 0.833  loss_dice_7: 4.388  loss_ce_8: 3.619  loss_mask_8: 1.072  loss_dice_8: 4.45  loss_mars: 0.6943    time: 5.6794  last_time: 5.7274  data_time: 0.0026  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 08:01:30] d2.utils.events INFO:  eta: 15:01:39  iter: 6059  total_loss: 104.2  loss_ce: 3.8  loss_mask: 1.387  loss_dice: 4.346  loss_ce_0: 3.786  loss_mask_0: 1.181  loss_dice_0: 4.417  loss_ce_1: 4.207  loss_mask_1: 1.166  loss_dice_1: 4.475  loss_ce_2: 4.012  loss_mask_2: 1.209  loss_dice_2: 4.383  loss_ce_3: 3.857  loss_mask_3: 1.164  loss_dice_3: 4.37  loss_ce_4: 4.067  loss_mask_4: 1.302  loss_dice_4: 4.587  loss_ce_5: 3.975  loss_mask_5: 1.609  loss_dice_5: 4.331  loss_ce_6: 3.668  loss_mask_6: 1.212  loss_dice_6: 4.326  loss_ce_7: 3.755  loss_mask_7: 1.193  loss_dice_7: 4.329  loss_ce_8: 3.674  loss_mask_8: 1.356  loss_dice_8: 4.326  loss_mars: 0.6548    time: 5.6787  last_time: 4.8586  data_time: 0.0021  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 08:03:22] d2.utils.events INFO:  eta: 14:57:08  iter: 6079  total_loss: 95.8  loss_ce: 3.828  loss_mask: 1.223  loss_dice: 4.557  loss_ce_0: 3.715  loss_mask_0: 0.9934  loss_dice_0: 4.198  loss_ce_1: 4.05  loss_mask_1: 0.9881  loss_dice_1: 4.201  loss_ce_2: 4.102  loss_mask_2: 0.9681  loss_dice_2: 4.223  loss_ce_3: 3.823  loss_mask_3: 0.9591  loss_dice_3: 4.181  loss_ce_4: 3.534  loss_mask_4: 1.223  loss_dice_4: 4.409  loss_ce_5: 3.297  loss_mask_5: 1.143  loss_dice_5: 4.551  loss_ce_6: 3.634  loss_mask_6: 1.105  loss_dice_6: 4.337  loss_ce_7: 3.777  loss_mask_7: 1.322  loss_dice_7: 4.542  loss_ce_8: 3.836  loss_mask_8: 1.629  loss_dice_8: 4.534  loss_mars: 0.3707    time: 5.6784  last_time: 5.9669  data_time: 0.0025  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 08:05:18] d2.utils.events INFO:  eta: 14:51:19  iter: 6099  total_loss: 82.56  loss_ce: 2.949  loss_mask: 0.8797  loss_dice: 4.037  loss_ce_0: 3.305  loss_mask_0: 0.832  loss_dice_0: 3.976  loss_ce_1: 2.971  loss_mask_1: 0.8125  loss_dice_1: 4.017  loss_ce_2: 2.888  loss_mask_2: 0.8165  loss_dice_2: 4.14  loss_ce_3: 2.889  loss_mask_3: 0.9391  loss_dice_3: 4.012  loss_ce_4: 3.054  loss_mask_4: 0.7983  loss_dice_4: 3.966  loss_ce_5: 2.731  loss_mask_5: 0.8561  loss_dice_5: 4.203  loss_ce_6: 2.829  loss_mask_6: 0.8989  loss_dice_6: 4.121  loss_ce_7: 2.8  loss_mask_7: 0.9069  loss_dice_7: 4.173  loss_ce_8: 3.071  loss_mask_8: 0.9458  loss_dice_8: 4.174  loss_mars: 0.7366    time: 5.6789  last_time: 5.0404  data_time: 0.0023  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 08:07:15] d2.utils.events INFO:  eta: 14:53:22  iter: 6119  total_loss: 89.8  loss_ce: 4.13  loss_mask: 1.185  loss_dice: 4.213  loss_ce_0: 3.88  loss_mask_0: 0.8389  loss_dice_0: 4.22  loss_ce_1: 3.501  loss_mask_1: 0.8131  loss_dice_1: 4.322  loss_ce_2: 3.404  loss_mask_2: 0.798  loss_dice_2: 4.153  loss_ce_3: 3.196  loss_mask_3: 0.7519  loss_dice_3: 4.094  loss_ce_4: 3.125  loss_mask_4: 0.7935  loss_dice_4: 4.215  loss_ce_5: 3.51  loss_mask_5: 0.898  loss_dice_5: 4.254  loss_ce_6: 3.32  loss_mask_6: 0.853  loss_dice_6: 4.262  loss_ce_7: 3.902  loss_mask_7: 1.108  loss_dice_7: 4.126  loss_ce_8: 4.073  loss_mask_8: 1.197  loss_dice_8: 4.227  loss_mars: 0.8136    time: 5.6797  last_time: 6.6147  data_time: 0.0026  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 08:09:04] d2.utils.events INFO:  eta: 14:49:09  iter: 6139  total_loss: 97.39  loss_ce: 3.978  loss_mask: 0.871  loss_dice: 4.304  loss_ce_0: 3.796  loss_mask_0: 0.8512  loss_dice_0: 4.206  loss_ce_1: 3.177  loss_mask_1: 0.9175  loss_dice_1: 4.426  loss_ce_2: 3.192  loss_mask_2: 1.058  loss_dice_2: 4.275  loss_ce_3: 3.267  loss_mask_3: 1.001  loss_dice_3: 4.282  loss_ce_4: 3.571  loss_mask_4: 1.155  loss_dice_4: 4.188  loss_ce_5: 3.419  loss_mask_5: 0.9128  loss_dice_5: 4.304  loss_ce_6: 3.588  loss_mask_6: 0.8706  loss_dice_6: 4.253  loss_ce_7: 3.608  loss_mask_7: 0.9239  loss_dice_7: 4.32  loss_ce_8: 3.354  loss_mask_8: 1.765  loss_dice_8: 4.894  loss_mars: 0.6518    time: 5.6786  last_time: 5.5226  data_time: 0.0023  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 08:10:54] d2.utils.events INFO:  eta: 14:46:25  iter: 6159  total_loss: 114.7  loss_ce: 5.794  loss_mask: 0.8655  loss_dice: 4.461  loss_ce_0: 6.093  loss_mask_0: 0.7724  loss_dice_0: 4.456  loss_ce_1: 5.801  loss_mask_1: 0.8521  loss_dice_1: 4.349  loss_ce_2: 5.709  loss_mask_2: 0.8273  loss_dice_2: 4.416  loss_ce_3: 5.867  loss_mask_3: 0.8301  loss_dice_3: 4.504  loss_ce_4: 5.731  loss_mask_4: 0.8819  loss_dice_4: 4.445  loss_ce_5: 5.907  loss_mask_5: 0.882  loss_dice_5: 4.527  loss_ce_6: 5.896  loss_mask_6: 0.9294  loss_dice_6: 4.543  loss_ce_7: 5.722  loss_mask_7: 0.8494  loss_dice_7: 4.519  loss_ce_8: 5.766  loss_mask_8: 0.9189  loss_dice_8: 4.582  loss_mars: 0.4863    time: 5.6778  last_time: 5.8254  data_time: 0.0027  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 08:12:48] d2.utils.events INFO:  eta: 14:42:51  iter: 6179  total_loss: 97.28  loss_ce: 3.184  loss_mask: 1.579  loss_dice: 4.769  loss_ce_0: 3.31  loss_mask_0: 1.129  loss_dice_0: 4.346  loss_ce_1: 4.114  loss_mask_1: 1.078  loss_dice_1: 4.324  loss_ce_2: 3.643  loss_mask_2: 1.203  loss_dice_2: 4.367  loss_ce_3: 3.131  loss_mask_3: 1.061  loss_dice_3: 4.526  loss_ce_4: 3.179  loss_mask_4: 1.176  loss_dice_4: 4.75  loss_ce_5: 3.561  loss_mask_5: 1.174  loss_dice_5: 4.446  loss_ce_6: 3.383  loss_mask_6: 1.389  loss_dice_6: 4.785  loss_ce_7: 3.173  loss_mask_7: 1.257  loss_dice_7: 4.58  loss_ce_8: 3.16  loss_mask_8: 1.414  loss_dice_8: 4.621  loss_mars: 0.361    time: 5.6779  last_time: 5.3551  data_time: 0.0024  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 08:14:41] d2.utils.events INFO:  eta: 14:43:32  iter: 6199  total_loss: 92.86  loss_ce: 3.987  loss_mask: 1.065  loss_dice: 4.521  loss_ce_0: 3.836  loss_mask_0: 0.9054  loss_dice_0: 4.408  loss_ce_1: 4.072  loss_mask_1: 0.9023  loss_dice_1: 4.46  loss_ce_2: 3.885  loss_mask_2: 0.9446  loss_dice_2: 4.54  loss_ce_3: 3.611  loss_mask_3: 0.9348  loss_dice_3: 4.541  loss_ce_4: 3.919  loss_mask_4: 0.9383  loss_dice_4: 4.584  loss_ce_5: 4.079  loss_mask_5: 0.9268  loss_dice_5: 4.52  loss_ce_6: 4.134  loss_mask_6: 0.8786  loss_dice_6: 4.447  loss_ce_7: 4.059  loss_mask_7: 0.9113  loss_dice_7: 4.481  loss_ce_8: 3.944  loss_mask_8: 1.053  loss_dice_8: 4.569  loss_mars: 0.6924    time: 5.6778  last_time: 5.1801  data_time: 0.0023  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 08:16:37] d2.utils.events INFO:  eta: 14:42:47  iter: 6219  total_loss: 89.02  loss_ce: 3.358  loss_mask: 0.8284  loss_dice: 4.274  loss_ce_0: 2.965  loss_mask_0: 0.8307  loss_dice_0: 4.142  loss_ce_1: 3.119  loss_mask_1: 0.779  loss_dice_1: 4.433  loss_ce_2: 3.022  loss_mask_2: 0.7229  loss_dice_2: 4.294  loss_ce_3: 2.925  loss_mask_3: 0.8709  loss_dice_3: 4.346  loss_ce_4: 2.656  loss_mask_4: 0.9884  loss_dice_4: 4.529  loss_ce_5: 3.188  loss_mask_5: 0.838  loss_dice_5: 4.147  loss_ce_6: 3.117  loss_mask_6: 0.9303  loss_dice_6: 4.367  loss_ce_7: 2.96  loss_mask_7: 0.8059  loss_dice_7: 4.456  loss_ce_8: 3.428  loss_mask_8: 0.8966  loss_dice_8: 4.3  loss_mars: 0.4587    time: 5.6783  last_time: 5.2751  data_time: 0.0028  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 08:18:32] d2.utils.events INFO:  eta: 14:40:55  iter: 6239  total_loss: 98.09  loss_ce: 3.55  loss_mask: 1.292  loss_dice: 4.413  loss_ce_0: 3.755  loss_mask_0: 1.359  loss_dice_0: 4.371  loss_ce_1: 3.929  loss_mask_1: 1.297  loss_dice_1: 4.364  loss_ce_2: 3.679  loss_mask_2: 1.327  loss_dice_2: 4.287  loss_ce_3: 3.741  loss_mask_3: 1.467  loss_dice_3: 4.391  loss_ce_4: 3.707  loss_mask_4: 1.119  loss_dice_4: 4.329  loss_ce_5: 4.177  loss_mask_5: 1.297  loss_dice_5: 4.387  loss_ce_6: 3.922  loss_mask_6: 1.276  loss_dice_6: 4.319  loss_ce_7: 3.716  loss_mask_7: 1.812  loss_dice_7: 4.63  loss_ce_8: 3.591  loss_mask_8: 1.444  loss_dice_8: 4.457  loss_mars: 0.6201    time: 5.6785  last_time: 5.2556  data_time: 0.0026  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 08:19:45] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0006251.pth
[10/19 08:19:45] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 08:19:45] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 08:19:46] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 08:19:46] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 08:20:34] d2.utils.events INFO:  eta: 14:42:48  iter: 6259  total_loss: 103  loss_ce: 4.299  loss_mask: 0.8697  loss_dice: 4.5  loss_ce_0: 4.356  loss_mask_0: 0.8527  loss_dice_0: 4.402  loss_ce_1: 4.754  loss_mask_1: 0.807  loss_dice_1: 4.415  loss_ce_2: 4.288  loss_mask_2: 0.8507  loss_dice_2: 4.394  loss_ce_3: 4.236  loss_mask_3: 0.867  loss_dice_3: 4.402  loss_ce_4: 4.154  loss_mask_4: 0.87  loss_dice_4: 4.429  loss_ce_5: 4.251  loss_mask_5: 0.8358  loss_dice_5: 4.489  loss_ce_6: 4.077  loss_mask_6: 0.8233  loss_dice_6: 4.51  loss_ce_7: 4.143  loss_mask_7: 0.7895  loss_dice_7: 4.491  loss_ce_8: 4.494  loss_mask_8: 0.7713  loss_dice_8: 4.501  loss_mars: 0.6674    time: 5.6802  last_time: 5.2506  data_time: 0.0029  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 08:22:27] d2.utils.events INFO:  eta: 14:40:55  iter: 6279  total_loss: 94.57  loss_ce: 3.788  loss_mask: 0.8487  loss_dice: 4.58  loss_ce_0: 3.504  loss_mask_0: 0.8472  loss_dice_0: 4.431  loss_ce_1: 3.752  loss_mask_1: 0.8086  loss_dice_1: 4.511  loss_ce_2: 3.574  loss_mask_2: 0.7013  loss_dice_2: 4.51  loss_ce_3: 3.357  loss_mask_3: 0.8823  loss_dice_3: 4.559  loss_ce_4: 3.399  loss_mask_4: 0.8197  loss_dice_4: 4.57  loss_ce_5: 3.681  loss_mask_5: 0.7033  loss_dice_5: 4.578  loss_ce_6: 3.722  loss_mask_6: 0.8218  loss_dice_6: 4.543  loss_ce_7: 3.801  loss_mask_7: 0.8262  loss_dice_7: 4.553  loss_ce_8: 3.846  loss_mask_8: 0.9305  loss_dice_8: 4.592  loss_mars: 0.6574    time: 5.6800  last_time: 6.0195  data_time: 0.0024  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 08:24:26] d2.utils.events INFO:  eta: 14:41:15  iter: 6299  total_loss: 99.77  loss_ce: 3.466  loss_mask: 2.322  loss_dice: 4.613  loss_ce_0: 3.334  loss_mask_0: 1.637  loss_dice_0: 3.952  loss_ce_1: 3.385  loss_mask_1: 1.536  loss_dice_1: 4.045  loss_ce_2: 3.485  loss_mask_2: 1.908  loss_dice_2: 4.29  loss_ce_3: 3.499  loss_mask_3: 1.963  loss_dice_3: 4.46  loss_ce_4: 3.568  loss_mask_4: 1.821  loss_dice_4: 4.492  loss_ce_5: 3.622  loss_mask_5: 1.792  loss_dice_5: 4.271  loss_ce_6: 3.609  loss_mask_6: 2.206  loss_dice_6: 4.395  loss_ce_7: 3.701  loss_mask_7: 2.588  loss_dice_7: 4.446  loss_ce_8: 3.524  loss_mask_8: 2.624  loss_dice_8: 4.527  loss_mars: -4.441e-06    time: 5.6811  last_time: 5.9672  data_time: 0.0023  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 08:26:27] d2.utils.events INFO:  eta: 14:38:35  iter: 6319  total_loss: 98.53  loss_ce: 3.783  loss_mask: 1.21  loss_dice: 4.742  loss_ce_0: 3.547  loss_mask_0: 0.859  loss_dice_0: 4.315  loss_ce_1: 3.481  loss_mask_1: 0.9219  loss_dice_1: 4.36  loss_ce_2: 3.462  loss_mask_2: 1.15  loss_dice_2: 4.636  loss_ce_3: 3.595  loss_mask_3: 1.219  loss_dice_3: 4.615  loss_ce_4: 3.529  loss_mask_4: 1.193  loss_dice_4: 4.62  loss_ce_5: 3.615  loss_mask_5: 1.211  loss_dice_5: 4.678  loss_ce_6: 3.677  loss_mask_6: 1.207  loss_dice_6: 4.721  loss_ce_7: 3.578  loss_mask_7: 1.346  loss_dice_7: 4.713  loss_ce_8: 3.639  loss_mask_8: 1.2  loss_dice_8: 4.749  loss_mars: 0.2067    time: 5.6827  last_time: 6.2330  data_time: 0.0026  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 08:28:17] d2.utils.events INFO:  eta: 14:34:06  iter: 6339  total_loss: 87.57  loss_ce: 3.052  loss_mask: 1.412  loss_dice: 4.536  loss_ce_0: 2.89  loss_mask_0: 0.7677  loss_dice_0: 4.14  loss_ce_1: 2.803  loss_mask_1: 0.9921  loss_dice_1: 4.179  loss_ce_2: 3.274  loss_mask_2: 1.011  loss_dice_2: 4.475  loss_ce_3: 3.287  loss_mask_3: 0.9085  loss_dice_3: 4.533  loss_ce_4: 3.118  loss_mask_4: 1.018  loss_dice_4: 4.51  loss_ce_5: 3.066  loss_mask_5: 1.022  loss_dice_5: 4.494  loss_ce_6: 2.964  loss_mask_6: 1.105  loss_dice_6: 4.481  loss_ce_7: 2.957  loss_mask_7: 1.691  loss_dice_7: 4.403  loss_ce_8: 3.058  loss_mask_8: 1.183  loss_dice_8: 4.655  loss_mars: 0.4822    time: 5.6819  last_time: 6.3613  data_time: 0.0023  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 08:30:16] d2.utils.events INFO:  eta: 14:33:56  iter: 6359  total_loss: 100.4  loss_ce: 3.48  loss_mask: 1.888  loss_dice: 4.535  loss_ce_0: 3.386  loss_mask_0: 1.555  loss_dice_0: 4.31  loss_ce_1: 3.512  loss_mask_1: 2.033  loss_dice_1: 4.296  loss_ce_2: 3.419  loss_mask_2: 1.561  loss_dice_2: 4.372  loss_ce_3: 3.363  loss_mask_3: 1.537  loss_dice_3: 4.387  loss_ce_4: 3.414  loss_mask_4: 1.53  loss_dice_4: 4.357  loss_ce_5: 3.463  loss_mask_5: 1.543  loss_dice_5: 4.375  loss_ce_6: 3.417  loss_mask_6: 1.637  loss_dice_6: 4.262  loss_ce_7: 3.481  loss_mask_7: 1.939  loss_dice_7: 4.433  loss_ce_8: 3.493  loss_mask_8: 2.327  loss_dice_8: 4.412  loss_mars: 0.5174    time: 5.6830  last_time: 6.0739  data_time: 0.0029  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 08:32:08] d2.utils.events INFO:  eta: 14:31:29  iter: 6379  total_loss: 93.7  loss_ce: 3.47  loss_mask: 1.773  loss_dice: 4.606  loss_ce_0: 2.988  loss_mask_0: 1.376  loss_dice_0: 4.297  loss_ce_1: 3.247  loss_mask_1: 1.505  loss_dice_1: 4.151  loss_ce_2: 3.456  loss_mask_2: 1.659  loss_dice_2: 4.371  loss_ce_3: 3.388  loss_mask_3: 1.619  loss_dice_3: 4.349  loss_ce_4: 3.341  loss_mask_4: 1.675  loss_dice_4: 4.387  loss_ce_5: 3.416  loss_mask_5: 1.654  loss_dice_5: 4.388  loss_ce_6: 3.427  loss_mask_6: 1.648  loss_dice_6: 4.39  loss_ce_7: 3.366  loss_mask_7: 1.669  loss_dice_7: 4.509  loss_ce_8: 3.392  loss_mask_8: 1.635  loss_dice_8: 4.651  loss_mars: 0.4841    time: 5.6827  last_time: 5.3572  data_time: 0.0025  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 08:34:03] d2.utils.events INFO:  eta: 14:30:10  iter: 6399  total_loss: 90.26  loss_ce: 2.587  loss_mask: 1.729  loss_dice: 4.397  loss_ce_0: 2.528  loss_mask_0: 1.214  loss_dice_0: 3.927  loss_ce_1: 2.526  loss_mask_1: 1.251  loss_dice_1: 3.896  loss_ce_2: 2.419  loss_mask_2: 1.565  loss_dice_2: 3.963  loss_ce_3: 2.43  loss_mask_3: 1.517  loss_dice_3: 4.015  loss_ce_4: 2.452  loss_mask_4: 1.5  loss_dice_4: 4.224  loss_ce_5: 2.544  loss_mask_5: 1.467  loss_dice_5: 4.134  loss_ce_6: 2.346  loss_mask_6: 1.484  loss_dice_6: 4.093  loss_ce_7: 2.435  loss_mask_7: 1.59  loss_dice_7: 4.147  loss_ce_8: 2.618  loss_mask_8: 1.806  loss_dice_8: 4.288  loss_mars: 0.5924    time: 5.6830  last_time: 6.2308  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 08:35:58] d2.utils.events INFO:  eta: 14:28:17  iter: 6419  total_loss: 95.1  loss_ce: 3.596  loss_mask: 1.928  loss_dice: 4.293  loss_ce_0: 3.179  loss_mask_0: 1.227  loss_dice_0: 3.936  loss_ce_1: 3.524  loss_mask_1: 1.4  loss_dice_1: 4.097  loss_ce_2: 2.982  loss_mask_2: 1.288  loss_dice_2: 4.256  loss_ce_3: 2.922  loss_mask_3: 1.597  loss_dice_3: 4.259  loss_ce_4: 2.967  loss_mask_4: 1.332  loss_dice_4: 4.258  loss_ce_5: 3.149  loss_mask_5: 1.85  loss_dice_5: 4.217  loss_ce_6: 3.414  loss_mask_6: 1.926  loss_dice_6: 4.371  loss_ce_7: 3.395  loss_mask_7: 1.718  loss_dice_7: 4.538  loss_ce_8: 3.554  loss_mask_8: 1.737  loss_dice_8: 4.344  loss_mars: 0.2812    time: 5.6832  last_time: 6.0455  data_time: 0.0023  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 08:37:51] d2.utils.events INFO:  eta: 14:27:43  iter: 6439  total_loss: 99.85  loss_ce: 3.327  loss_mask: 1.658  loss_dice: 4.339  loss_ce_0: 3.012  loss_mask_0: 1.482  loss_dice_0: 4.362  loss_ce_1: 2.835  loss_mask_1: 1.534  loss_dice_1: 4.365  loss_ce_2: 2.741  loss_mask_2: 1.994  loss_dice_2: 4.698  loss_ce_3: 2.703  loss_mask_3: 1.632  loss_dice_3: 4.671  loss_ce_4: 2.705  loss_mask_4: 1.776  loss_dice_4: 4.602  loss_ce_5: 2.962  loss_mask_5: 1.801  loss_dice_5: 4.572  loss_ce_6: 3.041  loss_mask_6: 1.977  loss_dice_6: 4.823  loss_ce_7: 3.402  loss_mask_7: 1.67  loss_dice_7: 4.65  loss_ce_8: 3.297  loss_mask_8: 1.62  loss_dice_8: 4.565  loss_mars: 0.05523    time: 5.6830  last_time: 6.1071  data_time: 0.0024  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 08:39:46] d2.utils.events INFO:  eta: 14:30:19  iter: 6459  total_loss: 94.31  loss_ce: 3.588  loss_mask: 1.456  loss_dice: 4.43  loss_ce_0: 3.012  loss_mask_0: 1.31  loss_dice_0: 4.032  loss_ce_1: 3.25  loss_mask_1: 1.348  loss_dice_1: 4.181  loss_ce_2: 3.146  loss_mask_2: 1.424  loss_dice_2: 4.215  loss_ce_3: 3.105  loss_mask_3: 1.624  loss_dice_3: 4.121  loss_ce_4: 3.326  loss_mask_4: 1.59  loss_dice_4: 4.035  loss_ce_5: 3.482  loss_mask_5: 1.628  loss_dice_5: 4.278  loss_ce_6: 3.369  loss_mask_6: 1.562  loss_dice_6: 4.425  loss_ce_7: 3.568  loss_mask_7: 1.752  loss_dice_7: 4.395  loss_ce_8: 3.482  loss_mask_8: 1.532  loss_dice_8: 4.43  loss_mars: 0.4116    time: 5.6834  last_time: 6.2877  data_time: 0.0026  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 08:41:37] d2.utils.events INFO:  eta: 14:25:53  iter: 6479  total_loss: 101.2  loss_ce: 4.273  loss_mask: 1.052  loss_dice: 4.642  loss_ce_0: 4.068  loss_mask_0: 0.8895  loss_dice_0: 4.426  loss_ce_1: 4.343  loss_mask_1: 0.9169  loss_dice_1: 4.4  loss_ce_2: 4.36  loss_mask_2: 0.9902  loss_dice_2: 4.544  loss_ce_3: 4.102  loss_mask_3: 0.9745  loss_dice_3: 4.478  loss_ce_4: 4.148  loss_mask_4: 0.9793  loss_dice_4: 4.318  loss_ce_5: 4.317  loss_mask_5: 1.034  loss_dice_5: 4.348  loss_ce_6: 4.489  loss_mask_6: 1.038  loss_dice_6: 4.406  loss_ce_7: 4.304  loss_mask_7: 1.034  loss_dice_7: 4.624  loss_ce_8: 4.33  loss_mask_8: 1.024  loss_dice_8: 4.666  loss_mars: 0.3518    time: 5.6828  last_time: 3.8449  data_time: 0.0030  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 08:43:29] d2.utils.events INFO:  eta: 14:20:44  iter: 6499  total_loss: 87.48  loss_ce: 3.302  loss_mask: 1.489  loss_dice: 4.23  loss_ce_0: 2.943  loss_mask_0: 1.299  loss_dice_0: 4.04  loss_ce_1: 2.647  loss_mask_1: 1.553  loss_dice_1: 4.15  loss_ce_2: 2.622  loss_mask_2: 1.537  loss_dice_2: 4.268  loss_ce_3: 2.51  loss_mask_3: 1.675  loss_dice_3: 4.158  loss_ce_4: 2.679  loss_mask_4: 1.643  loss_dice_4: 4.266  loss_ce_5: 2.627  loss_mask_5: 1.724  loss_dice_5: 4.117  loss_ce_6: 2.804  loss_mask_6: 1.453  loss_dice_6: 4.172  loss_ce_7: 3.027  loss_mask_7: 1.499  loss_dice_7: 4.4  loss_ce_8: 3.097  loss_mask_8: 1.458  loss_dice_8: 4.223  loss_mars: 0.2281    time: 5.6825  last_time: 5.7414  data_time: 0.0023  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 08:45:25] d2.utils.events INFO:  eta: 14:19:51  iter: 6519  total_loss: 94.84  loss_ce: 3.226  loss_mask: 1.05  loss_dice: 4.674  loss_ce_0: 2.816  loss_mask_0: 0.8891  loss_dice_0: 4.477  loss_ce_1: 2.771  loss_mask_1: 0.8966  loss_dice_1: 4.352  loss_ce_2: 2.835  loss_mask_2: 1.217  loss_dice_2: 4.387  loss_ce_3: 2.47  loss_mask_3: 1.453  loss_dice_3: 4.378  loss_ce_4: 2.567  loss_mask_4: 1.385  loss_dice_4: 4.486  loss_ce_5: 2.725  loss_mask_5: 1.242  loss_dice_5: 4.413  loss_ce_6: 2.736  loss_mask_6: 1.256  loss_dice_6: 4.554  loss_ce_7: 2.707  loss_mask_7: 1.083  loss_dice_7: 4.609  loss_ce_8: 2.92  loss_mask_8: 1.058  loss_dice_8: 4.632  loss_mars: 0.2785    time: 5.6829  last_time: 6.1247  data_time: 0.0029  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 08:47:18] d2.utils.events INFO:  eta: 14:20:12  iter: 6539  total_loss: 89.04  loss_ce: 2.86  loss_mask: 0.6457  loss_dice: 4.761  loss_ce_0: 3.492  loss_mask_0: 0.5243  loss_dice_0: 4.525  loss_ce_1: 2.679  loss_mask_1: 0.6864  loss_dice_1: 4.463  loss_ce_2: 3.679  loss_mask_2: 0.7645  loss_dice_2: 4.547  loss_ce_3: 3.069  loss_mask_3: 0.8609  loss_dice_3: 4.514  loss_ce_4: 2.804  loss_mask_4: 0.7434  loss_dice_4: 4.471  loss_ce_5: 3.282  loss_mask_5: 0.8318  loss_dice_5: 4.483  loss_ce_6: 2.591  loss_mask_6: 0.7789  loss_dice_6: 4.594  loss_ce_7: 3.243  loss_mask_7: 0.6652  loss_dice_7: 4.541  loss_ce_8: 2.897  loss_mask_8: 0.7389  loss_dice_8: 4.657  loss_mars: 0.5844    time: 5.6829  last_time: 5.2657  data_time: 0.0031  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 08:49:14] d2.utils.events INFO:  eta: 14:21:24  iter: 6559  total_loss: 88.58  loss_ce: 3.889  loss_mask: 0.8967  loss_dice: 4.307  loss_ce_0: 3.207  loss_mask_0: 0.9441  loss_dice_0: 4.322  loss_ce_1: 3.279  loss_mask_1: 0.997  loss_dice_1: 4.187  loss_ce_2: 3.375  loss_mask_2: 0.97  loss_dice_2: 4.298  loss_ce_3: 3.22  loss_mask_3: 1.005  loss_dice_3: 4.477  loss_ce_4: 3.262  loss_mask_4: 1.052  loss_dice_4: 4.299  loss_ce_5: 3.38  loss_mask_5: 1.048  loss_dice_5: 4.164  loss_ce_6: 3.405  loss_mask_6: 1.134  loss_dice_6: 4.385  loss_ce_7: 3.466  loss_mask_7: 1.238  loss_dice_7: 4.126  loss_ce_8: 3.693  loss_mask_8: 0.9784  loss_dice_8: 4.505  loss_mars: 0.6461    time: 5.6832  last_time: 6.4474  data_time: 0.0029  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 08:51:06] d2.utils.events INFO:  eta: 14:21:58  iter: 6579  total_loss: 93.59  loss_ce: 3.439  loss_mask: 1.305  loss_dice: 4.217  loss_ce_0: 3.492  loss_mask_0: 1.292  loss_dice_0: 4.319  loss_ce_1: 3.001  loss_mask_1: 1.467  loss_dice_1: 4.401  loss_ce_2: 3.01  loss_mask_2: 1.536  loss_dice_2: 4.375  loss_ce_3: 3.066  loss_mask_3: 1.669  loss_dice_3: 4.233  loss_ce_4: 3.079  loss_mask_4: 1.497  loss_dice_4: 4.308  loss_ce_5: 3.273  loss_mask_5: 1.458  loss_dice_5: 4.361  loss_ce_6: 3.165  loss_mask_6: 1.509  loss_dice_6: 4.366  loss_ce_7: 3.169  loss_mask_7: 1.442  loss_dice_7: 4.411  loss_ce_8: 3.281  loss_mask_8: 1.406  loss_dice_8: 4.468  loss_mars: 0.502    time: 5.6830  last_time: 6.6426  data_time: 0.0034  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 08:52:54] d2.utils.events INFO:  eta: 14:13:25  iter: 6599  total_loss: 90.97  loss_ce: 3.985  loss_mask: 1.131  loss_dice: 4.439  loss_ce_0: 3.757  loss_mask_0: 0.9412  loss_dice_0: 4.303  loss_ce_1: 3.489  loss_mask_1: 1.069  loss_dice_1: 4.37  loss_ce_2: 3.598  loss_mask_2: 1.152  loss_dice_2: 4.427  loss_ce_3: 3.541  loss_mask_3: 1.316  loss_dice_3: 4.404  loss_ce_4: 3.486  loss_mask_4: 1.169  loss_dice_4: 4.48  loss_ce_5: 3.912  loss_mask_5: 1.063  loss_dice_5: 4.413  loss_ce_6: 3.657  loss_mask_6: 1.103  loss_dice_6: 4.38  loss_ce_7: 3.571  loss_mask_7: 1.097  loss_dice_7: 4.424  loss_ce_8: 4.184  loss_mask_8: 1.334  loss_dice_8: 4.5  loss_mars: 0.2298    time: 5.6818  last_time: 5.9883  data_time: 0.0032  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 08:54:52] d2.utils.events INFO:  eta: 14:12:37  iter: 6619  total_loss: 93.35  loss_ce: 3.839  loss_mask: 1.108  loss_dice: 4.268  loss_ce_0: 4.037  loss_mask_0: 1.012  loss_dice_0: 4.226  loss_ce_1: 3.417  loss_mask_1: 1.022  loss_dice_1: 4.252  loss_ce_2: 3.738  loss_mask_2: 1.026  loss_dice_2: 4.294  loss_ce_3: 3.631  loss_mask_3: 1.056  loss_dice_3: 4.384  loss_ce_4: 3.673  loss_mask_4: 0.9912  loss_dice_4: 4.242  loss_ce_5: 3.864  loss_mask_5: 1.171  loss_dice_5: 4.138  loss_ce_6: 3.41  loss_mask_6: 1.318  loss_dice_6: 4.221  loss_ce_7: 3.716  loss_mask_7: 1.247  loss_dice_7: 4.319  loss_ce_8: 3.842  loss_mask_8: 1.097  loss_dice_8: 4.245  loss_mars: 0.7471    time: 5.6826  last_time: 5.3865  data_time: 0.0032  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 08:56:49] d2.utils.events INFO:  eta: 14:10:31  iter: 6639  total_loss: 93.89  loss_ce: 3.326  loss_mask: 0.9988  loss_dice: 3.897  loss_ce_0: 3.422  loss_mask_0: 0.9579  loss_dice_0: 4.02  loss_ce_1: 2.701  loss_mask_1: 1.149  loss_dice_1: 3.804  loss_ce_2: 2.95  loss_mask_2: 1.137  loss_dice_2: 3.979  loss_ce_3: 3.031  loss_mask_3: 1.481  loss_dice_3: 3.927  loss_ce_4: 3.065  loss_mask_4: 1.343  loss_dice_4: 3.969  loss_ce_5: 3.451  loss_mask_5: 1.21  loss_dice_5: 3.975  loss_ce_6: 3.401  loss_mask_6: 1.112  loss_dice_6: 4.029  loss_ce_7: 3.358  loss_mask_7: 1.089  loss_dice_7: 3.878  loss_ce_8: 3.433  loss_mask_8: 1.128  loss_dice_8: 4.107  loss_mars: 0.6203    time: 5.6833  last_time: 5.1859  data_time: 0.0028  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 08:58:45] d2.utils.events INFO:  eta: 14:09:45  iter: 6659  total_loss: 96.14  loss_ce: 3.258  loss_mask: 0.6362  loss_dice: 4.616  loss_ce_0: 3.364  loss_mask_0: 0.6451  loss_dice_0: 4.211  loss_ce_1: 3.035  loss_mask_1: 0.6696  loss_dice_1: 4.343  loss_ce_2: 2.713  loss_mask_2: 0.7372  loss_dice_2: 4.295  loss_ce_3: 2.952  loss_mask_3: 0.6262  loss_dice_3: 4.249  loss_ce_4: 3.062  loss_mask_4: 0.6448  loss_dice_4: 4.203  loss_ce_5: 3.04  loss_mask_5: 0.6432  loss_dice_5: 4.349  loss_ce_6: 3.058  loss_mask_6: 0.6275  loss_dice_6: 4.427  loss_ce_7: 2.991  loss_mask_7: 0.6564  loss_dice_7: 4.424  loss_ce_8: 3.129  loss_mask_8: 0.719  loss_dice_8: 4.643  loss_mars: 0.4394    time: 5.6839  last_time: 6.1750  data_time: 0.0027  last_data_time: 0.0101   lr: 0.0001  max_mem: 0M
[10/19 09:00:41] d2.utils.events INFO:  eta: 14:07:52  iter: 6679  total_loss: 86.53  loss_ce: 3.201  loss_mask: 0.6079  loss_dice: 4.48  loss_ce_0: 3.303  loss_mask_0: 0.5698  loss_dice_0: 4.503  loss_ce_1: 2.885  loss_mask_1: 0.603  loss_dice_1: 4.318  loss_ce_2: 2.79  loss_mask_2: 0.6416  loss_dice_2: 4.494  loss_ce_3: 2.76  loss_mask_3: 0.637  loss_dice_3: 4.372  loss_ce_4: 2.89  loss_mask_4: 0.6555  loss_dice_4: 4.401  loss_ce_5: 2.854  loss_mask_5: 0.6056  loss_dice_5: 4.614  loss_ce_6: 3.251  loss_mask_6: 0.5189  loss_dice_6: 4.331  loss_ce_7: 3.105  loss_mask_7: 0.6357  loss_dice_7: 4.278  loss_ce_8: 3.358  loss_mask_8: 0.5073  loss_dice_8: 4.584  loss_mars: 0.3928    time: 5.6843  last_time: 5.9139  data_time: 0.0030  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 09:02:33] d2.utils.events INFO:  eta: 14:03:58  iter: 6699  total_loss: 86.19  loss_ce: 3.058  loss_mask: 1.367  loss_dice: 4.17  loss_ce_0: 3.068  loss_mask_0: 1.489  loss_dice_0: 4.007  loss_ce_1: 2.781  loss_mask_1: 1.757  loss_dice_1: 4.026  loss_ce_2: 2.67  loss_mask_2: 1.755  loss_dice_2: 4.068  loss_ce_3: 2.85  loss_mask_3: 1.38  loss_dice_3: 3.957  loss_ce_4: 2.9  loss_mask_4: 1.541  loss_dice_4: 4.063  loss_ce_5: 2.753  loss_mask_5: 1.555  loss_dice_5: 4.605  loss_ce_6: 2.733  loss_mask_6: 1.38  loss_dice_6: 4.33  loss_ce_7: 2.766  loss_mask_7: 1.365  loss_dice_7: 4.184  loss_ce_8: 3.275  loss_mask_8: 1.347  loss_dice_8: 4.172  loss_mars: 0.48    time: 5.6839  last_time: 5.1671  data_time: 0.0020  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 09:04:32] d2.utils.events INFO:  eta: 13:58:52  iter: 6719  total_loss: 100.7  loss_ce: 3.593  loss_mask: 1.937  loss_dice: 4.59  loss_ce_0: 3.566  loss_mask_0: 0.8125  loss_dice_0: 4.328  loss_ce_1: 3.607  loss_mask_1: 1.18  loss_dice_1: 4.587  loss_ce_2: 3.575  loss_mask_2: 1.273  loss_dice_2: 4.377  loss_ce_3: 3.526  loss_mask_3: 1.292  loss_dice_3: 4.347  loss_ce_4: 3.512  loss_mask_4: 1.012  loss_dice_4: 4.181  loss_ce_5: 3.262  loss_mask_5: 0.9653  loss_dice_5: 4.197  loss_ce_6: 3.493  loss_mask_6: 0.8834  loss_dice_6: 4.333  loss_ce_7: 3.721  loss_mask_7: 0.9124  loss_dice_7: 4.343  loss_ce_8: 3.557  loss_mask_8: 1.353  loss_dice_8: 4.452  loss_mars: 0.6293    time: 5.6849  last_time: 5.2091  data_time: 0.0036  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 09:06:20] d2.utils.events INFO:  eta: 13:53:06  iter: 6739  total_loss: 96.15  loss_ce: 3.708  loss_mask: 1.302  loss_dice: 4.271  loss_ce_0: 3.624  loss_mask_0: 1.164  loss_dice_0: 4.124  loss_ce_1: 3.297  loss_mask_1: 1.226  loss_dice_1: 4.505  loss_ce_2: 3.223  loss_mask_2: 1.321  loss_dice_2: 4.396  loss_ce_3: 3.243  loss_mask_3: 1.164  loss_dice_3: 4.288  loss_ce_4: 3.389  loss_mask_4: 1.337  loss_dice_4: 4.169  loss_ce_5: 3.404  loss_mask_5: 1.307  loss_dice_5: 4.041  loss_ce_6: 3.224  loss_mask_6: 1.301  loss_dice_6: 4.29  loss_ce_7: 3.433  loss_mask_7: 1.362  loss_dice_7: 4.209  loss_ce_8: 3.67  loss_mask_8: 1.251  loss_dice_8: 4.237  loss_mars: 0.5149    time: 5.6839  last_time: 5.8438  data_time: 0.0032  last_data_time: 0.0037   lr: 0.0001  max_mem: 0M
[10/19 09:08:14] d2.utils.events INFO:  eta: 13:52:06  iter: 6759  total_loss: 89.35  loss_ce: 3.288  loss_mask: 1.224  loss_dice: 4.421  loss_ce_0: 2.96  loss_mask_0: 1.343  loss_dice_0: 4.398  loss_ce_1: 2.907  loss_mask_1: 1.314  loss_dice_1: 4.415  loss_ce_2: 2.959  loss_mask_2: 1.32  loss_dice_2: 4.428  loss_ce_3: 2.981  loss_mask_3: 1.253  loss_dice_3: 4.384  loss_ce_4: 2.974  loss_mask_4: 1.446  loss_dice_4: 4.424  loss_ce_5: 3.018  loss_mask_5: 1.613  loss_dice_5: 4.335  loss_ce_6: 2.816  loss_mask_6: 1.379  loss_dice_6: 4.412  loss_ce_7: 2.854  loss_mask_7: 1.169  loss_dice_7: 4.448  loss_ce_8: 3.177  loss_mask_8: 1.193  loss_dice_8: 4.432  loss_mars: 0.4497    time: 5.6839  last_time: 5.4384  data_time: 0.0024  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 09:10:06] d2.utils.events INFO:  eta: 13:50:13  iter: 6779  total_loss: 90.46  loss_ce: 2.691  loss_mask: 1.227  loss_dice: 4.439  loss_ce_0: 2.81  loss_mask_0: 1.539  loss_dice_0: 4.445  loss_ce_1: 3.24  loss_mask_1: 1.538  loss_dice_1: 4.5  loss_ce_2: 3.158  loss_mask_2: 1.32  loss_dice_2: 4.493  loss_ce_3: 3.08  loss_mask_3: 1.418  loss_dice_3: 4.521  loss_ce_4: 3.28  loss_mask_4: 1.357  loss_dice_4: 4.479  loss_ce_5: 3.264  loss_mask_5: 1.434  loss_dice_5: 4.46  loss_ce_6: 2.849  loss_mask_6: 1.423  loss_dice_6: 4.484  loss_ce_7: 2.85  loss_mask_7: 1.344  loss_dice_7: 4.487  loss_ce_8: 2.717  loss_mask_8: 1.392  loss_dice_8: 4.439  loss_mars: 0.4277    time: 5.6836  last_time: 5.2550  data_time: 0.0029  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 09:11:55] d2.utils.events INFO:  eta: 13:44:09  iter: 6799  total_loss: 93.06  loss_ce: 3.543  loss_mask: 1.386  loss_dice: 4.589  loss_ce_0: 3.491  loss_mask_0: 1.335  loss_dice_0: 4.241  loss_ce_1: 3.945  loss_mask_1: 1.273  loss_dice_1: 4.3  loss_ce_2: 4.065  loss_mask_2: 1.278  loss_dice_2: 4.429  loss_ce_3: 3.812  loss_mask_3: 1.376  loss_dice_3: 4.253  loss_ce_4: 3.893  loss_mask_4: 1.562  loss_dice_4: 4.34  loss_ce_5: 3.879  loss_mask_5: 1.347  loss_dice_5: 4.387  loss_ce_6: 3.35  loss_mask_6: 2.034  loss_dice_6: 4.661  loss_ce_7: 3.975  loss_mask_7: 1.439  loss_dice_7: 4.511  loss_ce_8: 3.45  loss_mask_8: 1.465  loss_dice_8: 4.483  loss_mars: 0.4969    time: 5.6828  last_time: 5.9134  data_time: 0.0030  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 09:13:47] d2.utils.events INFO:  eta: 13:43:53  iter: 6819  total_loss: 87.46  loss_ce: 2.689  loss_mask: 1.268  loss_dice: 4.46  loss_ce_0: 3.261  loss_mask_0: 1.28  loss_dice_0: 4.338  loss_ce_1: 2.755  loss_mask_1: 1.317  loss_dice_1: 4.151  loss_ce_2: 2.549  loss_mask_2: 1.285  loss_dice_2: 4.403  loss_ce_3: 2.755  loss_mask_3: 1.344  loss_dice_3: 4.255  loss_ce_4: 2.767  loss_mask_4: 1.294  loss_dice_4: 4.23  loss_ce_5: 2.626  loss_mask_5: 1.465  loss_dice_5: 4.31  loss_ce_6: 2.756  loss_mask_6: 2.078  loss_dice_6: 4.317  loss_ce_7: 2.781  loss_mask_7: 1.364  loss_dice_7: 4.287  loss_ce_8: 2.621  loss_mask_8: 1.497  loss_dice_8: 4.419  loss_mars: 0.682    time: 5.6824  last_time: 5.3841  data_time: 0.0028  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 09:15:38] d2.utils.events INFO:  eta: 13:38:49  iter: 6839  total_loss: 93.47  loss_ce: 3.155  loss_mask: 1.222  loss_dice: 4.42  loss_ce_0: 2.803  loss_mask_0: 1.217  loss_dice_0: 4.323  loss_ce_1: 2.76  loss_mask_1: 1.248  loss_dice_1: 4.259  loss_ce_2: 2.742  loss_mask_2: 1.254  loss_dice_2: 4.208  loss_ce_3: 2.877  loss_mask_3: 1.428  loss_dice_3: 4.198  loss_ce_4: 2.755  loss_mask_4: 1.242  loss_dice_4: 4.108  loss_ce_5: 2.522  loss_mask_5: 1.199  loss_dice_5: 4.238  loss_ce_6: 2.491  loss_mask_6: 1.906  loss_dice_6: 4.295  loss_ce_7: 2.963  loss_mask_7: 1.651  loss_dice_7: 4.394  loss_ce_8: 2.979  loss_mask_8: 1.761  loss_dice_8: 4.261  loss_mars: 0.3325    time: 5.6818  last_time: 5.3513  data_time: 0.0026  last_data_time: 0.0016   lr: 0.0001  max_mem: 0M
[10/19 09:17:27] d2.utils.events INFO:  eta: 13:35:26  iter: 6859  total_loss: 84.11  loss_ce: 2.707  loss_mask: 1.572  loss_dice: 4.109  loss_ce_0: 2.43  loss_mask_0: 1.696  loss_dice_0: 3.997  loss_ce_1: 2.313  loss_mask_1: 1.402  loss_dice_1: 3.864  loss_ce_2: 2.247  loss_mask_2: 1.603  loss_dice_2: 3.947  loss_ce_3: 2.393  loss_mask_3: 1.422  loss_dice_3: 3.966  loss_ce_4: 2.327  loss_mask_4: 1.494  loss_dice_4: 4.084  loss_ce_5: 2.398  loss_mask_5: 1.731  loss_dice_5: 3.973  loss_ce_6: 2.516  loss_mask_6: 1.785  loss_dice_6: 4.091  loss_ce_7: 2.357  loss_mask_7: 1.767  loss_dice_7: 4  loss_ce_8: 2.583  loss_mask_8: 1.432  loss_dice_8: 3.932  loss_mars: 0.4334    time: 5.6809  last_time: 6.3729  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 09:19:16] d2.utils.events INFO:  eta: 13:33:34  iter: 6879  total_loss: 87.43  loss_ce: 2.955  loss_mask: 1.07  loss_dice: 4.547  loss_ce_0: 2.868  loss_mask_0: 0.8161  loss_dice_0: 4.475  loss_ce_1: 2.761  loss_mask_1: 0.9451  loss_dice_1: 4.407  loss_ce_2: 2.891  loss_mask_2: 1.005  loss_dice_2: 4.377  loss_ce_3: 2.806  loss_mask_3: 0.949  loss_dice_3: 4.403  loss_ce_4: 2.832  loss_mask_4: 0.9572  loss_dice_4: 4.28  loss_ce_5: 2.644  loss_mask_5: 0.9707  loss_dice_5: 4.353  loss_ce_6: 3.37  loss_mask_6: 1.042  loss_dice_6: 4.349  loss_ce_7: 2.854  loss_mask_7: 0.9915  loss_dice_7: 4.514  loss_ce_8: 2.922  loss_mask_8: 1.068  loss_dice_8: 4.644  loss_mars: 0.4021    time: 5.6801  last_time: 5.3030  data_time: 0.0025  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 09:21:05] d2.utils.events INFO:  eta: 13:31:43  iter: 6899  total_loss: 95.64  loss_ce: 3.42  loss_mask: 1.087  loss_dice: 4.699  loss_ce_0: 3.556  loss_mask_0: 0.7992  loss_dice_0: 4.67  loss_ce_1: 3.569  loss_mask_1: 0.8146  loss_dice_1: 4.623  loss_ce_2: 3.476  loss_mask_2: 0.7801  loss_dice_2: 4.655  loss_ce_3: 3.338  loss_mask_3: 0.8331  loss_dice_3: 4.57  loss_ce_4: 3.351  loss_mask_4: 0.8461  loss_dice_4: 4.635  loss_ce_5: 3.219  loss_mask_5: 0.8291  loss_dice_5: 4.629  loss_ce_6: 3.56  loss_mask_6: 0.8641  loss_dice_6: 4.541  loss_ce_7: 3.531  loss_mask_7: 0.856  loss_dice_7: 4.671  loss_ce_8: 3.432  loss_mask_8: 1.061  loss_dice_8: 4.671  loss_mars: 0.0001716    time: 5.6793  last_time: 5.0912  data_time: 0.0029  last_data_time: 0.0047   lr: 0.0001  max_mem: 0M
[10/19 09:22:56] d2.utils.events INFO:  eta: 13:29:51  iter: 6919  total_loss: 97.57  loss_ce: 3.863  loss_mask: 0.9736  loss_dice: 4.649  loss_ce_0: 3.059  loss_mask_0: 0.992  loss_dice_0: 4.685  loss_ce_1: 3.805  loss_mask_1: 1.013  loss_dice_1: 4.636  loss_ce_2: 3.506  loss_mask_2: 0.8986  loss_dice_2: 4.698  loss_ce_3: 3.607  loss_mask_3: 0.9793  loss_dice_3: 4.685  loss_ce_4: 3.552  loss_mask_4: 0.9787  loss_dice_4: 4.702  loss_ce_5: 3.288  loss_mask_5: 1.097  loss_dice_5: 4.681  loss_ce_6: 3.157  loss_mask_6: 0.9007  loss_dice_6: 4.687  loss_ce_7: 3.741  loss_mask_7: 1.008  loss_dice_7: 4.586  loss_ce_8: 3.796  loss_mask_8: 0.9411  loss_dice_8: 4.545  loss_mars: 0.4682    time: 5.6787  last_time: 5.2676  data_time: 0.0039  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 09:24:48] d2.utils.events INFO:  eta: 13:27:59  iter: 6939  total_loss: 94.77  loss_ce: 3.463  loss_mask: 1.196  loss_dice: 4.271  loss_ce_0: 3.439  loss_mask_0: 1.253  loss_dice_0: 4.565  loss_ce_1: 3.556  loss_mask_1: 1.291  loss_dice_1: 4.441  loss_ce_2: 3.478  loss_mask_2: 1.318  loss_dice_2: 4.296  loss_ce_3: 3.489  loss_mask_3: 1.122  loss_dice_3: 4.381  loss_ce_4: 3.576  loss_mask_4: 1.162  loss_dice_4: 4.38  loss_ce_5: 3.271  loss_mask_5: 1.28  loss_dice_5: 4.467  loss_ce_6: 3.43  loss_mask_6: 1.119  loss_dice_6: 4.449  loss_ce_7: 3.74  loss_mask_7: 1.097  loss_dice_7: 4.443  loss_ce_8: 3.439  loss_mask_8: 1.19  loss_dice_8: 4.277  loss_mars: 0.5773    time: 5.6784  last_time: 5.1057  data_time: 0.0029  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/19 09:26:41] d2.utils.events INFO:  eta: 13:28:18  iter: 6959  total_loss: 96.35  loss_ce: 4.551  loss_mask: 0.8696  loss_dice: 4.616  loss_ce_0: 4.83  loss_mask_0: 0.7763  loss_dice_0: 4.506  loss_ce_1: 4.476  loss_mask_1: 0.8274  loss_dice_1: 4.523  loss_ce_2: 4.252  loss_mask_2: 0.8701  loss_dice_2: 4.473  loss_ce_3: 4.32  loss_mask_3: 0.9055  loss_dice_3: 4.47  loss_ce_4: 4.155  loss_mask_4: 0.8799  loss_dice_4: 4.473  loss_ce_5: 4.375  loss_mask_5: 0.9363  loss_dice_5: 4.588  loss_ce_6: 4.504  loss_mask_6: 0.8042  loss_dice_6: 4.617  loss_ce_7: 4.574  loss_mask_7: 0.9313  loss_dice_7: 4.577  loss_ce_8: 4.53  loss_mask_8: 0.9239  loss_dice_8: 4.471  loss_mars: 0.3906    time: 5.6783  last_time: 5.7834  data_time: 0.0040  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 09:28:32] d2.utils.events INFO:  eta: 13:30:01  iter: 6979  total_loss: 93.35  loss_ce: 4.137  loss_mask: 0.798  loss_dice: 4.287  loss_ce_0: 4.379  loss_mask_0: 0.7285  loss_dice_0: 4.359  loss_ce_1: 4.008  loss_mask_1: 0.778  loss_dice_1: 4.255  loss_ce_2: 3.763  loss_mask_2: 0.7783  loss_dice_2: 4.32  loss_ce_3: 3.984  loss_mask_3: 0.8275  loss_dice_3: 4.277  loss_ce_4: 3.754  loss_mask_4: 0.8231  loss_dice_4: 4.234  loss_ce_5: 3.814  loss_mask_5: 0.7661  loss_dice_5: 4.256  loss_ce_6: 3.609  loss_mask_6: 0.8707  loss_dice_6: 4.296  loss_ce_7: 3.96  loss_mask_7: 0.7858  loss_dice_7: 4.271  loss_ce_8: 3.737  loss_mask_8: 0.7847  loss_dice_8: 4.26  loss_mars: 0.712    time: 5.6779  last_time: 4.8879  data_time: 0.0025  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 09:30:23] d2.utils.events INFO:  eta: 13:28:08  iter: 6999  total_loss: 97.27  loss_ce: 4.134  loss_mask: 0.8806  loss_dice: 4.582  loss_ce_0: 4.007  loss_mask_0: 0.9784  loss_dice_0: 4.56  loss_ce_1: 3.845  loss_mask_1: 0.8892  loss_dice_1: 4.569  loss_ce_2: 3.906  loss_mask_2: 1.039  loss_dice_2: 4.602  loss_ce_3: 4.162  loss_mask_3: 0.9046  loss_dice_3: 4.575  loss_ce_4: 4.037  loss_mask_4: 0.9675  loss_dice_4: 4.571  loss_ce_5: 3.581  loss_mask_5: 0.9132  loss_dice_5: 4.599  loss_ce_6: 3.671  loss_mask_6: 0.9703  loss_dice_6: 4.68  loss_ce_7: 3.756  loss_mask_7: 0.9193  loss_dice_7: 4.579  loss_ce_8: 3.989  loss_mask_8: 1.424  loss_dice_8: 4.667  loss_mars: 0.4342    time: 5.6773  last_time: 5.2985  data_time: 0.0026  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 09:32:14] d2.utils.events INFO:  eta: 13:23:28  iter: 7019  total_loss: 91.6  loss_ce: 3.026  loss_mask: 1.505  loss_dice: 4.547  loss_ce_0: 2.659  loss_mask_0: 1.133  loss_dice_0: 4.405  loss_ce_1: 2.737  loss_mask_1: 1.375  loss_dice_1: 4.47  loss_ce_2: 2.886  loss_mask_2: 1.565  loss_dice_2: 4.479  loss_ce_3: 2.822  loss_mask_3: 1.613  loss_dice_3: 4.484  loss_ce_4: 2.808  loss_mask_4: 1.532  loss_dice_4: 4.37  loss_ce_5: 2.676  loss_mask_5: 1.51  loss_dice_5: 4.354  loss_ce_6: 2.581  loss_mask_6: 1.737  loss_dice_6: 4.578  loss_ce_7: 2.843  loss_mask_7: 1.819  loss_dice_7: 4.385  loss_ce_8: 2.75  loss_mask_8: 1.59  loss_dice_8: 4.408  loss_mars: 0.4221    time: 5.6769  last_time: 5.2844  data_time: 0.0025  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 09:34:03] d2.utils.events INFO:  eta: 13:21:59  iter: 7039  total_loss: 93.13  loss_ce: 3.911  loss_mask: 0.6294  loss_dice: 4.505  loss_ce_0: 3.423  loss_mask_0: 0.7368  loss_dice_0: 4.592  loss_ce_1: 3.405  loss_mask_1: 0.6959  loss_dice_1: 4.59  loss_ce_2: 3.419  loss_mask_2: 0.6756  loss_dice_2: 4.576  loss_ce_3: 3.482  loss_mask_3: 0.7883  loss_dice_3: 4.562  loss_ce_4: 3.479  loss_mask_4: 0.6948  loss_dice_4: 4.571  loss_ce_5: 3.283  loss_mask_5: 0.7933  loss_dice_5: 4.546  loss_ce_6: 3.162  loss_mask_6: 0.7406  loss_dice_6: 4.526  loss_ce_7: 3.33  loss_mask_7: 0.9452  loss_dice_7: 4.583  loss_ce_8: 3.579  loss_mask_8: 0.7553  loss_dice_8: 4.531  loss_mars: 0.4174    time: 5.6761  last_time: 5.1275  data_time: 0.0036  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 09:35:57] d2.utils.events INFO:  eta: 13:22:31  iter: 7059  total_loss: 93.24  loss_ce: 3.984  loss_mask: 0.875  loss_dice: 4.377  loss_ce_0: 3.877  loss_mask_0: 0.8206  loss_dice_0: 4.476  loss_ce_1: 3.719  loss_mask_1: 0.8583  loss_dice_1: 4.476  loss_ce_2: 3.68  loss_mask_2: 0.9653  loss_dice_2: 4.452  loss_ce_3: 3.61  loss_mask_3: 0.9529  loss_dice_3: 4.448  loss_ce_4: 3.505  loss_mask_4: 0.9741  loss_dice_4: 4.445  loss_ce_5: 3.754  loss_mask_5: 0.9923  loss_dice_5: 4.406  loss_ce_6: 3.698  loss_mask_6: 1.003  loss_dice_6: 4.418  loss_ce_7: 3.762  loss_mask_7: 1.01  loss_dice_7: 4.423  loss_ce_8: 3.605  loss_mask_8: 0.948  loss_dice_8: 4.434  loss_mars: 0.7565    time: 5.6761  last_time: 5.2880  data_time: 0.0030  last_data_time: 0.0039   lr: 0.0001  max_mem: 0M
[10/19 09:37:54] d2.utils.events INFO:  eta: 13:25:05  iter: 7079  total_loss: 91.13  loss_ce: 3.192  loss_mask: 1.031  loss_dice: 4.353  loss_ce_0: 3.108  loss_mask_0: 0.998  loss_dice_0: 4.364  loss_ce_1: 2.842  loss_mask_1: 0.9893  loss_dice_1: 4.51  loss_ce_2: 3.091  loss_mask_2: 1.04  loss_dice_2: 4.491  loss_ce_3: 3.129  loss_mask_3: 1.002  loss_dice_3: 4.55  loss_ce_4: 2.824  loss_mask_4: 0.991  loss_dice_4: 4.502  loss_ce_5: 3.144  loss_mask_5: 1.006  loss_dice_5: 4.379  loss_ce_6: 3.242  loss_mask_6: 0.9811  loss_dice_6: 4.56  loss_ce_7: 3.099  loss_mask_7: 1.052  loss_dice_7: 4.549  loss_ce_8: 3.008  loss_mask_8: 0.9208  loss_dice_8: 4.5  loss_mars: 0.7163    time: 5.6768  last_time: 5.3546  data_time: 0.0026  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 09:39:53] d2.utils.events INFO:  eta: 13:25:04  iter: 7099  total_loss: 91.23  loss_ce: 3.77  loss_mask: 1.382  loss_dice: 4.562  loss_ce_0: 3.989  loss_mask_0: 1.042  loss_dice_0: 4.45  loss_ce_1: 3.537  loss_mask_1: 1.158  loss_dice_1: 4.371  loss_ce_2: 3.508  loss_mask_2: 1.114  loss_dice_2: 4.382  loss_ce_3: 3.598  loss_mask_3: 1.059  loss_dice_3: 4.352  loss_ce_4: 3.583  loss_mask_4: 1.23  loss_dice_4: 4.378  loss_ce_5: 3.696  loss_mask_5: 1.134  loss_dice_5: 4.302  loss_ce_6: 4.021  loss_mask_6: 1.341  loss_dice_6: 4.403  loss_ce_7: 3.925  loss_mask_7: 1.476  loss_dice_7: 4.54  loss_ce_8: 3.999  loss_mask_8: 1.41  loss_dice_8: 4.436  loss_mars: 0.9004    time: 5.6778  last_time: 5.2804  data_time: 0.0037  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 09:41:40] d2.utils.events INFO:  eta: 13:15:15  iter: 7119  total_loss: 92.12  loss_ce: 3.165  loss_mask: 1.553  loss_dice: 4.023  loss_ce_0: 2.888  loss_mask_0: 1.662  loss_dice_0: 3.929  loss_ce_1: 2.992  loss_mask_1: 1.63  loss_dice_1: 3.902  loss_ce_2: 3.259  loss_mask_2: 1.584  loss_dice_2: 4.024  loss_ce_3: 3.372  loss_mask_3: 1.644  loss_dice_3: 3.845  loss_ce_4: 3.325  loss_mask_4: 1.628  loss_dice_4: 3.926  loss_ce_5: 3.205  loss_mask_5: 1.52  loss_dice_5: 4.002  loss_ce_6: 3.016  loss_mask_6: 1.644  loss_dice_6: 3.962  loss_ce_7: 3.125  loss_mask_7: 1.547  loss_dice_7: 3.958  loss_ce_8: 3.031  loss_mask_8: 1.57  loss_dice_8: 4.042  loss_mars: 0.8855    time: 5.6766  last_time: 5.3258  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 09:43:29] d2.utils.events INFO:  eta: 13:12:16  iter: 7139  total_loss: 93.27  loss_ce: 3.884  loss_mask: 0.7776  loss_dice: 4.525  loss_ce_0: 3.933  loss_mask_0: 0.7339  loss_dice_0: 4.533  loss_ce_1: 3.459  loss_mask_1: 0.748  loss_dice_1: 4.584  loss_ce_2: 3.447  loss_mask_2: 0.7856  loss_dice_2: 4.505  loss_ce_3: 3.606  loss_mask_3: 0.7769  loss_dice_3: 4.411  loss_ce_4: 3.63  loss_mask_4: 0.7657  loss_dice_4: 4.486  loss_ce_5: 3.559  loss_mask_5: 0.7626  loss_dice_5: 4.494  loss_ce_6: 3.592  loss_mask_6: 0.7533  loss_dice_6: 4.484  loss_ce_7: 3.732  loss_mask_7: 0.7489  loss_dice_7: 4.471  loss_ce_8: 3.769  loss_mask_8: 0.7628  loss_dice_8: 4.519  loss_mars: 0.5153    time: 5.6758  last_time: 3.7945  data_time: 0.0037  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 09:45:23] d2.utils.events INFO:  eta: 13:11:30  iter: 7159  total_loss: 90.02  loss_ce: 2.626  loss_mask: 0.8382  loss_dice: 4.241  loss_ce_0: 2.529  loss_mask_0: 0.7542  loss_dice_0: 4.251  loss_ce_1: 2.768  loss_mask_1: 0.9102  loss_dice_1: 4.266  loss_ce_2: 3.025  loss_mask_2: 0.8304  loss_dice_2: 4.094  loss_ce_3: 2.894  loss_mask_3: 0.8167  loss_dice_3: 4.069  loss_ce_4: 2.845  loss_mask_4: 0.9348  loss_dice_4: 4.177  loss_ce_5: 2.723  loss_mask_5: 0.871  loss_dice_5: 4.22  loss_ce_6: 2.584  loss_mask_6: 0.8316  loss_dice_6: 4.298  loss_ce_7: 2.729  loss_mask_7: 0.9797  loss_dice_7: 4.186  loss_ce_8: 2.643  loss_mask_8: 0.856  loss_dice_8: 4.191  loss_mars: 0.8859    time: 5.6757  last_time: 6.0426  data_time: 0.0026  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 09:47:15] d2.utils.events INFO:  eta: 13:08:54  iter: 7179  total_loss: 87.44  loss_ce: 3.345  loss_mask: 1.245  loss_dice: 4.209  loss_ce_0: 3.507  loss_mask_0: 1.034  loss_dice_0: 4.29  loss_ce_1: 3.62  loss_mask_1: 0.9877  loss_dice_1: 4.387  loss_ce_2: 3.213  loss_mask_2: 1.031  loss_dice_2: 4.271  loss_ce_3: 3.283  loss_mask_3: 0.9816  loss_dice_3: 4.272  loss_ce_4: 3.141  loss_mask_4: 1.035  loss_dice_4: 4.238  loss_ce_5: 3.019  loss_mask_5: 1.05  loss_dice_5: 4.179  loss_ce_6: 3.207  loss_mask_6: 1.075  loss_dice_6: 4.23  loss_ce_7: 3.263  loss_mask_7: 1.143  loss_dice_7: 4.371  loss_ce_8: 3.203  loss_mask_8: 1.048  loss_dice_8: 4.328  loss_mars: 0.7968    time: 5.6755  last_time: 5.2951  data_time: 0.0026  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 09:49:04] d2.utils.events INFO:  eta: 13:06:15  iter: 7199  total_loss: 79.79  loss_ce: 2.501  loss_mask: 1.417  loss_dice: 4.043  loss_ce_0: 2.684  loss_mask_0: 1.546  loss_dice_0: 3.985  loss_ce_1: 2.958  loss_mask_1: 1.478  loss_dice_1: 3.777  loss_ce_2: 2.965  loss_mask_2: 1.508  loss_dice_2: 3.932  loss_ce_3: 2.783  loss_mask_3: 1.371  loss_dice_3: 3.777  loss_ce_4: 2.47  loss_mask_4: 1.398  loss_dice_4: 4.029  loss_ce_5: 2.32  loss_mask_5: 1.432  loss_dice_5: 4.01  loss_ce_6: 2.442  loss_mask_6: 1.488  loss_dice_6: 4.105  loss_ce_7: 2.378  loss_mask_7: 1.462  loss_dice_7: 3.977  loss_ce_8: 2.372  loss_mask_8: 1.495  loss_dice_8: 3.884  loss_mars: 0.5248    time: 5.6748  last_time: 5.1294  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 09:50:56] d2.utils.events INFO:  eta: 13:03:25  iter: 7219  total_loss: 86.57  loss_ce: 2.603  loss_mask: 1.031  loss_dice: 4.5  loss_ce_0: 2.703  loss_mask_0: 0.9665  loss_dice_0: 4.4  loss_ce_1: 2.947  loss_mask_1: 0.9571  loss_dice_1: 4.385  loss_ce_2: 3.159  loss_mask_2: 0.9729  loss_dice_2: 4.347  loss_ce_3: 3.002  loss_mask_3: 0.973  loss_dice_3: 4.281  loss_ce_4: 3.027  loss_mask_4: 1.039  loss_dice_4: 4.455  loss_ce_5: 2.41  loss_mask_5: 1.092  loss_dice_5: 4.41  loss_ce_6: 2.901  loss_mask_6: 1.035  loss_dice_6: 4.151  loss_ce_7: 2.622  loss_mask_7: 1.074  loss_dice_7: 4.422  loss_ce_8: 2.352  loss_mask_8: 1.116  loss_dice_8: 4.366  loss_mars: 0.6351    time: 5.6744  last_time: 5.1276  data_time: 0.0028  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 09:52:42] d2.utils.events INFO:  eta: 13:00:25  iter: 7239  total_loss: 92.18  loss_ce: 2.718  loss_mask: 0.8925  loss_dice: 4.509  loss_ce_0: 3.11  loss_mask_0: 1.058  loss_dice_0: 4.577  loss_ce_1: 3.188  loss_mask_1: 0.8517  loss_dice_1: 4.621  loss_ce_2: 3.283  loss_mask_2: 1.05  loss_dice_2: 4.587  loss_ce_3: 3.171  loss_mask_3: 1.1  loss_dice_3: 4.612  loss_ce_4: 3.345  loss_mask_4: 0.9766  loss_dice_4: 4.649  loss_ce_5: 2.937  loss_mask_5: 0.9481  loss_dice_5: 4.617  loss_ce_6: 2.859  loss_mask_6: 1.086  loss_dice_6: 4.594  loss_ce_7: 2.884  loss_mask_7: 0.969  loss_dice_7: 4.597  loss_ce_8: 2.836  loss_mask_8: 1.004  loss_dice_8: 4.572  loss_mars: 0.1092    time: 5.6731  last_time: 8.0963  data_time: 0.0026  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 09:54:27] d2.utils.events INFO:  eta: 12:56:23  iter: 7259  total_loss: 89.02  loss_ce: 3.476  loss_mask: 1.229  loss_dice: 4.334  loss_ce_0: 3.732  loss_mask_0: 1.122  loss_dice_0: 4.44  loss_ce_1: 3.461  loss_mask_1: 1.08  loss_dice_1: 4.429  loss_ce_2: 3.398  loss_mask_2: 0.9556  loss_dice_2: 4.521  loss_ce_3: 3.455  loss_mask_3: 0.9507  loss_dice_3: 4.498  loss_ce_4: 3.332  loss_mask_4: 0.9843  loss_dice_4: 4.191  loss_ce_5: 3.593  loss_mask_5: 0.9783  loss_dice_5: 4.308  loss_ce_6: 3.344  loss_mask_6: 1.137  loss_dice_6: 4.168  loss_ce_7: 3.522  loss_mask_7: 1.173  loss_dice_7: 4.366  loss_ce_8: 3.453  loss_mask_8: 1.258  loss_dice_8: 4.487  loss_mars: 0.3302    time: 5.6717  last_time: 6.2691  data_time: 0.0029  last_data_time: 0.0056   lr: 0.0001  max_mem: 0M
[10/19 09:56:18] d2.utils.events INFO:  eta: 12:53:31  iter: 7279  total_loss: 88.07  loss_ce: 2.913  loss_mask: 1.376  loss_dice: 3.716  loss_ce_0: 2.993  loss_mask_0: 1.461  loss_dice_0: 4.035  loss_ce_1: 2.824  loss_mask_1: 1.596  loss_dice_1: 3.935  loss_ce_2: 2.799  loss_mask_2: 1.639  loss_dice_2: 3.85  loss_ce_3: 2.837  loss_mask_3: 1.444  loss_dice_3: 3.853  loss_ce_4: 2.574  loss_mask_4: 1.421  loss_dice_4: 3.925  loss_ce_5: 2.664  loss_mask_5: 1.49  loss_dice_5: 4.065  loss_ce_6: 2.503  loss_mask_6: 1.49  loss_dice_6: 4.055  loss_ce_7: 2.745  loss_mask_7: 1.495  loss_dice_7: 3.887  loss_ce_8: 2.728  loss_mask_8: 1.433  loss_dice_8: 4.086  loss_mars: 0.742    time: 5.6713  last_time: 6.0476  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 09:58:09] d2.utils.events INFO:  eta: 12:50:26  iter: 7299  total_loss: 95.1  loss_ce: 4.224  loss_mask: 1.074  loss_dice: 4.503  loss_ce_0: 3.637  loss_mask_0: 1.028  loss_dice_0: 4.447  loss_ce_1: 3.637  loss_mask_1: 1.013  loss_dice_1: 4.445  loss_ce_2: 3.814  loss_mask_2: 1.011  loss_dice_2: 4.434  loss_ce_3: 3.571  loss_mask_3: 0.9312  loss_dice_3: 4.46  loss_ce_4: 3.427  loss_mask_4: 0.981  loss_dice_4: 4.494  loss_ce_5: 3.347  loss_mask_5: 1.163  loss_dice_5: 4.73  loss_ce_6: 3.343  loss_mask_6: 1.233  loss_dice_6: 4.619  loss_ce_7: 4.134  loss_mask_7: 0.9794  loss_dice_7: 4.42  loss_ce_8: 3.869  loss_mask_8: 0.9832  loss_dice_8: 4.572  loss_mars: 0.6458    time: 5.6708  last_time: 5.9750  data_time: 0.0029  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 10:00:07] d2.utils.events INFO:  eta: 12:48:35  iter: 7319  total_loss: 98.28  loss_ce: 3.755  loss_mask: 1.522  loss_dice: 3.827  loss_ce_0: 3.231  loss_mask_0: 1.473  loss_dice_0: 4.03  loss_ce_1: 3.346  loss_mask_1: 1.493  loss_dice_1: 3.839  loss_ce_2: 3.492  loss_mask_2: 1.487  loss_dice_2: 3.859  loss_ce_3: 3.327  loss_mask_3: 1.486  loss_dice_3: 3.687  loss_ce_4: 3.324  loss_mask_4: 1.703  loss_dice_4: 3.896  loss_ce_5: 3.425  loss_mask_5: 1.947  loss_dice_5: 3.834  loss_ce_6: 3.402  loss_mask_6: 1.831  loss_dice_6: 3.894  loss_ce_7: 3.561  loss_mask_7: 1.702  loss_dice_7: 3.814  loss_ce_8: 3.964  loss_mask_8: 1.527  loss_dice_8: 3.839  loss_mars: 0.597    time: 5.6716  last_time: 5.3284  data_time: 0.0031  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/19 10:01:55] d2.utils.events INFO:  eta: 12:46:01  iter: 7339  total_loss: 89.77  loss_ce: 3.305  loss_mask: 1.642  loss_dice: 4.147  loss_ce_0: 3.198  loss_mask_0: 1.129  loss_dice_0: 4.237  loss_ce_1: 2.985  loss_mask_1: 1.197  loss_dice_1: 4.306  loss_ce_2: 2.959  loss_mask_2: 1.213  loss_dice_2: 4.369  loss_ce_3: 3.108  loss_mask_3: 1.199  loss_dice_3: 4.106  loss_ce_4: 2.671  loss_mask_4: 1.103  loss_dice_4: 4.188  loss_ce_5: 2.677  loss_mask_5: 1.417  loss_dice_5: 4.357  loss_ce_6: 2.778  loss_mask_6: 1.27  loss_dice_6: 4.006  loss_ce_7: 2.808  loss_mask_7: 1.513  loss_dice_7: 4.292  loss_ce_8: 2.909  loss_mask_8: 1.372  loss_dice_8: 4.223  loss_mars: 0.2717    time: 5.6707  last_time: 5.7809  data_time: 0.0022  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 10:03:56] d2.utils.events INFO:  eta: 12:43:33  iter: 7359  total_loss: 95.41  loss_ce: 4.034  loss_mask: 1.06  loss_dice: 4.69  loss_ce_0: 4.499  loss_mask_0: 0.7707  loss_dice_0: 4.698  loss_ce_1: 3.964  loss_mask_1: 0.7651  loss_dice_1: 4.469  loss_ce_2: 3.963  loss_mask_2: 0.7592  loss_dice_2: 4.596  loss_ce_3: 4.077  loss_mask_3: 0.6937  loss_dice_3: 4.581  loss_ce_4: 3.928  loss_mask_4: 0.7511  loss_dice_4: 4.527  loss_ce_5: 3.795  loss_mask_5: 0.7511  loss_dice_5: 4.671  loss_ce_6: 3.738  loss_mask_6: 0.8293  loss_dice_6: 4.583  loss_ce_7: 3.753  loss_mask_7: 0.9883  loss_dice_7: 4.65  loss_ce_8: 3.928  loss_mask_8: 1.336  loss_dice_8: 4.649  loss_mars: 0.351    time: 5.6720  last_time: 5.0887  data_time: 0.0031  last_data_time: 0.0035   lr: 0.0001  max_mem: 0M
[10/19 10:05:57] d2.utils.events INFO:  eta: 12:45:15  iter: 7379  total_loss: 88.37  loss_ce: 2.921  loss_mask: 1.606  loss_dice: 4.104  loss_ce_0: 3.327  loss_mask_0: 1.836  loss_dice_0: 4.079  loss_ce_1: 3.565  loss_mask_1: 1.544  loss_dice_1: 3.928  loss_ce_2: 3.327  loss_mask_2: 1.588  loss_dice_2: 4.027  loss_ce_3: 3.231  loss_mask_3: 1.563  loss_dice_3: 4.095  loss_ce_4: 3.157  loss_mask_4: 1.515  loss_dice_4: 4.054  loss_ce_5: 3.22  loss_mask_5: 1.687  loss_dice_5: 4.068  loss_ce_6: 3.089  loss_mask_6: 1.671  loss_dice_6: 3.982  loss_ce_7: 2.793  loss_mask_7: 1.449  loss_dice_7: 4.021  loss_ce_8: 3.204  loss_mask_8: 1.529  loss_dice_8: 4.028  loss_mars: 0.7275    time: 5.6733  last_time: 5.7093  data_time: 0.0029  last_data_time: 0.0035   lr: 0.0001  max_mem: 0M
[10/19 10:08:02] d2.utils.events INFO:  eta: 12:41:44  iter: 7399  total_loss: 97.55  loss_ce: 3.354  loss_mask: 1.011  loss_dice: 4.385  loss_ce_0: 3.999  loss_mask_0: 1.107  loss_dice_0: 4.436  loss_ce_1: 3.406  loss_mask_1: 1.036  loss_dice_1: 4.405  loss_ce_2: 3.52  loss_mask_2: 1.141  loss_dice_2: 4.333  loss_ce_3: 3.659  loss_mask_3: 1.067  loss_dice_3: 4.337  loss_ce_4: 3.616  loss_mask_4: 0.9921  loss_dice_4: 4.324  loss_ce_5: 3.797  loss_mask_5: 0.9694  loss_dice_5: 4.308  loss_ce_6: 3.433  loss_mask_6: 1.029  loss_dice_6: 4.315  loss_ce_7: 3.337  loss_mask_7: 0.9637  loss_dice_7: 4.331  loss_ce_8: 3.164  loss_mask_8: 1.103  loss_dice_8: 4.315  loss_mars: 0.6936    time: 5.6753  last_time: 9.4095  data_time: 0.0024  last_data_time: 0.0034   lr: 0.0001  max_mem: 0M
[10/19 10:10:00] d2.utils.events INFO:  eta: 12:39:53  iter: 7419  total_loss: 87.07  loss_ce: 2.563  loss_mask: 1.504  loss_dice: 4.26  loss_ce_0: 3.637  loss_mask_0: 1.466  loss_dice_0: 4.225  loss_ce_1: 2.724  loss_mask_1: 1.416  loss_dice_1: 4.28  loss_ce_2: 2.701  loss_mask_2: 1.522  loss_dice_2: 4.187  loss_ce_3: 3.044  loss_mask_3: 1.5  loss_dice_3: 4.226  loss_ce_4: 3.034  loss_mask_4: 1.595  loss_dice_4: 4.147  loss_ce_5: 2.796  loss_mask_5: 1.622  loss_dice_5: 4.058  loss_ce_6: 2.597  loss_mask_6: 1.609  loss_dice_6: 4.228  loss_ce_7: 2.519  loss_mask_7: 1.705  loss_dice_7: 4.368  loss_ce_8: 2.493  loss_mask_8: 1.85  loss_dice_8: 4.249  loss_mars: 0.552    time: 5.6759  last_time: 6.3300  data_time: 0.0029  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 10:11:55] d2.utils.events INFO:  eta: 12:38:41  iter: 7439  total_loss: 79.09  loss_ce: 2.479  loss_mask: 1.773  loss_dice: 3.728  loss_ce_0: 2.432  loss_mask_0: 2.053  loss_dice_0: 3.599  loss_ce_1: 2.21  loss_mask_1: 1.607  loss_dice_1: 3.663  loss_ce_2: 2.064  loss_mask_2: 1.7  loss_dice_2: 3.711  loss_ce_3: 2.259  loss_mask_3: 1.74  loss_dice_3: 3.709  loss_ce_4: 2.161  loss_mask_4: 1.862  loss_dice_4: 3.598  loss_ce_5: 2.154  loss_mask_5: 1.664  loss_dice_5: 3.63  loss_ce_6: 2.045  loss_mask_6: 1.767  loss_dice_6: 3.647  loss_ce_7: 2.156  loss_mask_7: 1.939  loss_dice_7: 3.666  loss_ce_8: 2.245  loss_mask_8: 1.741  loss_dice_8: 3.611  loss_mars: 0.6099    time: 5.6762  last_time: 6.1162  data_time: 0.0021  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 10:13:54] d2.utils.events INFO:  eta: 12:36:11  iter: 7459  total_loss: 93.02  loss_ce: 3.883  loss_mask: 1.198  loss_dice: 4.522  loss_ce_0: 4.358  loss_mask_0: 1.167  loss_dice_0: 4.6  loss_ce_1: 3.658  loss_mask_1: 1.228  loss_dice_1: 4.595  loss_ce_2: 3.695  loss_mask_2: 1.193  loss_dice_2: 4.529  loss_ce_3: 3.585  loss_mask_3: 1.087  loss_dice_3: 4.475  loss_ce_4: 3.56  loss_mask_4: 1.088  loss_dice_4: 4.462  loss_ce_5: 3.309  loss_mask_5: 1.253  loss_dice_5: 4.673  loss_ce_6: 3.34  loss_mask_6: 1.009  loss_dice_6: 4.615  loss_ce_7: 3.438  loss_mask_7: 0.9738  loss_dice_7: 4.568  loss_ce_8: 3.675  loss_mask_8: 1.051  loss_dice_8: 4.52  loss_mars: 0.6096    time: 5.6773  last_time: 5.4705  data_time: 0.0027  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 10:15:51] d2.utils.events INFO:  eta: 12:36:37  iter: 7479  total_loss: 93.05  loss_ce: 3.601  loss_mask: 0.9627  loss_dice: 4.481  loss_ce_0: 4.34  loss_mask_0: 0.8806  loss_dice_0: 4.438  loss_ce_1: 3.463  loss_mask_1: 0.9664  loss_dice_1: 4.477  loss_ce_2: 3.723  loss_mask_2: 0.9453  loss_dice_2: 4.521  loss_ce_3: 3.927  loss_mask_3: 0.9366  loss_dice_3: 4.558  loss_ce_4: 3.615  loss_mask_4: 0.8504  loss_dice_4: 4.483  loss_ce_5: 3.359  loss_mask_5: 0.9255  loss_dice_5: 4.591  loss_ce_6: 3.516  loss_mask_6: 0.992  loss_dice_6: 4.535  loss_ce_7: 3.727  loss_mask_7: 1.095  loss_dice_7: 4.32  loss_ce_8: 3.581  loss_mask_8: 0.8782  loss_dice_8: 4.428  loss_mars: 0.5623    time: 5.6778  last_time: 5.7894  data_time: 0.0030  last_data_time: 0.0037   lr: 0.0001  max_mem: 0M
[10/19 10:17:39] d2.utils.events INFO:  eta: 12:34:46  iter: 7499  total_loss: 93.36  loss_ce: 3.723  loss_mask: 0.7895  loss_dice: 4.628  loss_ce_0: 4.61  loss_mask_0: 0.8127  loss_dice_0: 4.526  loss_ce_1: 3.95  loss_mask_1: 0.7968  loss_dice_1: 4.61  loss_ce_2: 3.666  loss_mask_2: 0.7913  loss_dice_2: 4.543  loss_ce_3: 3.634  loss_mask_3: 0.7994  loss_dice_3: 4.558  loss_ce_4: 3.705  loss_mask_4: 0.8648  loss_dice_4: 4.538  loss_ce_5: 3.509  loss_mask_5: 0.8542  loss_dice_5: 4.57  loss_ce_6: 3.667  loss_mask_6: 0.9912  loss_dice_6: 4.645  loss_ce_7: 3.821  loss_mask_7: 0.9544  loss_dice_7: 4.602  loss_ce_8: 3.914  loss_mask_8: 0.7774  loss_dice_8: 4.608  loss_mars: 0.6792    time: 5.6769  last_time: 5.3589  data_time: 0.0028  last_data_time: 0.0041   lr: 0.0001  max_mem: 0M
[10/19 10:19:34] d2.utils.events INFO:  eta: 12:31:30  iter: 7519  total_loss: 91.89  loss_ce: 2.749  loss_mask: 1.605  loss_dice: 3.784  loss_ce_0: 2.897  loss_mask_0: 1.61  loss_dice_0: 3.83  loss_ce_1: 2.767  loss_mask_1: 2.173  loss_dice_1: 4.146  loss_ce_2: 2.799  loss_mask_2: 2.157  loss_dice_2: 3.832  loss_ce_3: 2.747  loss_mask_3: 1.551  loss_dice_3: 3.836  loss_ce_4: 2.915  loss_mask_4: 1.691  loss_dice_4: 3.589  loss_ce_5: 2.688  loss_mask_5: 1.668  loss_dice_5: 3.732  loss_ce_6: 2.488  loss_mask_6: 2.168  loss_dice_6: 3.666  loss_ce_7: 2.544  loss_mask_7: 2.111  loss_dice_7: 3.889  loss_ce_8: 2.809  loss_mask_8: 1.851  loss_dice_8: 3.871  loss_mars: 0.4567    time: 5.6770  last_time: 3.8630  data_time: 0.0023  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 10:21:29] d2.utils.events INFO:  eta: 12:29:38  iter: 7539  total_loss: 89.66  loss_ce: 2.997  loss_mask: 1.462  loss_dice: 3.955  loss_ce_0: 3.107  loss_mask_0: 1.659  loss_dice_0: 4.382  loss_ce_1: 2.942  loss_mask_1: 1.362  loss_dice_1: 4.288  loss_ce_2: 2.965  loss_mask_2: 1.498  loss_dice_2: 4.177  loss_ce_3: 2.89  loss_mask_3: 1.472  loss_dice_3: 4.297  loss_ce_4: 2.774  loss_mask_4: 1.474  loss_dice_4: 4.074  loss_ce_5: 2.677  loss_mask_5: 1.472  loss_dice_5: 3.969  loss_ce_6: 3.022  loss_mask_6: 1.462  loss_dice_6: 3.995  loss_ce_7: 2.795  loss_mask_7: 1.719  loss_dice_7: 4.192  loss_ce_8: 2.878  loss_mask_8: 1.492  loss_dice_8: 4.011  loss_mars: 0.5486    time: 5.6774  last_time: 5.9308  data_time: 0.0023  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 10:23:21] d2.utils.events INFO:  eta: 12:25:14  iter: 7559  total_loss: 90.78  loss_ce: 3.655  loss_mask: 1.32  loss_dice: 4.328  loss_ce_0: 3.605  loss_mask_0: 1.104  loss_dice_0: 4.172  loss_ce_1: 3.103  loss_mask_1: 1.339  loss_dice_1: 4.437  loss_ce_2: 3.167  loss_mask_2: 1.231  loss_dice_2: 4.297  loss_ce_3: 3.365  loss_mask_3: 1.198  loss_dice_3: 4.147  loss_ce_4: 3.334  loss_mask_4: 1.197  loss_dice_4: 4.168  loss_ce_5: 3.24  loss_mask_5: 1.439  loss_dice_5: 4.303  loss_ce_6: 3.726  loss_mask_6: 1.232  loss_dice_6: 4.075  loss_ce_7: 4.212  loss_mask_7: 1.37  loss_dice_7: 4.304  loss_ce_8: 3.538  loss_mask_8: 1.31  loss_dice_8: 4.273  loss_mars: 0.5122    time: 5.6771  last_time: 5.7246  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 10:25:10] d2.utils.events INFO:  eta: 12:23:23  iter: 7579  total_loss: 96.51  loss_ce: 3.99  loss_mask: 0.9148  loss_dice: 4.375  loss_ce_0: 3.952  loss_mask_0: 0.7559  loss_dice_0: 4.313  loss_ce_1: 3.311  loss_mask_1: 0.8138  loss_dice_1: 4.269  loss_ce_2: 3.407  loss_mask_2: 0.837  loss_dice_2: 4.212  loss_ce_3: 3.398  loss_mask_3: 0.8056  loss_dice_3: 4.288  loss_ce_4: 3.436  loss_mask_4: 0.7898  loss_dice_4: 4.334  loss_ce_5: 2.924  loss_mask_5: 0.8801  loss_dice_5: 4.543  loss_ce_6: 3.31  loss_mask_6: 0.7585  loss_dice_6: 4.454  loss_ce_7: 3.32  loss_mask_7: 0.9254  loss_dice_7: 4.342  loss_ce_8: 3.839  loss_mask_8: 0.8929  loss_dice_8: 4.4  loss_mars: 0.6493    time: 5.6763  last_time: 5.9380  data_time: 0.0022  last_data_time: 0.0037   lr: 0.0001  max_mem: 0M
[10/19 10:27:08] d2.utils.events INFO:  eta: 12:24:54  iter: 7599  total_loss: 98.44  loss_ce: 4.276  loss_mask: 1.279  loss_dice: 4.375  loss_ce_0: 4.775  loss_mask_0: 1.272  loss_dice_0: 4.282  loss_ce_1: 4.263  loss_mask_1: 1.285  loss_dice_1: 4.19  loss_ce_2: 4.197  loss_mask_2: 1.292  loss_dice_2: 4.26  loss_ce_3: 4.298  loss_mask_3: 1.269  loss_dice_3: 4.271  loss_ce_4: 4.264  loss_mask_4: 1.242  loss_dice_4: 4.26  loss_ce_5: 4.094  loss_mask_5: 1.287  loss_dice_5: 4.383  loss_ce_6: 4.053  loss_mask_6: 1.304  loss_dice_6: 4.301  loss_ce_7: 4.25  loss_mask_7: 1.385  loss_dice_7: 4.35  loss_ce_8: 4.278  loss_mask_8: 1.377  loss_dice_8: 4.3  loss_mars: 0.6879    time: 5.6771  last_time: 6.1353  data_time: 0.0039  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 10:29:07] d2.utils.events INFO:  eta: 12:22:13  iter: 7619  total_loss: 95.44  loss_ce: 4.428  loss_mask: 1.102  loss_dice: 4.413  loss_ce_0: 4.737  loss_mask_0: 0.8539  loss_dice_0: 4.415  loss_ce_1: 4.1  loss_mask_1: 0.9164  loss_dice_1: 4.477  loss_ce_2: 3.917  loss_mask_2: 0.9012  loss_dice_2: 4.473  loss_ce_3: 3.627  loss_mask_3: 0.8909  loss_dice_3: 4.425  loss_ce_4: 4.084  loss_mask_4: 0.9001  loss_dice_4: 4.471  loss_ce_5: 4.035  loss_mask_5: 1.09  loss_dice_5: 4.466  loss_ce_6: 4.459  loss_mask_6: 1.032  loss_dice_6: 4.452  loss_ce_7: 4.345  loss_mask_7: 1.116  loss_dice_7: 4.473  loss_ce_8: 4.535  loss_mask_8: 1.154  loss_dice_8: 4.518  loss_mars: 0.4807    time: 5.6780  last_time: 5.4595  data_time: 0.0038  last_data_time: 0.0034   lr: 0.0001  max_mem: 0M
[10/19 10:31:04] d2.utils.events INFO:  eta: 12:20:22  iter: 7639  total_loss: 89.79  loss_ce: 3.473  loss_mask: 1.361  loss_dice: 4.232  loss_ce_0: 3.605  loss_mask_0: 1.306  loss_dice_0: 4.127  loss_ce_1: 3.24  loss_mask_1: 1.26  loss_dice_1: 3.97  loss_ce_2: 3.197  loss_mask_2: 1.456  loss_dice_2: 3.999  loss_ce_3: 3.43  loss_mask_3: 1.366  loss_dice_3: 4.026  loss_ce_4: 3.158  loss_mask_4: 1.295  loss_dice_4: 4.495  loss_ce_5: 3.042  loss_mask_5: 1.563  loss_dice_5: 4.461  loss_ce_6: 3.326  loss_mask_6: 1.454  loss_dice_6: 4.179  loss_ce_7: 3.472  loss_mask_7: 1.367  loss_dice_7: 4.247  loss_ce_8: 3.54  loss_mask_8: 1.419  loss_dice_8: 4.172  loss_mars: 0.4592    time: 5.6784  last_time: 5.1452  data_time: 0.0028  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 10:32:55] d2.utils.events INFO:  eta: 12:16:59  iter: 7659  total_loss: 99.31  loss_ce: 3.816  loss_mask: 1.03  loss_dice: 4.56  loss_ce_0: 4.634  loss_mask_0: 0.8304  loss_dice_0: 4.477  loss_ce_1: 3.954  loss_mask_1: 0.7911  loss_dice_1: 4.475  loss_ce_2: 3.9  loss_mask_2: 0.7542  loss_dice_2: 4.463  loss_ce_3: 3.924  loss_mask_3: 0.6649  loss_dice_3: 4.454  loss_ce_4: 3.806  loss_mask_4: 0.7492  loss_dice_4: 4.589  loss_ce_5: 3.827  loss_mask_5: 0.7522  loss_dice_5: 4.622  loss_ce_6: 4.016  loss_mask_6: 0.7924  loss_dice_6: 4.571  loss_ce_7: 3.686  loss_mask_7: 0.9115  loss_dice_7: 4.595  loss_ce_8: 3.704  loss_mask_8: 0.9501  loss_dice_8: 4.587  loss_mars: 0.3267    time: 5.6780  last_time: 5.5172  data_time: 0.0031  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 10:34:49] d2.utils.events INFO:  eta: 12:15:08  iter: 7679  total_loss: 96.39  loss_ce: 3.332  loss_mask: 1.091  loss_dice: 4.32  loss_ce_0: 4.506  loss_mask_0: 0.9633  loss_dice_0: 4.253  loss_ce_1: 3.77  loss_mask_1: 0.9532  loss_dice_1: 4.062  loss_ce_2: 3.826  loss_mask_2: 1.023  loss_dice_2: 4.044  loss_ce_3: 4.011  loss_mask_3: 0.976  loss_dice_3: 4.12  loss_ce_4: 3.653  loss_mask_4: 1.137  loss_dice_4: 4.026  loss_ce_5: 3.321  loss_mask_5: 1.006  loss_dice_5: 4.207  loss_ce_6: 3.483  loss_mask_6: 1.06  loss_dice_6: 4.145  loss_ce_7: 3.453  loss_mask_7: 1.28  loss_dice_7: 4.175  loss_ce_8: 3.365  loss_mask_8: 1.204  loss_dice_8: 4.155  loss_mars: 0.423    time: 5.6781  last_time: 5.3457  data_time: 0.0026  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 10:36:44] d2.utils.events INFO:  eta: 12:13:17  iter: 7699  total_loss: 89.42  loss_ce: 2.723  loss_mask: 1.24  loss_dice: 4.182  loss_ce_0: 3.512  loss_mask_0: 1.254  loss_dice_0: 4.102  loss_ce_1: 3.021  loss_mask_1: 1.329  loss_dice_1: 3.935  loss_ce_2: 3.021  loss_mask_2: 1.199  loss_dice_2: 4.023  loss_ce_3: 3.046  loss_mask_3: 1.259  loss_dice_3: 4.064  loss_ce_4: 2.941  loss_mask_4: 1.349  loss_dice_4: 4.081  loss_ce_5: 2.615  loss_mask_5: 1.302  loss_dice_5: 4.09  loss_ce_6: 2.902  loss_mask_6: 1.281  loss_dice_6: 4.062  loss_ce_7: 2.762  loss_mask_7: 1.231  loss_dice_7: 4.285  loss_ce_8: 2.83  loss_mask_8: 1.393  loss_dice_8: 4.063  loss_mars: 0.6109    time: 5.6784  last_time: 5.5732  data_time: 0.0040  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 10:38:35] d2.utils.events INFO:  eta: 12:09:25  iter: 7719  total_loss: 89.89  loss_ce: 2.678  loss_mask: 1.674  loss_dice: 4.053  loss_ce_0: 3.146  loss_mask_0: 1.48  loss_dice_0: 4.029  loss_ce_1: 2.866  loss_mask_1: 1.175  loss_dice_1: 3.907  loss_ce_2: 2.929  loss_mask_2: 1.226  loss_dice_2: 3.957  loss_ce_3: 3.033  loss_mask_3: 1.32  loss_dice_3: 3.913  loss_ce_4: 2.846  loss_mask_4: 1.442  loss_dice_4: 3.933  loss_ce_5: 2.757  loss_mask_5: 1.469  loss_dice_5: 4.123  loss_ce_6: 2.97  loss_mask_6: 1.469  loss_dice_6: 4.067  loss_ce_7: 2.791  loss_mask_7: 1.659  loss_dice_7: 4.207  loss_ce_8: 2.762  loss_mask_8: 1.433  loss_dice_8: 4.01  loss_mars: 0.5524    time: 5.6780  last_time: 6.3161  data_time: 0.0038  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 10:40:28] d2.utils.events INFO:  eta: 12:07:01  iter: 7739  total_loss: 84.55  loss_ce: 2.153  loss_mask: 1.778  loss_dice: 3.723  loss_ce_0: 2.739  loss_mask_0: 1.576  loss_dice_0: 3.774  loss_ce_1: 2.327  loss_mask_1: 1.533  loss_dice_1: 3.669  loss_ce_2: 2.346  loss_mask_2: 1.566  loss_dice_2: 3.749  loss_ce_3: 2.601  loss_mask_3: 1.719  loss_dice_3: 3.616  loss_ce_4: 2.539  loss_mask_4: 1.566  loss_dice_4: 3.679  loss_ce_5: 2.326  loss_mask_5: 1.792  loss_dice_5: 3.861  loss_ce_6: 2.385  loss_mask_6: 1.607  loss_dice_6: 3.702  loss_ce_7: 2.31  loss_mask_7: 1.702  loss_dice_7: 3.942  loss_ce_8: 2.145  loss_mask_8: 1.678  loss_dice_8: 3.922  loss_mars: 0.5686    time: 5.6778  last_time: 5.3388  data_time: 0.0030  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 10:42:23] d2.utils.events INFO:  eta: 12:05:44  iter: 7759  total_loss: 87.44  loss_ce: 2.967  loss_mask: 1.356  loss_dice: 4.314  loss_ce_0: 3.583  loss_mask_0: 1.064  loss_dice_0: 4.144  loss_ce_1: 2.979  loss_mask_1: 1.152  loss_dice_1: 4.184  loss_ce_2: 2.954  loss_mask_2: 1.107  loss_dice_2: 4.254  loss_ce_3: 3.117  loss_mask_3: 1.213  loss_dice_3: 4.189  loss_ce_4: 3.065  loss_mask_4: 1.299  loss_dice_4: 4.328  loss_ce_5: 3.287  loss_mask_5: 1.285  loss_dice_5: 4.438  loss_ce_6: 3.323  loss_mask_6: 1.117  loss_dice_6: 4.228  loss_ce_7: 2.94  loss_mask_7: 1.3  loss_dice_7: 4.223  loss_ce_8: 3.053  loss_mask_8: 1.385  loss_dice_8: 4.341  loss_mars: 0.5188    time: 5.6780  last_time: 5.8727  data_time: 0.0030  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 10:44:16] d2.utils.events INFO:  eta: 12:02:19  iter: 7779  total_loss: 81  loss_ce: 2.343  loss_mask: 1.657  loss_dice: 4.411  loss_ce_0: 3.319  loss_mask_0: 1.518  loss_dice_0: 4.291  loss_ce_1: 2.686  loss_mask_1: 1.545  loss_dice_1: 4.326  loss_ce_2: 2.449  loss_mask_2: 1.579  loss_dice_2: 4.366  loss_ce_3: 2.541  loss_mask_3: 1.386  loss_dice_3: 4.383  loss_ce_4: 2.808  loss_mask_4: 1.314  loss_dice_4: 4.229  loss_ce_5: 2.744  loss_mask_5: 1.545  loss_dice_5: 4.357  loss_ce_6: 2.539  loss_mask_6: 1.563  loss_dice_6: 4.375  loss_ce_7: 2.521  loss_mask_7: 1.658  loss_dice_7: 4.584  loss_ce_8: 2.308  loss_mask_8: 1.648  loss_dice_8: 4.363  loss_mars: 0.4756    time: 5.6780  last_time: 5.1744  data_time: 0.0031  last_data_time: 0.0039   lr: 0.0001  max_mem: 0M
[10/19 10:46:05] d2.utils.events INFO:  eta: 12:00:50  iter: 7799  total_loss: 91.76  loss_ce: 2.782  loss_mask: 0.9175  loss_dice: 3.984  loss_ce_0: 3.297  loss_mask_0: 0.7203  loss_dice_0: 4.239  loss_ce_1: 2.915  loss_mask_1: 0.877  loss_dice_1: 4.08  loss_ce_2: 2.974  loss_mask_2: 1.011  loss_dice_2: 4.484  loss_ce_3: 2.916  loss_mask_3: 0.8853  loss_dice_3: 4.229  loss_ce_4: 2.854  loss_mask_4: 0.8585  loss_dice_4: 4.008  loss_ce_5: 2.758  loss_mask_5: 0.8766  loss_dice_5: 3.943  loss_ce_6: 2.702  loss_mask_6: 0.9466  loss_dice_6: 4.189  loss_ce_7: 2.864  loss_mask_7: 1.008  loss_dice_7: 4.202  loss_ce_8: 2.85  loss_mask_8: 0.9515  loss_dice_8: 4.269  loss_mars: 0.5458    time: 5.6773  last_time: 5.1722  data_time: 0.0030  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 10:47:29] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0007814.pth
[10/19 10:47:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 10:47:29] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 10:47:30] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 10:47:30] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 10:47:59] d2.utils.events INFO:  eta: 11:59:00  iter: 7819  total_loss: 95.28  loss_ce: 3.353  loss_mask: 0.7205  loss_dice: 4.449  loss_ce_0: 3.854  loss_mask_0: 0.7975  loss_dice_0: 4.451  loss_ce_1: 3.065  loss_mask_1: 0.7645  loss_dice_1: 4.383  loss_ce_2: 3.367  loss_mask_2: 0.7031  loss_dice_2: 4.513  loss_ce_3: 3.5  loss_mask_3: 0.7188  loss_dice_3: 4.488  loss_ce_4: 3.631  loss_mask_4: 0.7654  loss_dice_4: 4.315  loss_ce_5: 3.499  loss_mask_5: 0.9147  loss_dice_5: 4.468  loss_ce_6: 3.456  loss_mask_6: 0.7898  loss_dice_6: 4.461  loss_ce_7: 3.335  loss_mask_7: 0.7323  loss_dice_7: 4.497  loss_ce_8: 3.472  loss_mask_8: 0.7405  loss_dice_8: 4.392  loss_mars: 0.5986    time: 5.6771  last_time: 3.8583  data_time: 0.0034  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 10:49:49] d2.utils.events INFO:  eta: 11:58:21  iter: 7839  total_loss: 92.45  loss_ce: 3.418  loss_mask: 1.361  loss_dice: 4.528  loss_ce_0: 3.989  loss_mask_0: 1.324  loss_dice_0: 4.525  loss_ce_1: 3.073  loss_mask_1: 1.284  loss_dice_1: 4.474  loss_ce_2: 3.236  loss_mask_2: 1.251  loss_dice_2: 4.434  loss_ce_3: 3.366  loss_mask_3: 1.142  loss_dice_3: 4.502  loss_ce_4: 3.28  loss_mask_4: 1.22  loss_dice_4: 4.528  loss_ce_5: 3.243  loss_mask_5: 1.196  loss_dice_5: 4.518  loss_ce_6: 3.398  loss_mask_6: 1.304  loss_dice_6: 4.616  loss_ce_7: 3.572  loss_mask_7: 1.313  loss_dice_7: 4.528  loss_ce_8: 3.368  loss_mask_8: 1.386  loss_dice_8: 4.592  loss_mars: 0.7265    time: 5.6765  last_time: 6.1270  data_time: 0.0025  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 10:51:44] d2.utils.events INFO:  eta: 11:57:06  iter: 7859  total_loss: 88.69  loss_ce: 3.585  loss_mask: 1.291  loss_dice: 4.194  loss_ce_0: 4.392  loss_mask_0: 1.301  loss_dice_0: 4.264  loss_ce_1: 2.949  loss_mask_1: 1.341  loss_dice_1: 4.313  loss_ce_2: 2.992  loss_mask_2: 1.331  loss_dice_2: 4.284  loss_ce_3: 2.961  loss_mask_3: 1.323  loss_dice_3: 4.189  loss_ce_4: 3.229  loss_mask_4: 1.558  loss_dice_4: 4.179  loss_ce_5: 3.035  loss_mask_5: 1.284  loss_dice_5: 4.303  loss_ce_6: 3.363  loss_mask_6: 1.324  loss_dice_6: 4.217  loss_ce_7: 3.441  loss_mask_7: 1.402  loss_dice_7: 4.336  loss_ce_8: 3.58  loss_mask_8: 1.388  loss_dice_8: 4.263  loss_mars: 0.7755    time: 5.6768  last_time: 5.8321  data_time: 0.0028  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 10:53:37] d2.utils.events INFO:  eta: 11:57:30  iter: 7879  total_loss: 90.44  loss_ce: 3.961  loss_mask: 1.041  loss_dice: 4.27  loss_ce_0: 4.026  loss_mask_0: 1.045  loss_dice_0: 4.133  loss_ce_1: 3.199  loss_mask_1: 0.9822  loss_dice_1: 4.179  loss_ce_2: 3.272  loss_mask_2: 0.9299  loss_dice_2: 4.289  loss_ce_3: 3.6  loss_mask_3: 1.04  loss_dice_3: 4.11  loss_ce_4: 3.855  loss_mask_4: 1.2  loss_dice_4: 4.03  loss_ce_5: 3.833  loss_mask_5: 1.06  loss_dice_5: 4.076  loss_ce_6: 3.911  loss_mask_6: 1.049  loss_dice_6: 4.286  loss_ce_7: 3.574  loss_mask_7: 1.12  loss_dice_7: 4.45  loss_ce_8: 4.015  loss_mask_8: 0.9994  loss_dice_8: 4.22  loss_mars: 0.697    time: 5.6768  last_time: 6.0211  data_time: 0.0025  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 10:55:30] d2.utils.events INFO:  eta: 11:56:16  iter: 7899  total_loss: 86.78  loss_ce: 3.152  loss_mask: 1.763  loss_dice: 4.165  loss_ce_0: 3.497  loss_mask_0: 1.592  loss_dice_0: 4.052  loss_ce_1: 2.627  loss_mask_1: 1.636  loss_dice_1: 3.931  loss_ce_2: 2.737  loss_mask_2: 1.464  loss_dice_2: 4.163  loss_ce_3: 2.919  loss_mask_3: 1.569  loss_dice_3: 4.068  loss_ce_4: 2.827  loss_mask_4: 1.543  loss_dice_4: 3.96  loss_ce_5: 3.004  loss_mask_5: 1.551  loss_dice_5: 4.028  loss_ce_6: 3.129  loss_mask_6: 1.639  loss_dice_6: 4.171  loss_ce_7: 3.158  loss_mask_7: 1.869  loss_dice_7: 4.319  loss_ce_8: 3.075  loss_mask_8: 1.753  loss_dice_8: 4.201  loss_mars: 0.6778    time: 5.6766  last_time: 5.9667  data_time: 0.0029  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 10:57:19] d2.utils.events INFO:  eta: 11:53:48  iter: 7919  total_loss: 94.96  loss_ce: 3.596  loss_mask: 1.225  loss_dice: 4.509  loss_ce_0: 3.29  loss_mask_0: 1.059  loss_dice_0: 4.576  loss_ce_1: 3.021  loss_mask_1: 0.9152  loss_dice_1: 4.479  loss_ce_2: 3.112  loss_mask_2: 1.101  loss_dice_2: 4.482  loss_ce_3: 3.072  loss_mask_3: 1.088  loss_dice_3: 4.504  loss_ce_4: 3.427  loss_mask_4: 0.9897  loss_dice_4: 4.363  loss_ce_5: 3.618  loss_mask_5: 1.084  loss_dice_5: 4.323  loss_ce_6: 3.182  loss_mask_6: 1.056  loss_dice_6: 4.589  loss_ce_7: 3.292  loss_mask_7: 1.104  loss_dice_7: 4.542  loss_ce_8: 3.749  loss_mask_8: 1.119  loss_dice_8: 4.635  loss_mars: 0.4765    time: 5.6760  last_time: 5.3968  data_time: 0.0027  last_data_time: 0.0036   lr: 0.0001  max_mem: 0M
[10/19 10:59:13] d2.utils.events INFO:  eta: 11:51:56  iter: 7939  total_loss: 97.05  loss_ce: 2.76  loss_mask: 1.605  loss_dice: 4.384  loss_ce_0: 2.565  loss_mask_0: 1.422  loss_dice_0: 4.273  loss_ce_1: 2.505  loss_mask_1: 1.38  loss_dice_1: 4.349  loss_ce_2: 2.984  loss_mask_2: 1.277  loss_dice_2: 4.422  loss_ce_3: 2.579  loss_mask_3: 1.487  loss_dice_3: 4.278  loss_ce_4: 2.538  loss_mask_4: 1.322  loss_dice_4: 4.473  loss_ce_5: 2.822  loss_mask_5: 1.406  loss_dice_5: 4.254  loss_ce_6: 2.569  loss_mask_6: 1.629  loss_dice_6: 4.441  loss_ce_7: 2.707  loss_mask_7: 1.609  loss_dice_7: 4.413  loss_ce_8: 2.73  loss_mask_8: 1.624  loss_dice_8: 4.518  loss_mars: 0.3906    time: 5.6760  last_time: 5.2446  data_time: 0.0028  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 11:01:04] d2.utils.events INFO:  eta: 11:49:11  iter: 7959  total_loss: 95.56  loss_ce: 4.502  loss_mask: 1.104  loss_dice: 4.607  loss_ce_0: 3.573  loss_mask_0: 0.9716  loss_dice_0: 4.541  loss_ce_1: 3.051  loss_mask_1: 0.8972  loss_dice_1: 4.524  loss_ce_2: 3.263  loss_mask_2: 0.8606  loss_dice_2: 4.485  loss_ce_3: 3.209  loss_mask_3: 0.9698  loss_dice_3: 4.654  loss_ce_4: 3.327  loss_mask_4: 0.9248  loss_dice_4: 4.467  loss_ce_5: 3.618  loss_mask_5: 1  loss_dice_5: 4.506  loss_ce_6: 4.049  loss_mask_6: 0.8366  loss_dice_6: 4.784  loss_ce_7: 4.225  loss_mask_7: 0.9707  loss_dice_7: 4.667  loss_ce_8: 4.567  loss_mask_8: 0.9401  loss_dice_8: 4.467  loss_mars: 0.4203    time: 5.6756  last_time: 4.9775  data_time: 0.0030  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 11:02:54] d2.utils.events INFO:  eta: 11:46:32  iter: 7979  total_loss: 101.3  loss_ce: 4.124  loss_mask: 1.154  loss_dice: 4.475  loss_ce_0: 3.955  loss_mask_0: 1.335  loss_dice_0: 4.649  loss_ce_1: 3.966  loss_mask_1: 1.07  loss_dice_1: 4.641  loss_ce_2: 4.498  loss_mask_2: 1.131  loss_dice_2: 4.469  loss_ce_3: 4.262  loss_mask_3: 1.194  loss_dice_3: 4.38  loss_ce_4: 4.425  loss_mask_4: 1.181  loss_dice_4: 4.481  loss_ce_5: 4.193  loss_mask_5: 1.137  loss_dice_5: 4.412  loss_ce_6: 4.387  loss_mask_6: 1.161  loss_dice_6: 4.49  loss_ce_7: 4.262  loss_mask_7: 1.491  loss_dice_7: 4.432  loss_ce_8: 4.368  loss_mask_8: 1.143  loss_dice_8: 4.583  loss_mars: 0.2795    time: 5.6750  last_time: 5.9271  data_time: 0.0038  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 11:04:48] d2.utils.events INFO:  eta: 11:46:38  iter: 7999  total_loss: 93.45  loss_ce: 3.31  loss_mask: 1.552  loss_dice: 4.298  loss_ce_0: 3.285  loss_mask_0: 1.503  loss_dice_0: 4.143  loss_ce_1: 3.617  loss_mask_1: 1.519  loss_dice_1: 4.036  loss_ce_2: 3.287  loss_mask_2: 1.532  loss_dice_2: 3.821  loss_ce_3: 3.743  loss_mask_3: 1.451  loss_dice_3: 3.731  loss_ce_4: 3.454  loss_mask_4: 1.552  loss_dice_4: 3.738  loss_ce_5: 3.206  loss_mask_5: 1.559  loss_dice_5: 3.898  loss_ce_6: 3.33  loss_mask_6: 1.825  loss_dice_6: 3.815  loss_ce_7: 3.43  loss_mask_7: 1.523  loss_dice_7: 4.168  loss_ce_8: 3.072  loss_mask_8: 1.591  loss_dice_8: 4.117  loss_mars: 0.5906    time: 5.6751  last_time: 5.8241  data_time: 0.0024  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 11:06:41] d2.utils.events INFO:  eta: 11:45:46  iter: 8019  total_loss: 90.32  loss_ce: 3.102  loss_mask: 1.649  loss_dice: 4.416  loss_ce_0: 2.821  loss_mask_0: 1.146  loss_dice_0: 3.982  loss_ce_1: 2.805  loss_mask_1: 1.298  loss_dice_1: 4.059  loss_ce_2: 3.223  loss_mask_2: 1.508  loss_dice_2: 4.123  loss_ce_3: 3.145  loss_mask_3: 1.193  loss_dice_3: 4.404  loss_ce_4: 2.849  loss_mask_4: 1.52  loss_dice_4: 4.373  loss_ce_5: 3.163  loss_mask_5: 1.569  loss_dice_5: 3.78  loss_ce_6: 3.324  loss_mask_6: 1.235  loss_dice_6: 4.063  loss_ce_7: 2.748  loss_mask_7: 1.444  loss_dice_7: 4.049  loss_ce_8: 2.995  loss_mask_8: 1.384  loss_dice_8: 4.264  loss_mars: 0.4646    time: 5.6751  last_time: 6.3206  data_time: 0.0028  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 11:08:31] d2.utils.events INFO:  eta: 11:44:21  iter: 8039  total_loss: 93.84  loss_ce: 3.625  loss_mask: 1.093  loss_dice: 4.478  loss_ce_0: 3.377  loss_mask_0: 1.028  loss_dice_0: 4.433  loss_ce_1: 3.665  loss_mask_1: 1.025  loss_dice_1: 4.392  loss_ce_2: 3.356  loss_mask_2: 1.022  loss_dice_2: 4.314  loss_ce_3: 3.544  loss_mask_3: 1.142  loss_dice_3: 4.29  loss_ce_4: 3.554  loss_mask_4: 1.08  loss_dice_4: 4.28  loss_ce_5: 3.401  loss_mask_5: 1.071  loss_dice_5: 4.282  loss_ce_6: 3.393  loss_mask_6: 1.207  loss_dice_6: 4.486  loss_ce_7: 3.572  loss_mask_7: 1.085  loss_dice_7: 4.426  loss_ce_8: 3.477  loss_mask_8: 1.062  loss_dice_8: 4.493  loss_mars: 0.7296    time: 5.6745  last_time: 5.8668  data_time: 0.0034  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 11:10:25] d2.utils.events INFO:  eta: 11:41:25  iter: 8059  total_loss: 103.5  loss_ce: 4.107  loss_mask: 1.757  loss_dice: 4.113  loss_ce_0: 3.978  loss_mask_0: 1.491  loss_dice_0: 4.205  loss_ce_1: 3.883  loss_mask_1: 1.773  loss_dice_1: 4.08  loss_ce_2: 3.855  loss_mask_2: 1.758  loss_dice_2: 4.1  loss_ce_3: 3.591  loss_mask_3: 1.603  loss_dice_3: 4.056  loss_ce_4: 4.057  loss_mask_4: 1.715  loss_dice_4: 4.077  loss_ce_5: 4  loss_mask_5: 1.854  loss_dice_5: 4.062  loss_ce_6: 4.237  loss_mask_6: 1.944  loss_dice_6: 4.272  loss_ce_7: 4.191  loss_mask_7: 1.636  loss_dice_7: 4.137  loss_ce_8: 4.06  loss_mask_8: 1.729  loss_dice_8: 4.092  loss_mars: 0.6436    time: 5.6745  last_time: 5.3556  data_time: 0.0032  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 11:12:14] d2.utils.events INFO:  eta: 11:37:18  iter: 8079  total_loss: 83.55  loss_ce: 1.822  loss_mask: 2.211  loss_dice: 4.356  loss_ce_0: 2.218  loss_mask_0: 1.791  loss_dice_0: 3.738  loss_ce_1: 2.222  loss_mask_1: 1.761  loss_dice_1: 3.817  loss_ce_2: 2.294  loss_mask_2: 1.817  loss_dice_2: 3.705  loss_ce_3: 1.975  loss_mask_3: 1.8  loss_dice_3: 3.891  loss_ce_4: 2.436  loss_mask_4: 1.919  loss_dice_4: 4.003  loss_ce_5: 2.301  loss_mask_5: 1.875  loss_dice_5: 3.803  loss_ce_6: 2.26  loss_mask_6: 1.883  loss_dice_6: 3.98  loss_ce_7: 1.981  loss_mask_7: 1.816  loss_dice_7: 3.841  loss_ce_8: 2.005  loss_mask_8: 2.534  loss_dice_8: 4.436  loss_mars: 0.4418    time: 5.6738  last_time: 6.2242  data_time: 0.0024  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 11:14:03] d2.utils.events INFO:  eta: 11:34:22  iter: 8099  total_loss: 90.51  loss_ce: 2.475  loss_mask: 1.722  loss_dice: 4.263  loss_ce_0: 2.867  loss_mask_0: 1.621  loss_dice_0: 4.239  loss_ce_1: 2.94  loss_mask_1: 1.629  loss_dice_1: 4.131  loss_ce_2: 3.003  loss_mask_2: 1.626  loss_dice_2: 4.16  loss_ce_3: 3.074  loss_mask_3: 1.684  loss_dice_3: 4.329  loss_ce_4: 3.292  loss_mask_4: 1.538  loss_dice_4: 4.158  loss_ce_5: 2.832  loss_mask_5: 1.661  loss_dice_5: 4.157  loss_ce_6: 3.259  loss_mask_6: 1.857  loss_dice_6: 4.096  loss_ce_7: 2.97  loss_mask_7: 1.861  loss_dice_7: 4.236  loss_ce_8: 2.557  loss_mask_8: 1.678  loss_dice_8: 4.265  loss_mars: 0.6867    time: 5.6732  last_time: 5.9549  data_time: 0.0033  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 11:15:51] d2.utils.events INFO:  eta: 11:32:41  iter: 8119  total_loss: 85.82  loss_ce: 2.322  loss_mask: 1.178  loss_dice: 4.355  loss_ce_0: 2.801  loss_mask_0: 0.9618  loss_dice_0: 4.214  loss_ce_1: 2.37  loss_mask_1: 1.049  loss_dice_1: 4.069  loss_ce_2: 2.512  loss_mask_2: 0.983  loss_dice_2: 3.944  loss_ce_3: 2.812  loss_mask_3: 1.023  loss_dice_3: 4.062  loss_ce_4: 2.987  loss_mask_4: 1.049  loss_dice_4: 4.052  loss_ce_5: 2.196  loss_mask_5: 1.3  loss_dice_5: 4.327  loss_ce_6: 2.46  loss_mask_6: 1.149  loss_dice_6: 4.005  loss_ce_7: 2.266  loss_mask_7: 1.179  loss_dice_7: 4.256  loss_ce_8: 2.244  loss_mask_8: 1.052  loss_dice_8: 4.365  loss_mars: 0.5021    time: 5.6724  last_time: 5.9785  data_time: 0.0031  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 11:17:43] d2.utils.events INFO:  eta: 11:33:39  iter: 8139  total_loss: 92.97  loss_ce: 3.373  loss_mask: 1.172  loss_dice: 4.486  loss_ce_0: 3.328  loss_mask_0: 1.423  loss_dice_0: 4.227  loss_ce_1: 3.123  loss_mask_1: 1.257  loss_dice_1: 4.551  loss_ce_2: 3.209  loss_mask_2: 1.14  loss_dice_2: 4.379  loss_ce_3: 3.536  loss_mask_3: 1.099  loss_dice_3: 4.342  loss_ce_4: 3.47  loss_mask_4: 1.228  loss_dice_4: 4.543  loss_ce_5: 2.836  loss_mask_5: 1.237  loss_dice_5: 4.39  loss_ce_6: 3.297  loss_mask_6: 1.211  loss_dice_6: 4.367  loss_ce_7: 3.301  loss_mask_7: 1.336  loss_dice_7: 4.488  loss_ce_8: 3.467  loss_mask_8: 1.14  loss_dice_8: 4.291  loss_mars: 0.3146    time: 5.6721  last_time: 5.9604  data_time: 0.0029  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 11:19:34] d2.utils.events INFO:  eta: 11:32:46  iter: 8159  total_loss: 101.7  loss_ce: 3.093  loss_mask: 1.335  loss_dice: 4.342  loss_ce_0: 3.347  loss_mask_0: 1.339  loss_dice_0: 4.376  loss_ce_1: 2.875  loss_mask_1: 1.34  loss_dice_1: 4.096  loss_ce_2: 3.142  loss_mask_2: 1.295  loss_dice_2: 4.551  loss_ce_3: 2.913  loss_mask_3: 1.538  loss_dice_3: 4.436  loss_ce_4: 2.977  loss_mask_4: 1.797  loss_dice_4: 4.538  loss_ce_5: 3.101  loss_mask_5: 1.471  loss_dice_5: 4.417  loss_ce_6: 3.03  loss_mask_6: 1.549  loss_dice_6: 4.327  loss_ce_7: 3.549  loss_mask_7: 1.186  loss_dice_7: 4.466  loss_ce_8: 3.129  loss_mask_8: 1.242  loss_dice_8: 4.242  loss_mars: 0.2665    time: 5.6717  last_time: 6.6067  data_time: 0.0030  last_data_time: 0.0091   lr: 0.0001  max_mem: 0M
[10/19 11:21:23] d2.utils.events INFO:  eta: 11:30:18  iter: 8179  total_loss: 95.42  loss_ce: 2.319  loss_mask: 1.048  loss_dice: 4.2  loss_ce_0: 2.559  loss_mask_0: 1.001  loss_dice_0: 4.555  loss_ce_1: 2.451  loss_mask_1: 0.9401  loss_dice_1: 4.179  loss_ce_2: 2.399  loss_mask_2: 1.075  loss_dice_2: 4.205  loss_ce_3: 2.33  loss_mask_3: 0.9919  loss_dice_3: 4.505  loss_ce_4: 2.256  loss_mask_4: 1.094  loss_dice_4: 4.4  loss_ce_5: 2.359  loss_mask_5: 1.024  loss_dice_5: 4.306  loss_ce_6: 2.376  loss_mask_6: 1.051  loss_dice_6: 4.47  loss_ce_7: 2.437  loss_mask_7: 0.9624  loss_dice_7: 4.225  loss_ce_8: 2.371  loss_mask_8: 1.017  loss_dice_8: 4.411  loss_mars: 0.2834    time: 5.6711  last_time: 3.9057  data_time: 0.0032  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 11:23:11] d2.utils.events INFO:  eta: 11:27:30  iter: 8199  total_loss: 106.8  loss_ce: 3.569  loss_mask: 1.42  loss_dice: 4.505  loss_ce_0: 3.834  loss_mask_0: 1.187  loss_dice_0: 4.603  loss_ce_1: 3.723  loss_mask_1: 1.406  loss_dice_1: 4.467  loss_ce_2: 3.607  loss_mask_2: 1.591  loss_dice_2: 4.557  loss_ce_3: 3.867  loss_mask_3: 1.413  loss_dice_3: 4.25  loss_ce_4: 3.242  loss_mask_4: 1.476  loss_dice_4: 4.57  loss_ce_5: 3.608  loss_mask_5: 1.822  loss_dice_5: 4.444  loss_ce_6: 3.76  loss_mask_6: 1.877  loss_dice_6: 4.586  loss_ce_7: 3.848  loss_mask_7: 1.623  loss_dice_7: 4.559  loss_ce_8: 3.55  loss_mask_8: 1.551  loss_dice_8: 4.509  loss_mars: 0.1411    time: 5.6702  last_time: 5.1424  data_time: 0.0036  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 11:25:05] d2.utils.events INFO:  eta: 11:26:14  iter: 8219  total_loss: 103  loss_ce: 4.666  loss_mask: 1.031  loss_dice: 4.338  loss_ce_0: 3.777  loss_mask_0: 1.07  loss_dice_0: 4.229  loss_ce_1: 3.676  loss_mask_1: 1.115  loss_dice_1: 4.249  loss_ce_2: 4.043  loss_mask_2: 1.185  loss_dice_2: 4.299  loss_ce_3: 4.32  loss_mask_3: 1.286  loss_dice_3: 4.466  loss_ce_4: 4.134  loss_mask_4: 1.352  loss_dice_4: 4.551  loss_ce_5: 4.287  loss_mask_5: 1.218  loss_dice_5: 4.129  loss_ce_6: 4.377  loss_mask_6: 1.134  loss_dice_6: 4.17  loss_ce_7: 4.353  loss_mask_7: 1.085  loss_dice_7: 4.243  loss_ce_8: 4.546  loss_mask_8: 1.044  loss_dice_8: 4.391  loss_mars: 0.3295    time: 5.6703  last_time: 5.1783  data_time: 0.0039  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 11:26:49] d2.utils.events INFO:  eta: 11:25:21  iter: 8239  total_loss: 82.66  loss_ce: 2.515  loss_mask: 1.573  loss_dice: 3.979  loss_ce_0: 2.314  loss_mask_0: 1.476  loss_dice_0: 4.05  loss_ce_1: 2.574  loss_mask_1: 1.106  loss_dice_1: 4.067  loss_ce_2: 2.559  loss_mask_2: 1.526  loss_dice_2: 4.194  loss_ce_3: 2.57  loss_mask_3: 1.299  loss_dice_3: 4.293  loss_ce_4: 2.803  loss_mask_4: 1.541  loss_dice_4: 3.821  loss_ce_5: 2.364  loss_mask_5: 1.625  loss_dice_5: 3.942  loss_ce_6: 2.389  loss_mask_6: 1.443  loss_dice_6: 4.163  loss_ce_7: 2.524  loss_mask_7: 1.585  loss_dice_7: 4.256  loss_ce_8: 2.529  loss_mask_8: 1.324  loss_dice_8: 4.001  loss_mars: 0.601    time: 5.6689  last_time: 5.9540  data_time: 0.0028  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 11:28:40] d2.utils.events INFO:  eta: 11:26:45  iter: 8259  total_loss: 98.13  loss_ce: 4.093  loss_mask: 0.8642  loss_dice: 4.37  loss_ce_0: 4.29  loss_mask_0: 0.9899  loss_dice_0: 4.499  loss_ce_1: 4.039  loss_mask_1: 1.018  loss_dice_1: 4.467  loss_ce_2: 3.815  loss_mask_2: 1.019  loss_dice_2: 4.608  loss_ce_3: 4.46  loss_mask_3: 0.9361  loss_dice_3: 4.511  loss_ce_4: 3.855  loss_mask_4: 1.135  loss_dice_4: 4.543  loss_ce_5: 3.278  loss_mask_5: 1.137  loss_dice_5: 4.525  loss_ce_6: 4.414  loss_mask_6: 0.9153  loss_dice_6: 4.402  loss_ce_7: 4.784  loss_mask_7: 0.8655  loss_dice_7: 4.377  loss_ce_8: 3.973  loss_mask_8: 0.8788  loss_dice_8: 4.619  loss_mars: 0.4016    time: 5.6686  last_time: 3.7444  data_time: 0.0040  last_data_time: 0.0085   lr: 0.0001  max_mem: 0M
[10/19 11:30:29] d2.utils.events INFO:  eta: 11:24:08  iter: 8279  total_loss: 93.1  loss_ce: 2.696  loss_mask: 1.807  loss_dice: 4.248  loss_ce_0: 2.836  loss_mask_0: 1.548  loss_dice_0: 3.854  loss_ce_1: 3.27  loss_mask_1: 1.862  loss_dice_1: 4.018  loss_ce_2: 2.663  loss_mask_2: 1.669  loss_dice_2: 4.139  loss_ce_3: 2.713  loss_mask_3: 1.76  loss_dice_3: 4.318  loss_ce_4: 2.686  loss_mask_4: 1.538  loss_dice_4: 4.164  loss_ce_5: 2.72  loss_mask_5: 2.693  loss_dice_5: 4.478  loss_ce_6: 2.943  loss_mask_6: 2.246  loss_dice_6: 4.24  loss_ce_7: 2.71  loss_mask_7: 2.322  loss_dice_7: 4.146  loss_ce_8: 2.828  loss_mask_8: 1.683  loss_dice_8: 4.374  loss_mars: 0.4404    time: 5.6679  last_time: 6.2277  data_time: 0.0032  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 11:32:12] d2.utils.events INFO:  eta: 11:21:54  iter: 8299  total_loss: 95.1  loss_ce: 3.747  loss_mask: 1.552  loss_dice: 4.126  loss_ce_0: 3.612  loss_mask_0: 1.411  loss_dice_0: 4.175  loss_ce_1: 3.542  loss_mask_1: 1.56  loss_dice_1: 4.25  loss_ce_2: 3.577  loss_mask_2: 1.538  loss_dice_2: 4.137  loss_ce_3: 3.435  loss_mask_3: 1.554  loss_dice_3: 4.149  loss_ce_4: 3.62  loss_mask_4: 1.712  loss_dice_4: 4.139  loss_ce_5: 3.49  loss_mask_5: 2.342  loss_dice_5: 4.485  loss_ce_6: 3.507  loss_mask_6: 1.473  loss_dice_6: 4.152  loss_ce_7: 3.869  loss_mask_7: 1.548  loss_dice_7: 4.237  loss_ce_8: 3.874  loss_mask_8: 1.535  loss_dice_8: 3.975  loss_mars: 0.7169    time: 5.6664  last_time: 3.8642  data_time: 0.0029  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 11:34:02] d2.utils.events INFO:  eta: 11:18:37  iter: 8319  total_loss: 92.05  loss_ce: 3.761  loss_mask: 0.7726  loss_dice: 4.337  loss_ce_0: 4.435  loss_mask_0: 0.7677  loss_dice_0: 4.328  loss_ce_1: 4.136  loss_mask_1: 0.7202  loss_dice_1: 4.363  loss_ce_2: 4.049  loss_mask_2: 0.5702  loss_dice_2: 4.221  loss_ce_3: 3.561  loss_mask_3: 0.5672  loss_dice_3: 4.473  loss_ce_4: 3.513  loss_mask_4: 0.7302  loss_dice_4: 4.309  loss_ce_5: 3.443  loss_mask_5: 0.5589  loss_dice_5: 4.56  loss_ce_6: 3.765  loss_mask_6: 0.6709  loss_dice_6: 4.398  loss_ce_7: 3.926  loss_mask_7: 0.5823  loss_dice_7: 4.422  loss_ce_8: 3.575  loss_mask_8: 0.9137  loss_dice_8: 4.474  loss_mars: 0.5855    time: 5.6659  last_time: 5.9393  data_time: 0.0028  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 11:35:54] d2.utils.events INFO:  eta: 11:17:23  iter: 8339  total_loss: 102.4  loss_ce: 3.553  loss_mask: 1.751  loss_dice: 4.207  loss_ce_0: 4.047  loss_mask_0: 1.721  loss_dice_0: 4.191  loss_ce_1: 3.686  loss_mask_1: 1.401  loss_dice_1: 4.092  loss_ce_2: 4.015  loss_mask_2: 1.442  loss_dice_2: 4.237  loss_ce_3: 3.904  loss_mask_3: 1.497  loss_dice_3: 4.236  loss_ce_4: 4.151  loss_mask_4: 1.467  loss_dice_4: 4.186  loss_ce_5: 4.168  loss_mask_5: 1.948  loss_dice_5: 4.299  loss_ce_6: 4.316  loss_mask_6: 1.933  loss_dice_6: 4.323  loss_ce_7: 3.893  loss_mask_7: 1.765  loss_dice_7: 4.32  loss_ce_8: 3.598  loss_mask_8: 1.697  loss_dice_8: 4.265  loss_mars: 0.3859    time: 5.6656  last_time: 5.0671  data_time: 0.0026  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 11:37:47] d2.utils.events INFO:  eta: 11:16:41  iter: 8359  total_loss: 86.23  loss_ce: 2.503  loss_mask: 1.195  loss_dice: 4.32  loss_ce_0: 2.843  loss_mask_0: 1.371  loss_dice_0: 4.265  loss_ce_1: 2.879  loss_mask_1: 1.094  loss_dice_1: 4.255  loss_ce_2: 2.943  loss_mask_2: 1.308  loss_dice_2: 4.207  loss_ce_3: 2.88  loss_mask_3: 1.237  loss_dice_3: 4.202  loss_ce_4: 2.991  loss_mask_4: 1.346  loss_dice_4: 4.252  loss_ce_5: 3.16  loss_mask_5: 1.441  loss_dice_5: 4.342  loss_ce_6: 3.037  loss_mask_6: 1.24  loss_dice_6: 4.311  loss_ce_7: 2.794  loss_mask_7: 1.323  loss_dice_7: 4.222  loss_ce_8: 2.591  loss_mask_8: 1.242  loss_dice_8: 4.446  loss_mars: 0.566    time: 5.6655  last_time: 5.8020  data_time: 0.0020  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 11:39:38] d2.utils.events INFO:  eta: 11:14:42  iter: 8379  total_loss: 80.9  loss_ce: 2.353  loss_mask: 1.853  loss_dice: 4.438  loss_ce_0: 2.365  loss_mask_0: 1.792  loss_dice_0: 3.992  loss_ce_1: 2.431  loss_mask_1: 1.586  loss_dice_1: 3.65  loss_ce_2: 2.254  loss_mask_2: 1.476  loss_dice_2: 3.69  loss_ce_3: 2.279  loss_mask_3: 1.653  loss_dice_3: 3.903  loss_ce_4: 2.422  loss_mask_4: 1.562  loss_dice_4: 3.97  loss_ce_5: 2.409  loss_mask_5: 1.561  loss_dice_5: 3.818  loss_ce_6: 2.543  loss_mask_6: 1.774  loss_dice_6: 3.869  loss_ce_7: 2.385  loss_mask_7: 1.761  loss_dice_7: 4.144  loss_ce_8: 2.335  loss_mask_8: 1.725  loss_dice_8: 3.856  loss_mars: 0.6676    time: 5.6653  last_time: 5.1681  data_time: 0.0027  last_data_time: 0.0014   lr: 0.0001  max_mem: 0M
[10/19 11:41:28] d2.utils.events INFO:  eta: 11:10:31  iter: 8399  total_loss: 95.64  loss_ce: 3.795  loss_mask: 1.448  loss_dice: 4.39  loss_ce_0: 4.148  loss_mask_0: 1.309  loss_dice_0: 4.303  loss_ce_1: 3.406  loss_mask_1: 1.752  loss_dice_1: 4.43  loss_ce_2: 3.082  loss_mask_2: 1.868  loss_dice_2: 4.461  loss_ce_3: 3.116  loss_mask_3: 1.218  loss_dice_3: 4.333  loss_ce_4: 3.103  loss_mask_4: 1.564  loss_dice_4: 4.465  loss_ce_5: 3.519  loss_mask_5: 2.14  loss_dice_5: 4.348  loss_ce_6: 3.974  loss_mask_6: 1.784  loss_dice_6: 4.367  loss_ce_7: 3.806  loss_mask_7: 1.21  loss_dice_7: 4.211  loss_ce_8: 3.745  loss_mask_8: 1.176  loss_dice_8: 4.312  loss_mars: 0.4002    time: 5.6647  last_time: 5.2267  data_time: 0.0027  last_data_time: 0.0107   lr: 0.0001  max_mem: 0M
[10/19 11:43:18] d2.utils.events INFO:  eta: 11:08:39  iter: 8419  total_loss: 103.1  loss_ce: 3.95  loss_mask: 1.204  loss_dice: 4.357  loss_ce_0: 4.596  loss_mask_0: 1.426  loss_dice_0: 4.488  loss_ce_1: 3.803  loss_mask_1: 1.392  loss_dice_1: 4.234  loss_ce_2: 3.653  loss_mask_2: 1.473  loss_dice_2: 4.265  loss_ce_3: 3.588  loss_mask_3: 1.901  loss_dice_3: 4.474  loss_ce_4: 3.401  loss_mask_4: 1.457  loss_dice_4: 4.227  loss_ce_5: 3.856  loss_mask_5: 1.515  loss_dice_5: 4.291  loss_ce_6: 4.041  loss_mask_6: 1.377  loss_dice_6: 4.293  loss_ce_7: 4.633  loss_mask_7: 1.175  loss_dice_7: 4.284  loss_ce_8: 4.199  loss_mask_8: 1.196  loss_dice_8: 4.317  loss_mars: 0.5757    time: 5.6643  last_time: 5.0984  data_time: 0.0026  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 11:45:07] d2.utils.events INFO:  eta: 11:06:12  iter: 8439  total_loss: 96.36  loss_ce: 2.789  loss_mask: 1.834  loss_dice: 4.46  loss_ce_0: 2.785  loss_mask_0: 1.535  loss_dice_0: 4.459  loss_ce_1: 2.588  loss_mask_1: 1.536  loss_dice_1: 4.339  loss_ce_2: 2.666  loss_mask_2: 1.593  loss_dice_2: 4.428  loss_ce_3: 2.554  loss_mask_3: 1.573  loss_dice_3: 4.334  loss_ce_4: 2.453  loss_mask_4: 1.567  loss_dice_4: 4.331  loss_ce_5: 2.713  loss_mask_5: 1.669  loss_dice_5: 4.359  loss_ce_6: 3.412  loss_mask_6: 1.705  loss_dice_6: 4.438  loss_ce_7: 3.22  loss_mask_7: 1.621  loss_dice_7: 4.361  loss_ce_8: 2.708  loss_mask_8: 1.632  loss_dice_8: 4.477  loss_mars: 0.4071    time: 5.6637  last_time: 5.1120  data_time: 0.0026  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 11:47:04] d2.utils.events INFO:  eta: 11:04:57  iter: 8459  total_loss: 98.79  loss_ce: 3.909  loss_mask: 1.129  loss_dice: 4.808  loss_ce_0: 3.7  loss_mask_0: 0.9291  loss_dice_0: 4.557  loss_ce_1: 3.747  loss_mask_1: 0.9623  loss_dice_1: 4.572  loss_ce_2: 3.577  loss_mask_2: 0.8256  loss_dice_2: 4.435  loss_ce_3: 3.679  loss_mask_3: 0.9058  loss_dice_3: 4.389  loss_ce_4: 3.558  loss_mask_4: 1.071  loss_dice_4: 4.624  loss_ce_5: 4.17  loss_mask_5: 0.784  loss_dice_5: 4.571  loss_ce_6: 3.938  loss_mask_6: 0.8443  loss_dice_6: 4.565  loss_ce_7: 3.933  loss_mask_7: 0.9197  loss_dice_7: 4.729  loss_ce_8: 3.931  loss_mask_8: 1.299  loss_dice_8: 4.865  loss_mars: 0.3674    time: 5.6642  last_time: 5.2805  data_time: 0.0033  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 11:48:56] d2.utils.events INFO:  eta: 11:03:29  iter: 8479  total_loss: 97.3  loss_ce: 2.876  loss_mask: 1.242  loss_dice: 4.769  loss_ce_0: 2.81  loss_mask_0: 0.9889  loss_dice_0: 4.606  loss_ce_1: 3.016  loss_mask_1: 0.7965  loss_dice_1: 4.482  loss_ce_2: 2.956  loss_mask_2: 0.9201  loss_dice_2: 4.366  loss_ce_3: 3.266  loss_mask_3: 0.918  loss_dice_3: 4.391  loss_ce_4: 3.033  loss_mask_4: 0.9031  loss_dice_4: 4.461  loss_ce_5: 3.424  loss_mask_5: 0.9498  loss_dice_5: 4.623  loss_ce_6: 3.756  loss_mask_6: 1.023  loss_dice_6: 4.687  loss_ce_7: 3.81  loss_mask_7: 0.9942  loss_dice_7: 4.583  loss_ce_8: 2.992  loss_mask_8: 1.678  loss_dice_8: 4.61  loss_mars: 0.3464    time: 5.6641  last_time: 6.5039  data_time: 0.0032  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/19 11:50:50] d2.utils.events INFO:  eta: 11:05:58  iter: 8499  total_loss: 90.35  loss_ce: 2.797  loss_mask: 1.79  loss_dice: 4.799  loss_ce_0: 2.38  loss_mask_0: 1.564  loss_dice_0: 4.299  loss_ce_1: 2.724  loss_mask_1: 1.591  loss_dice_1: 4.171  loss_ce_2: 2.481  loss_mask_2: 1.754  loss_dice_2: 4.297  loss_ce_3: 2.433  loss_mask_3: 1.646  loss_dice_3: 4.217  loss_ce_4: 2.423  loss_mask_4: 1.642  loss_dice_4: 4.44  loss_ce_5: 2.583  loss_mask_5: 1.623  loss_dice_5: 4.25  loss_ce_6: 2.438  loss_mask_6: 1.624  loss_dice_6: 4.357  loss_ce_7: 2.77  loss_mask_7: 1.728  loss_dice_7: 4.309  loss_ce_8: 2.589  loss_mask_8: 2.208  loss_dice_8: 4.586  loss_mars: 0.5569    time: 5.6642  last_time: 6.0496  data_time: 0.0035  last_data_time: 0.0095   lr: 0.0001  max_mem: 0M
[10/19 11:52:38] d2.utils.events INFO:  eta: 11:00:34  iter: 8519  total_loss: 92.07  loss_ce: 2.769  loss_mask: 2.005  loss_dice: 4.013  loss_ce_0: 2.521  loss_mask_0: 1.874  loss_dice_0: 4.169  loss_ce_1: 2.966  loss_mask_1: 2.224  loss_dice_1: 3.995  loss_ce_2: 2.533  loss_mask_2: 2.317  loss_dice_2: 4.199  loss_ce_3: 2.724  loss_mask_3: 2.157  loss_dice_3: 4.06  loss_ce_4: 2.707  loss_mask_4: 2.103  loss_dice_4: 3.903  loss_ce_5: 2.787  loss_mask_5: 2.185  loss_dice_5: 4.022  loss_ce_6: 2.874  loss_mask_6: 2.19  loss_dice_6: 3.877  loss_ce_7: 2.826  loss_mask_7: 2.073  loss_dice_7: 4.073  loss_ce_8: 3.025  loss_mask_8: 1.991  loss_dice_8: 3.892  loss_mars: 0.7764    time: 5.6634  last_time: 4.7232  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 11:54:29] d2.utils.events INFO:  eta: 10:57:31  iter: 8539  total_loss: 89.49  loss_ce: 3.107  loss_mask: 1.501  loss_dice: 4.462  loss_ce_0: 2.8  loss_mask_0: 1.15  loss_dice_0: 4.458  loss_ce_1: 2.966  loss_mask_1: 1.015  loss_dice_1: 4.031  loss_ce_2: 2.867  loss_mask_2: 1.38  loss_dice_2: 4.368  loss_ce_3: 2.925  loss_mask_3: 1.139  loss_dice_3: 4.3  loss_ce_4: 2.928  loss_mask_4: 1.218  loss_dice_4: 4.228  loss_ce_5: 2.895  loss_mask_5: 1.088  loss_dice_5: 4.241  loss_ce_6: 2.79  loss_mask_6: 1.309  loss_dice_6: 4.192  loss_ce_7: 2.765  loss_mask_7: 1.061  loss_dice_7: 4.527  loss_ce_8: 3.175  loss_mask_8: 1.043  loss_dice_8: 4.4  loss_mars: 0.6891    time: 5.6630  last_time: 4.7720  data_time: 0.0029  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 11:56:23] d2.utils.events INFO:  eta: 10:57:32  iter: 8559  total_loss: 102.7  loss_ce: 3.71  loss_mask: 1.661  loss_dice: 4.367  loss_ce_0: 3.275  loss_mask_0: 1.204  loss_dice_0: 4.485  loss_ce_1: 3.351  loss_mask_1: 0.9693  loss_dice_1: 4.428  loss_ce_2: 3.381  loss_mask_2: 1.618  loss_dice_2: 4.513  loss_ce_3: 3.669  loss_mask_3: 1.506  loss_dice_3: 4.481  loss_ce_4: 3.593  loss_mask_4: 1.03  loss_dice_4: 4.244  loss_ce_5: 3.557  loss_mask_5: 0.9753  loss_dice_5: 4.261  loss_ce_6: 3.449  loss_mask_6: 1.309  loss_dice_6: 4.521  loss_ce_7: 3.41  loss_mask_7: 1.323  loss_dice_7: 4.338  loss_ce_8: 3.654  loss_mask_8: 1.229  loss_dice_8: 4.332  loss_mars: 0.4819    time: 5.6632  last_time: 5.9347  data_time: 0.0025  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 11:58:17] d2.utils.events INFO:  eta: 10:54:59  iter: 8579  total_loss: 87.47  loss_ce: 3.482  loss_mask: 0.7707  loss_dice: 3.749  loss_ce_0: 2.945  loss_mask_0: 0.8525  loss_dice_0: 3.991  loss_ce_1: 3.049  loss_mask_1: 0.7462  loss_dice_1: 3.895  loss_ce_2: 2.886  loss_mask_2: 0.778  loss_dice_2: 3.899  loss_ce_3: 3.315  loss_mask_3: 0.9028  loss_dice_3: 3.978  loss_ce_4: 3.274  loss_mask_4: 0.725  loss_dice_4: 3.93  loss_ce_5: 2.984  loss_mask_5: 0.7319  loss_dice_5: 4.014  loss_ce_6: 2.814  loss_mask_6: 0.7904  loss_dice_6: 3.662  loss_ce_7: 3.127  loss_mask_7: 0.6969  loss_dice_7: 3.665  loss_ce_8: 3.426  loss_mask_8: 0.9378  loss_dice_8: 3.933  loss_mars: 0.5401    time: 5.6632  last_time: 5.6413  data_time: 0.0022  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 12:00:10] d2.utils.events INFO:  eta: 10:51:22  iter: 8599  total_loss: 93.77  loss_ce: 3.34  loss_mask: 1.426  loss_dice: 4.115  loss_ce_0: 2.938  loss_mask_0: 1.575  loss_dice_0: 4.37  loss_ce_1: 2.79  loss_mask_1: 1.374  loss_dice_1: 4.084  loss_ce_2: 2.752  loss_mask_2: 1.566  loss_dice_2: 4.687  loss_ce_3: 2.737  loss_mask_3: 1.534  loss_dice_3: 4.07  loss_ce_4: 2.7  loss_mask_4: 1.503  loss_dice_4: 4.263  loss_ce_5: 2.677  loss_mask_5: 1.554  loss_dice_5: 4.192  loss_ce_6: 2.671  loss_mask_6: 1.29  loss_dice_6: 4.046  loss_ce_7: 3.042  loss_mask_7: 1.203  loss_dice_7: 4.168  loss_ce_8: 3.552  loss_mask_8: 1.277  loss_dice_8: 4.135  loss_mars: 0.6444    time: 5.6631  last_time: 5.1663  data_time: 0.0029  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 12:02:05] d2.utils.events INFO:  eta: 10:45:49  iter: 8619  total_loss: 100.9  loss_ce: 4.524  loss_mask: 0.887  loss_dice: 4.51  loss_ce_0: 4.384  loss_mask_0: 0.8654  loss_dice_0: 4.365  loss_ce_1: 3.671  loss_mask_1: 0.9401  loss_dice_1: 4.472  loss_ce_2: 3.452  loss_mask_2: 1.114  loss_dice_2: 4.504  loss_ce_3: 3.114  loss_mask_3: 1.145  loss_dice_3: 4.44  loss_ce_4: 3.241  loss_mask_4: 1.102  loss_dice_4: 4.441  loss_ce_5: 3.74  loss_mask_5: 1.059  loss_dice_5: 4.471  loss_ce_6: 4.067  loss_mask_6: 1.066  loss_dice_6: 4.461  loss_ce_7: 4.177  loss_mask_7: 0.9636  loss_dice_7: 4.426  loss_ce_8: 4.114  loss_mask_8: 0.9815  loss_dice_8: 4.408  loss_mars: 0.4502    time: 5.6635  last_time: 5.2926  data_time: 0.0033  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 12:03:54] d2.utils.events INFO:  eta: 10:42:11  iter: 8639  total_loss: 95.75  loss_ce: 2.921  loss_mask: 1.119  loss_dice: 4.182  loss_ce_0: 2.813  loss_mask_0: 1.062  loss_dice_0: 4.089  loss_ce_1: 2.744  loss_mask_1: 1.202  loss_dice_1: 4.189  loss_ce_2: 2.8  loss_mask_2: 1.156  loss_dice_2: 4.366  loss_ce_3: 2.816  loss_mask_3: 1.097  loss_dice_3: 4.323  loss_ce_4: 2.797  loss_mask_4: 1.36  loss_dice_4: 4.311  loss_ce_5: 2.703  loss_mask_5: 1.132  loss_dice_5: 4.172  loss_ce_6: 3.066  loss_mask_6: 1.022  loss_dice_6: 4.118  loss_ce_7: 2.839  loss_mask_7: 1.317  loss_dice_7: 4.275  loss_ce_8: 2.962  loss_mask_8: 1.107  loss_dice_8: 4.16  loss_mars: 0.5584    time: 5.6628  last_time: 5.3719  data_time: 0.0029  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 12:05:44] d2.utils.events INFO:  eta: 10:40:14  iter: 8659  total_loss: 88.08  loss_ce: 3.484  loss_mask: 1.354  loss_dice: 3.912  loss_ce_0: 3.49  loss_mask_0: 1.172  loss_dice_0: 4.223  loss_ce_1: 3.215  loss_mask_1: 0.9792  loss_dice_1: 4.388  loss_ce_2: 3.254  loss_mask_2: 1.435  loss_dice_2: 4.107  loss_ce_3: 2.954  loss_mask_3: 1.452  loss_dice_3: 4.228  loss_ce_4: 3.6  loss_mask_4: 1.199  loss_dice_4: 4.176  loss_ce_5: 4.092  loss_mask_5: 1.119  loss_dice_5: 4.07  loss_ce_6: 4.265  loss_mask_6: 1.123  loss_dice_6: 3.596  loss_ce_7: 3.446  loss_mask_7: 1.345  loss_dice_7: 3.992  loss_ce_8: 3.426  loss_mask_8: 1.265  loss_dice_8: 3.941  loss_mars: 0.5483    time: 5.6623  last_time: 5.3629  data_time: 0.0030  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 12:07:41] d2.utils.events INFO:  eta: 10:38:35  iter: 8679  total_loss: 89.29  loss_ce: 2.742  loss_mask: 1.144  loss_dice: 4.388  loss_ce_0: 3.076  loss_mask_0: 1.044  loss_dice_0: 4.165  loss_ce_1: 2.656  loss_mask_1: 0.8294  loss_dice_1: 4.359  loss_ce_2: 2.515  loss_mask_2: 1.19  loss_dice_2: 4.264  loss_ce_3: 2.522  loss_mask_3: 1.172  loss_dice_3: 4.586  loss_ce_4: 3.004  loss_mask_4: 1.01  loss_dice_4: 4.273  loss_ce_5: 3.011  loss_mask_5: 1.097  loss_dice_5: 4.339  loss_ce_6: 3.024  loss_mask_6: 1.131  loss_dice_6: 4.172  loss_ce_7: 2.777  loss_mask_7: 1.097  loss_dice_7: 4.358  loss_ce_8: 2.774  loss_mask_8: 1.197  loss_dice_8: 4.344  loss_mars: 0.5164    time: 5.6629  last_time: 5.7860  data_time: 0.0033  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 12:09:32] d2.utils.events INFO:  eta: 10:36:29  iter: 8699  total_loss: 97.37  loss_ce: 4.138  loss_mask: 1.103  loss_dice: 4.28  loss_ce_0: 4.546  loss_mask_0: 1.032  loss_dice_0: 4.274  loss_ce_1: 4.318  loss_mask_1: 0.9737  loss_dice_1: 4.232  loss_ce_2: 4.383  loss_mask_2: 1.083  loss_dice_2: 4.353  loss_ce_3: 4.28  loss_mask_3: 0.9455  loss_dice_3: 4.173  loss_ce_4: 4.47  loss_mask_4: 0.9287  loss_dice_4: 4.362  loss_ce_5: 4.507  loss_mask_5: 0.9863  loss_dice_5: 4.374  loss_ce_6: 4.349  loss_mask_6: 1.079  loss_dice_6: 4.36  loss_ce_7: 4.604  loss_mask_7: 1.175  loss_dice_7: 4.317  loss_ce_8: 4.186  loss_mask_8: 0.8758  loss_dice_8: 4.327  loss_mars: 0.5075    time: 5.6626  last_time: 6.0338  data_time: 0.0039  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 12:11:21] d2.utils.events INFO:  eta: 10:34:39  iter: 8719  total_loss: 92.65  loss_ce: 2.45  loss_mask: 2.053  loss_dice: 4.216  loss_ce_0: 2.555  loss_mask_0: 2.019  loss_dice_0: 4.166  loss_ce_1: 2.5  loss_mask_1: 2.229  loss_dice_1: 4.204  loss_ce_2: 2.431  loss_mask_2: 2.302  loss_dice_2: 4.307  loss_ce_3: 2.321  loss_mask_3: 2.314  loss_dice_3: 3.84  loss_ce_4: 2.653  loss_mask_4: 2.087  loss_dice_4: 3.958  loss_ce_5: 2.801  loss_mask_5: 2.358  loss_dice_5: 4.108  loss_ce_6: 2.588  loss_mask_6: 2.014  loss_dice_6: 4.095  loss_ce_7: 2.944  loss_mask_7: 2.068  loss_dice_7: 4.046  loss_ce_8: 2.466  loss_mask_8: 2.091  loss_dice_8: 4.143  loss_mars: 0.3412    time: 5.6620  last_time: 5.1326  data_time: 0.0033  last_data_time: 0.0080   lr: 0.0001  max_mem: 0M
[10/19 12:13:04] d2.utils.events INFO:  eta: 10:29:29  iter: 8739  total_loss: 88.12  loss_ce: 3.266  loss_mask: 1.366  loss_dice: 4.113  loss_ce_0: 2.682  loss_mask_0: 1.509  loss_dice_0: 4.258  loss_ce_1: 2.278  loss_mask_1: 1.574  loss_dice_1: 4.018  loss_ce_2: 2.343  loss_mask_2: 1.584  loss_dice_2: 4.222  loss_ce_3: 2.21  loss_mask_3: 1.781  loss_dice_3: 4.057  loss_ce_4: 2.969  loss_mask_4: 1.501  loss_dice_4: 4.009  loss_ce_5: 3.276  loss_mask_5: 1.354  loss_dice_5: 4.167  loss_ce_6: 3.386  loss_mask_6: 1.587  loss_dice_6: 4.026  loss_ce_7: 3.241  loss_mask_7: 1.547  loss_dice_7: 4.105  loss_ce_8: 3.057  loss_mask_8: 1.423  loss_dice_8: 4.142  loss_mars: 0.4629    time: 5.6605  last_time: 5.0170  data_time: 0.0037  last_data_time: 0.0086   lr: 0.0001  max_mem: 0M
[10/19 12:14:52] d2.utils.events INFO:  eta: 10:23:30  iter: 8759  total_loss: 102.6  loss_ce: 3.361  loss_mask: 1.321  loss_dice: 4.068  loss_ce_0: 3.697  loss_mask_0: 1.237  loss_dice_0: 4.107  loss_ce_1: 3.411  loss_mask_1: 1.752  loss_dice_1: 4.314  loss_ce_2: 3.269  loss_mask_2: 1.725  loss_dice_2: 4.314  loss_ce_3: 3.32  loss_mask_3: 1.991  loss_dice_3: 4.499  loss_ce_4: 3.534  loss_mask_4: 1.392  loss_dice_4: 4.321  loss_ce_5: 3.839  loss_mask_5: 1.455  loss_dice_5: 4.162  loss_ce_6: 3.454  loss_mask_6: 1.489  loss_dice_6: 4.255  loss_ce_7: 3.498  loss_mask_7: 1.495  loss_dice_7: 4.065  loss_ce_8: 3.512  loss_mask_8: 1.513  loss_dice_8: 4.24  loss_mars: 0.4076    time: 5.6599  last_time: 5.2308  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 12:16:44] d2.utils.events INFO:  eta: 10:21:46  iter: 8779  total_loss: 86.32  loss_ce: 2.491  loss_mask: 1.732  loss_dice: 4.106  loss_ce_0: 2.502  loss_mask_0: 1.267  loss_dice_0: 3.911  loss_ce_1: 2.419  loss_mask_1: 1.186  loss_dice_1: 4.125  loss_ce_2: 2.234  loss_mask_2: 1.481  loss_dice_2: 4.157  loss_ce_3: 2.293  loss_mask_3: 1.551  loss_dice_3: 3.918  loss_ce_4: 2.764  loss_mask_4: 1.311  loss_dice_4: 4.001  loss_ce_5: 2.533  loss_mask_5: 1.352  loss_dice_5: 3.966  loss_ce_6: 2.517  loss_mask_6: 1.393  loss_dice_6: 3.887  loss_ce_7: 2.448  loss_mask_7: 1.219  loss_dice_7: 4.088  loss_ce_8: 2.49  loss_mask_8: 1.827  loss_dice_8: 4.154  loss_mars: 0.4184    time: 5.6597  last_time: 5.1463  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 12:18:41] d2.utils.events INFO:  eta: 10:22:49  iter: 8799  total_loss: 108.7  loss_ce: 3.314  loss_mask: 1.837  loss_dice: 4.65  loss_ce_0: 3.429  loss_mask_0: 1.757  loss_dice_0: 4.455  loss_ce_1: 3.189  loss_mask_1: 1.533  loss_dice_1: 4.437  loss_ce_2: 3.175  loss_mask_2: 1.675  loss_dice_2: 4.519  loss_ce_3: 2.972  loss_mask_3: 1.334  loss_dice_3: 4.459  loss_ce_4: 3.274  loss_mask_4: 1.93  loss_dice_4: 4.359  loss_ce_5: 3.233  loss_mask_5: 2.422  loss_dice_5: 4.62  loss_ce_6: 3.252  loss_mask_6: 1.407  loss_dice_6: 4.404  loss_ce_7: 3.316  loss_mask_7: 2.115  loss_dice_7: 4.754  loss_ce_8: 3.134  loss_mask_8: 1.969  loss_dice_8: 4.697  loss_mars: 0.3675    time: 5.6602  last_time: 5.6733  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 12:20:30] d2.utils.events INFO:  eta: 10:18:33  iter: 8819  total_loss: 98.84  loss_ce: 3.776  loss_mask: 1.111  loss_dice: 4.26  loss_ce_0: 3.422  loss_mask_0: 0.9802  loss_dice_0: 4.569  loss_ce_1: 2.902  loss_mask_1: 1.014  loss_dice_1: 4.46  loss_ce_2: 2.769  loss_mask_2: 1.089  loss_dice_2: 4.349  loss_ce_3: 2.882  loss_mask_3: 1.254  loss_dice_3: 4.421  loss_ce_4: 2.947  loss_mask_4: 1.315  loss_dice_4: 4.431  loss_ce_5: 3.324  loss_mask_5: 1.17  loss_dice_5: 4.408  loss_ce_6: 3.249  loss_mask_6: 1.102  loss_dice_6: 4.25  loss_ce_7: 3.061  loss_mask_7: 1.613  loss_dice_7: 4.595  loss_ce_8: 3.348  loss_mask_8: 1.333  loss_dice_8: 4.425  loss_mars: 0.5604    time: 5.6595  last_time: 8.0132  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 12:22:19] d2.utils.events INFO:  eta: 10:16:44  iter: 8839  total_loss: 95.88  loss_ce: 4.428  loss_mask: 0.7974  loss_dice: 4.433  loss_ce_0: 4.064  loss_mask_0: 0.7797  loss_dice_0: 4.35  loss_ce_1: 3.767  loss_mask_1: 0.7631  loss_dice_1: 4.447  loss_ce_2: 3.756  loss_mask_2: 0.7722  loss_dice_2: 4.408  loss_ce_3: 3.841  loss_mask_3: 0.9532  loss_dice_3: 4.603  loss_ce_4: 3.817  loss_mask_4: 0.8684  loss_dice_4: 4.363  loss_ce_5: 4.476  loss_mask_5: 0.7224  loss_dice_5: 4.385  loss_ce_6: 4.209  loss_mask_6: 0.8504  loss_dice_6: 4.514  loss_ce_7: 3.903  loss_mask_7: 0.9582  loss_dice_7: 4.438  loss_ce_8: 4.151  loss_mask_8: 1.059  loss_dice_8: 4.345  loss_mars: 0.4852    time: 5.6590  last_time: 5.6512  data_time: 0.0025  last_data_time: 0.0036   lr: 0.0001  max_mem: 0M
[10/19 12:24:11] d2.utils.events INFO:  eta: 10:16:23  iter: 8859  total_loss: 103.1  loss_ce: 4.327  loss_mask: 0.8362  loss_dice: 4.659  loss_ce_0: 4.37  loss_mask_0: 1.035  loss_dice_0: 4.391  loss_ce_1: 3.991  loss_mask_1: 1.135  loss_dice_1: 4.609  loss_ce_2: 3.901  loss_mask_2: 1.095  loss_dice_2: 4.42  loss_ce_3: 3.645  loss_mask_3: 1.216  loss_dice_3: 4.492  loss_ce_4: 3.676  loss_mask_4: 1.513  loss_dice_4: 4.679  loss_ce_5: 4.044  loss_mask_5: 1.025  loss_dice_5: 4.503  loss_ce_6: 4.171  loss_mask_6: 1.016  loss_dice_6: 4.44  loss_ce_7: 4.017  loss_mask_7: 2.034  loss_dice_7: 4.714  loss_ce_8: 4.149  loss_mask_8: 1.257  loss_dice_8: 4.52  loss_mars: 0.3008    time: 5.6588  last_time: 5.8584  data_time: 0.0041  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 12:26:01] d2.utils.events INFO:  eta: 10:13:34  iter: 8879  total_loss: 101  loss_ce: 3.946  loss_mask: 1.775  loss_dice: 4.501  loss_ce_0: 3.829  loss_mask_0: 1.494  loss_dice_0: 4.449  loss_ce_1: 3.048  loss_mask_1: 1.496  loss_dice_1: 4.536  loss_ce_2: 2.866  loss_mask_2: 1.362  loss_dice_2: 4.34  loss_ce_3: 2.924  loss_mask_3: 1.445  loss_dice_3: 4.407  loss_ce_4: 3.188  loss_mask_4: 1.281  loss_dice_4: 4.119  loss_ce_5: 3.214  loss_mask_5: 1.292  loss_dice_5: 4.343  loss_ce_6: 3.778  loss_mask_6: 1.195  loss_dice_6: 4.175  loss_ce_7: 3.678  loss_mask_7: 1.455  loss_dice_7: 4.606  loss_ce_8: 3.615  loss_mask_8: 1.263  loss_dice_8: 4.449  loss_mars: 0.5454    time: 5.6584  last_time: 4.9957  data_time: 0.0047  last_data_time: 0.0077   lr: 0.0001  max_mem: 0M
[10/19 12:27:55] d2.utils.events INFO:  eta: 10:17:50  iter: 8899  total_loss: 97.74  loss_ce: 3.328  loss_mask: 1.059  loss_dice: 4.408  loss_ce_0: 3.73  loss_mask_0: 1.211  loss_dice_0: 4.307  loss_ce_1: 3.302  loss_mask_1: 1.131  loss_dice_1: 4.292  loss_ce_2: 2.947  loss_mask_2: 1.807  loss_dice_2: 4.425  loss_ce_3: 2.916  loss_mask_3: 1.436  loss_dice_3: 4.434  loss_ce_4: 3.204  loss_mask_4: 1.18  loss_dice_4: 4.223  loss_ce_5: 3.73  loss_mask_5: 1.334  loss_dice_5: 4.333  loss_ce_6: 4.235  loss_mask_6: 1.088  loss_dice_6: 4.416  loss_ce_7: 2.92  loss_mask_7: 1.054  loss_dice_7: 4.307  loss_ce_8: 2.998  loss_mask_8: 1.016  loss_dice_8: 4.316  loss_mars: 0.3321    time: 5.6585  last_time: 5.9918  data_time: 0.0032  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 12:29:51] d2.utils.events INFO:  eta: 10:19:35  iter: 8919  total_loss: 97.48  loss_ce: 2.818  loss_mask: 0.9409  loss_dice: 4.346  loss_ce_0: 3.225  loss_mask_0: 0.8854  loss_dice_0: 4.552  loss_ce_1: 2.857  loss_mask_1: 1.13  loss_dice_1: 4.485  loss_ce_2: 2.833  loss_mask_2: 2.029  loss_dice_2: 4.524  loss_ce_3: 2.632  loss_mask_3: 1.674  loss_dice_3: 4.863  loss_ce_4: 2.72  loss_mask_4: 2.051  loss_dice_4: 4.707  loss_ce_5: 3.445  loss_mask_5: 0.8638  loss_dice_5: 4.391  loss_ce_6: 3.665  loss_mask_6: 0.9448  loss_dice_6: 4.497  loss_ce_7: 2.747  loss_mask_7: 1.495  loss_dice_7: 4.591  loss_ce_8: 2.802  loss_mask_8: 1.169  loss_dice_8: 4.301  loss_mars: 0.554    time: 5.6589  last_time: 6.3740  data_time: 0.0027  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 12:31:39] d2.utils.events INFO:  eta: 10:16:39  iter: 8939  total_loss: 99.78  loss_ce: 3.698  loss_mask: 1.86  loss_dice: 4.501  loss_ce_0: 4.008  loss_mask_0: 1.447  loss_dice_0: 4.377  loss_ce_1: 3.525  loss_mask_1: 1.419  loss_dice_1: 4.589  loss_ce_2: 3.781  loss_mask_2: 1.58  loss_dice_2: 4.39  loss_ce_3: 3.583  loss_mask_3: 1.271  loss_dice_3: 4.538  loss_ce_4: 3.717  loss_mask_4: 1.29  loss_dice_4: 4.567  loss_ce_5: 4.124  loss_mask_5: 1.299  loss_dice_5: 4.441  loss_ce_6: 4.075  loss_mask_6: 1.483  loss_dice_6: 4.614  loss_ce_7: 3.87  loss_mask_7: 1.772  loss_dice_7: 4.483  loss_ce_8: 3.778  loss_mask_8: 1.893  loss_dice_8: 4.452  loss_mars: 0.8728    time: 5.6581  last_time: 6.1613  data_time: 0.0042  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/19 12:33:32] d2.utils.events INFO:  eta: 10:15:54  iter: 8959  total_loss: 87.88  loss_ce: 3.082  loss_mask: 0.9222  loss_dice: 4.338  loss_ce_0: 3.275  loss_mask_0: 0.8817  loss_dice_0: 4.381  loss_ce_1: 3.025  loss_mask_1: 0.9799  loss_dice_1: 4.301  loss_ce_2: 3.278  loss_mask_2: 0.8755  loss_dice_2: 4.404  loss_ce_3: 3.092  loss_mask_3: 0.785  loss_dice_3: 4.492  loss_ce_4: 2.938  loss_mask_4: 0.9535  loss_dice_4: 4.531  loss_ce_5: 3.122  loss_mask_5: 0.7804  loss_dice_5: 4.36  loss_ce_6: 3.372  loss_mask_6: 0.797  loss_dice_6: 4.422  loss_ce_7: 3.037  loss_mask_7: 0.8283  loss_dice_7: 4.321  loss_ce_8: 3.26  loss_mask_8: 0.8777  loss_dice_8: 4.357  loss_mars: 0.665    time: 5.6581  last_time: 5.1768  data_time: 0.0037  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 12:35:24] d2.utils.events INFO:  eta: 10:13:35  iter: 8979  total_loss: 89.44  loss_ce: 3.205  loss_mask: 0.9462  loss_dice: 4.165  loss_ce_0: 3.07  loss_mask_0: 0.9746  loss_dice_0: 4.166  loss_ce_1: 3.039  loss_mask_1: 1.268  loss_dice_1: 4.216  loss_ce_2: 3.107  loss_mask_2: 0.9519  loss_dice_2: 3.929  loss_ce_3: 3.187  loss_mask_3: 1.086  loss_dice_3: 4.156  loss_ce_4: 3.107  loss_mask_4: 0.969  loss_dice_4: 4.293  loss_ce_5: 3.532  loss_mask_5: 1.102  loss_dice_5: 3.659  loss_ce_6: 3.294  loss_mask_6: 0.9248  loss_dice_6: 3.747  loss_ce_7: 3.344  loss_mask_7: 0.9418  loss_dice_7: 4.03  loss_ce_8: 3.141  loss_mask_8: 1.041  loss_dice_8: 4.081  loss_mars: 0.4531    time: 5.6580  last_time: 4.9980  data_time: 0.0031  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 12:37:20] d2.utils.events INFO:  eta: 10:11:44  iter: 8999  total_loss: 100.1  loss_ce: 3.294  loss_mask: 0.9057  loss_dice: 3.91  loss_ce_0: 3.09  loss_mask_0: 1.183  loss_dice_0: 4.28  loss_ce_1: 2.893  loss_mask_1: 1.515  loss_dice_1: 4.341  loss_ce_2: 3.068  loss_mask_2: 0.9917  loss_dice_2: 4.291  loss_ce_3: 3.147  loss_mask_3: 0.9995  loss_dice_3: 4.138  loss_ce_4: 2.948  loss_mask_4: 0.9966  loss_dice_4: 4.68  loss_ce_5: 3.066  loss_mask_5: 0.8411  loss_dice_5: 4.553  loss_ce_6: 3.463  loss_mask_6: 0.9831  loss_dice_6: 4.432  loss_ce_7: 3.442  loss_mask_7: 0.9343  loss_dice_7: 3.853  loss_ce_8: 3.454  loss_mask_8: 0.9855  loss_dice_8: 3.99  loss_mars: 0.4571    time: 5.6584  last_time: 5.1461  data_time: 0.0035  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 12:39:09] d2.utils.events INFO:  eta: 10:09:17  iter: 9019  total_loss: 100.9  loss_ce: 3.093  loss_mask: 2.504  loss_dice: 4.338  loss_ce_0: 2.728  loss_mask_0: 2.527  loss_dice_0: 4.133  loss_ce_1: 2.552  loss_mask_1: 3.132  loss_dice_1: 4.009  loss_ce_2: 2.463  loss_mask_2: 2.874  loss_dice_2: 4.1  loss_ce_3: 2.512  loss_mask_3: 2.296  loss_dice_3: 4.207  loss_ce_4: 2.283  loss_mask_4: 2.372  loss_dice_4: 4.643  loss_ce_5: 2.728  loss_mask_5: 2.341  loss_dice_5: 3.886  loss_ce_6: 2.958  loss_mask_6: 2.566  loss_dice_6: 4.069  loss_ce_7: 3.133  loss_mask_7: 2.818  loss_dice_7: 4.34  loss_ce_8: 2.876  loss_mask_8: 2.699  loss_dice_8: 4.548  loss_mars: 2.02e-06    time: 5.6578  last_time: 3.6018  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 12:41:02] d2.utils.events INFO:  eta: 10:12:12  iter: 9039  total_loss: 86.65  loss_ce: 2.721  loss_mask: 2.077  loss_dice: 4.081  loss_ce_0: 2.261  loss_mask_0: 1.923  loss_dice_0: 3.848  loss_ce_1: 2.113  loss_mask_1: 2.262  loss_dice_1: 3.999  loss_ce_2: 1.937  loss_mask_2: 2.402  loss_dice_2: 4.087  loss_ce_3: 1.85  loss_mask_3: 1.998  loss_dice_3: 4.243  loss_ce_4: 2.001  loss_mask_4: 2.683  loss_dice_4: 4.702  loss_ce_5: 2.239  loss_mask_5: 1.837  loss_dice_5: 4.062  loss_ce_6: 2.464  loss_mask_6: 1.966  loss_dice_6: 4.069  loss_ce_7: 2.454  loss_mask_7: 2.36  loss_dice_7: 4.408  loss_ce_8: 2.509  loss_mask_8: 1.888  loss_dice_8: 4.072  loss_mars: 0.4571    time: 5.6579  last_time: 6.9764  data_time: 0.0022  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 12:42:52] d2.utils.events INFO:  eta: 10:10:49  iter: 9059  total_loss: 103.9  loss_ce: 3.184  loss_mask: 1.132  loss_dice: 4.359  loss_ce_0: 3.096  loss_mask_0: 1.228  loss_dice_0: 4.345  loss_ce_1: 2.617  loss_mask_1: 1.26  loss_dice_1: 4.049  loss_ce_2: 2.631  loss_mask_2: 1.472  loss_dice_2: 4.184  loss_ce_3: 2.433  loss_mask_3: 1.719  loss_dice_3: 4.244  loss_ce_4: 2.758  loss_mask_4: 1.68  loss_dice_4: 3.918  loss_ce_5: 2.915  loss_mask_5: 1.736  loss_dice_5: 4.573  loss_ce_6: 3.081  loss_mask_6: 1.092  loss_dice_6: 4.407  loss_ce_7: 3.172  loss_mask_7: 1.476  loss_dice_7: 4.549  loss_ce_8: 3.127  loss_mask_8: 1.063  loss_dice_8: 4.351  loss_mars: 0.4618    time: 5.6573  last_time: 5.7528  data_time: 0.0022  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 12:44:40] d2.utils.events INFO:  eta: 10:08:29  iter: 9079  total_loss: 97.83  loss_ce: 3.884  loss_mask: 1.842  loss_dice: 4.579  loss_ce_0: 3.295  loss_mask_0: 1.374  loss_dice_0: 4.424  loss_ce_1: 3.238  loss_mask_1: 1.419  loss_dice_1: 4.447  loss_ce_2: 3.112  loss_mask_2: 1.514  loss_dice_2: 4.52  loss_ce_3: 2.905  loss_mask_3: 1.818  loss_dice_3: 4.595  loss_ce_4: 3.24  loss_mask_4: 1.771  loss_dice_4: 4.59  loss_ce_5: 3.45  loss_mask_5: 1.311  loss_dice_5: 4.314  loss_ce_6: 3.559  loss_mask_6: 1.144  loss_dice_6: 4.371  loss_ce_7: 3.685  loss_mask_7: 1.294  loss_dice_7: 4.501  loss_ce_8: 3.545  loss_mask_8: 1.287  loss_dice_8: 4.421  loss_mars: 0.3442    time: 5.6567  last_time: 3.7985  data_time: 0.0031  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 12:46:29] d2.utils.events INFO:  eta: 10:06:37  iter: 9099  total_loss: 95.98  loss_ce: 4.062  loss_mask: 1.335  loss_dice: 4.599  loss_ce_0: 3.794  loss_mask_0: 1.135  loss_dice_0: 4.29  loss_ce_1: 3.578  loss_mask_1: 1.048  loss_dice_1: 4.358  loss_ce_2: 3.343  loss_mask_2: 1.104  loss_dice_2: 4.53  loss_ce_3: 3.295  loss_mask_3: 1.283  loss_dice_3: 4.595  loss_ce_4: 3.507  loss_mask_4: 1.27  loss_dice_4: 4.615  loss_ce_5: 3.486  loss_mask_5: 1.077  loss_dice_5: 4.604  loss_ce_6: 3.428  loss_mask_6: 1.059  loss_dice_6: 4.43  loss_ce_7: 3.565  loss_mask_7: 1.118  loss_dice_7: 4.643  loss_ce_8: 3.941  loss_mask_8: 1.08  loss_dice_8: 4.602  loss_mars: 0.5221    time: 5.6561  last_time: 5.1793  data_time: 0.0026  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 12:48:15] d2.utils.events INFO:  eta: 10:02:23  iter: 9119  total_loss: 89.02  loss_ce: 2.457  loss_mask: 1.376  loss_dice: 4.518  loss_ce_0: 2.554  loss_mask_0: 1.364  loss_dice_0: 4.401  loss_ce_1: 2.407  loss_mask_1: 1.581  loss_dice_1: 4.468  loss_ce_2: 2.427  loss_mask_2: 1.573  loss_dice_2: 4.493  loss_ce_3: 2.218  loss_mask_3: 1.536  loss_dice_3: 4.571  loss_ce_4: 2.394  loss_mask_4: 1.393  loss_dice_4: 4.383  loss_ce_5: 2.427  loss_mask_5: 1.736  loss_dice_5: 4.639  loss_ce_6: 2.52  loss_mask_6: 1.593  loss_dice_6: 4.453  loss_ce_7: 2.365  loss_mask_7: 1.604  loss_dice_7: 4.474  loss_ce_8: 2.516  loss_mask_8: 1.472  loss_dice_8: 4.441  loss_mars: 0.4683    time: 5.6552  last_time: 5.1048  data_time: 0.0022  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 12:50:06] d2.utils.events INFO:  eta: 9:59:16  iter: 9139  total_loss: 103.2  loss_ce: 4.018  loss_mask: 1.703  loss_dice: 4.509  loss_ce_0: 4.047  loss_mask_0: 1.428  loss_dice_0: 4.298  loss_ce_1: 3.615  loss_mask_1: 1.321  loss_dice_1: 4.287  loss_ce_2: 3.35  loss_mask_2: 1.754  loss_dice_2: 4.453  loss_ce_3: 3.711  loss_mask_3: 1.482  loss_dice_3: 4.428  loss_ce_4: 3.279  loss_mask_4: 1.671  loss_dice_4: 4.601  loss_ce_5: 3.799  loss_mask_5: 1.891  loss_dice_5: 4.463  loss_ce_6: 3.975  loss_mask_6: 1.475  loss_dice_6: 4.543  loss_ce_7: 4.062  loss_mask_7: 1.626  loss_dice_7: 4.66  loss_ce_8: 3.838  loss_mask_8: 1.753  loss_dice_8: 4.656  loss_mars: 0.7153    time: 5.6549  last_time: 6.0375  data_time: 0.0031  last_data_time: 0.0083   lr: 0.0001  max_mem: 0M
[10/19 12:51:56] d2.utils.events INFO:  eta: 9:55:10  iter: 9159  total_loss: 87.19  loss_ce: 2.878  loss_mask: 1.348  loss_dice: 4.13  loss_ce_0: 3.139  loss_mask_0: 1.008  loss_dice_0: 4.192  loss_ce_1: 2.888  loss_mask_1: 0.9912  loss_dice_1: 4.513  loss_ce_2: 2.624  loss_mask_2: 1.082  loss_dice_2: 4.248  loss_ce_3: 2.542  loss_mask_3: 1.084  loss_dice_3: 4.122  loss_ce_4: 2.737  loss_mask_4: 1.352  loss_dice_4: 4.541  loss_ce_5: 3.004  loss_mask_5: 1.115  loss_dice_5: 3.949  loss_ce_6: 2.874  loss_mask_6: 1.298  loss_dice_6: 3.96  loss_ce_7: 3.18  loss_mask_7: 1.144  loss_dice_7: 4.115  loss_ce_8: 2.882  loss_mask_8: 1.218  loss_dice_8: 4.144  loss_mars: 0.5531    time: 5.6545  last_time: 5.8166  data_time: 0.0031  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 12:53:50] d2.utils.events INFO:  eta: 9:59:11  iter: 9179  total_loss: 94.93  loss_ce: 2.885  loss_mask: 1.293  loss_dice: 4.433  loss_ce_0: 2.907  loss_mask_0: 1.025  loss_dice_0: 4.322  loss_ce_1: 2.711  loss_mask_1: 1.155  loss_dice_1: 4.49  loss_ce_2: 2.737  loss_mask_2: 1.074  loss_dice_2: 4.422  loss_ce_3: 2.76  loss_mask_3: 1.236  loss_dice_3: 4.117  loss_ce_4: 2.738  loss_mask_4: 1.344  loss_dice_4: 4.217  loss_ce_5: 2.728  loss_mask_5: 1.106  loss_dice_5: 4.315  loss_ce_6: 3.012  loss_mask_6: 1.031  loss_dice_6: 4.168  loss_ce_7: 3.24  loss_mask_7: 1.073  loss_dice_7: 4.133  loss_ce_8: 3.321  loss_mask_8: 0.9677  loss_dice_8: 4.33  loss_mars: 0.6278    time: 5.6545  last_time: 5.6914  data_time: 0.0027  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 12:55:42] d2.utils.events INFO:  eta: 9:59:22  iter: 9199  total_loss: 102.8  loss_ce: 3.428  loss_mask: 1.964  loss_dice: 4.614  loss_ce_0: 3.271  loss_mask_0: 1.878  loss_dice_0: 3.841  loss_ce_1: 2.833  loss_mask_1: 1.712  loss_dice_1: 4.582  loss_ce_2: 2.792  loss_mask_2: 1.944  loss_dice_2: 4.577  loss_ce_3: 2.711  loss_mask_3: 1.88  loss_dice_3: 4.607  loss_ce_4: 2.731  loss_mask_4: 2.187  loss_dice_4: 4.467  loss_ce_5: 2.908  loss_mask_5: 2.022  loss_dice_5: 4.42  loss_ce_6: 3.224  loss_mask_6: 1.985  loss_dice_6: 4.504  loss_ce_7: 3.755  loss_mask_7: 2.013  loss_dice_7: 4.229  loss_ce_8: 3.762  loss_mask_8: 1.866  loss_dice_8: 4.425  loss_mars: 0.4205    time: 5.6545  last_time: 5.6914  data_time: 0.0029  last_data_time: 0.0083   lr: 0.0001  max_mem: 0M
[10/19 12:57:25] d2.utils.events INFO:  eta: 9:56:01  iter: 9219  total_loss: 87.22  loss_ce: 3.373  loss_mask: 1.505  loss_dice: 4.057  loss_ce_0: 2.905  loss_mask_0: 1.842  loss_dice_0: 4.122  loss_ce_1: 2.882  loss_mask_1: 1.452  loss_dice_1: 4.179  loss_ce_2: 2.666  loss_mask_2: 1.98  loss_dice_2: 4.065  loss_ce_3: 2.503  loss_mask_3: 1.665  loss_dice_3: 4.048  loss_ce_4: 2.506  loss_mask_4: 1.756  loss_dice_4: 4.284  loss_ce_5: 2.945  loss_mask_5: 1.774  loss_dice_5: 4.095  loss_ce_6: 3.195  loss_mask_6: 1.643  loss_dice_6: 4.182  loss_ce_7: 3.474  loss_mask_7: 1.438  loss_dice_7: 4.002  loss_ce_8: 3.348  loss_mask_8: 1.458  loss_dice_8: 4.081  loss_mars: 0.3644    time: 5.6532  last_time: 5.1890  data_time: 0.0033  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 12:59:16] d2.utils.events INFO:  eta: 9:55:38  iter: 9239  total_loss: 77.29  loss_ce: 2.847  loss_mask: 1.802  loss_dice: 3.648  loss_ce_0: 2.293  loss_mask_0: 2.392  loss_dice_0: 4.144  loss_ce_1: 2.21  loss_mask_1: 1.611  loss_dice_1: 3.985  loss_ce_2: 1.979  loss_mask_2: 1.91  loss_dice_2: 3.948  loss_ce_3: 2.002  loss_mask_3: 1.784  loss_dice_3: 4.234  loss_ce_4: 2.072  loss_mask_4: 1.956  loss_dice_4: 3.717  loss_ce_5: 2.275  loss_mask_5: 1.823  loss_dice_5: 3.816  loss_ce_6: 2.635  loss_mask_6: 1.623  loss_dice_6: 3.639  loss_ce_7: 3.271  loss_mask_7: 1.504  loss_dice_7: 3.619  loss_ce_8: 3.096  loss_mask_8: 1.709  loss_dice_8: 3.775  loss_mars: 0.5989    time: 5.6528  last_time: 5.9979  data_time: 0.0024  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 13:01:07] d2.utils.events INFO:  eta: 9:52:45  iter: 9259  total_loss: 98.52  loss_ce: 4.228  loss_mask: 1.429  loss_dice: 4.328  loss_ce_0: 3.719  loss_mask_0: 1.528  loss_dice_0: 4.425  loss_ce_1: 2.911  loss_mask_1: 1.693  loss_dice_1: 4.306  loss_ce_2: 2.768  loss_mask_2: 1.792  loss_dice_2: 4.478  loss_ce_3: 2.843  loss_mask_3: 1.661  loss_dice_3: 4.501  loss_ce_4: 2.878  loss_mask_4: 2.019  loss_dice_4: 4.461  loss_ce_5: 3.242  loss_mask_5: 1.886  loss_dice_5: 4.384  loss_ce_6: 3.642  loss_mask_6: 1.731  loss_dice_6: 4.522  loss_ce_7: 3.493  loss_mask_7: 1.73  loss_dice_7: 4.479  loss_ce_8: 3.784  loss_mask_8: 1.555  loss_dice_8: 4.464  loss_mars: 0.559    time: 5.6526  last_time: 5.6891  data_time: 0.0037  last_data_time: 0.0087   lr: 0.0001  max_mem: 0M
[10/19 13:02:52] d2.utils.events INFO:  eta: 9:51:29  iter: 9279  total_loss: 90.16  loss_ce: 3.018  loss_mask: 1.643  loss_dice: 4.283  loss_ce_0: 2.67  loss_mask_0: 1.671  loss_dice_0: 4.117  loss_ce_1: 2.415  loss_mask_1: 1.858  loss_dice_1: 4.502  loss_ce_2: 2.262  loss_mask_2: 1.866  loss_dice_2: 4.436  loss_ce_3: 2.307  loss_mask_3: 1.923  loss_dice_3: 4.518  loss_ce_4: 2.288  loss_mask_4: 1.635  loss_dice_4: 4.43  loss_ce_5: 2.417  loss_mask_5: 1.784  loss_dice_5: 4.331  loss_ce_6: 2.542  loss_mask_6: 2.19  loss_dice_6: 4.245  loss_ce_7: 3.157  loss_mask_7: 1.512  loss_dice_7: 4.354  loss_ce_8: 2.992  loss_mask_8: 1.48  loss_dice_8: 4.313  loss_mars: 0.1991    time: 5.6515  last_time: 3.7062  data_time: 0.0032  last_data_time: 0.0036   lr: 0.0001  max_mem: 0M
[10/19 13:04:33] d2.utils.events INFO:  eta: 9:48:34  iter: 9299  total_loss: 96.11  loss_ce: 3.054  loss_mask: 0.9736  loss_dice: 4.349  loss_ce_0: 2.623  loss_mask_0: 0.9947  loss_dice_0: 4.285  loss_ce_1: 2.637  loss_mask_1: 0.9412  loss_dice_1: 4.429  loss_ce_2: 2.636  loss_mask_2: 1.107  loss_dice_2: 4.427  loss_ce_3: 2.473  loss_mask_3: 0.9822  loss_dice_3: 4.389  loss_ce_4: 2.324  loss_mask_4: 1.274  loss_dice_4: 4.515  loss_ce_5: 2.678  loss_mask_5: 1.015  loss_dice_5: 4.206  loss_ce_6: 3.133  loss_mask_6: 0.9601  loss_dice_6: 4.353  loss_ce_7: 3.074  loss_mask_7: 1.105  loss_dice_7: 4.424  loss_ce_8: 3.031  loss_mask_8: 1.073  loss_dice_8: 4.387  loss_mars: 0.2965    time: 5.6500  last_time: 3.9108  data_time: 0.0024  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 13:06:28] d2.utils.events INFO:  eta: 9:48:10  iter: 9319  total_loss: 95.27  loss_ce: 3.283  loss_mask: 1.489  loss_dice: 4.369  loss_ce_0: 3.43  loss_mask_0: 1.086  loss_dice_0: 4.281  loss_ce_1: 4.149  loss_mask_1: 1.465  loss_dice_1: 4.391  loss_ce_2: 3.904  loss_mask_2: 1.255  loss_dice_2: 4.394  loss_ce_3: 2.344  loss_mask_3: 1.813  loss_dice_3: 4.354  loss_ce_4: 2.324  loss_mask_4: 1.514  loss_dice_4: 4.293  loss_ce_5: 2.766  loss_mask_5: 1.688  loss_dice_5: 4.358  loss_ce_6: 2.726  loss_mask_6: 1.776  loss_dice_6: 4.465  loss_ce_7: 3.006  loss_mask_7: 1.864  loss_dice_7: 4.321  loss_ce_8: 2.951  loss_mask_8: 1.482  loss_dice_8: 4.32  loss_mars: 0.5525    time: 5.6502  last_time: 5.7192  data_time: 0.0038  last_data_time: 0.0093   lr: 0.0001  max_mem: 0M
[10/19 13:08:16] d2.utils.events INFO:  eta: 9:46:15  iter: 9339  total_loss: 97.54  loss_ce: 3.967  loss_mask: 1.145  loss_dice: 4.313  loss_ce_0: 4.171  loss_mask_0: 1.221  loss_dice_0: 4.255  loss_ce_1: 3.811  loss_mask_1: 1.144  loss_dice_1: 4.231  loss_ce_2: 4.133  loss_mask_2: 1.132  loss_dice_2: 4.329  loss_ce_3: 3.335  loss_mask_3: 1.434  loss_dice_3: 4.322  loss_ce_4: 3.211  loss_mask_4: 1.367  loss_dice_4: 4.266  loss_ce_5: 3.6  loss_mask_5: 1.232  loss_dice_5: 4.391  loss_ce_6: 3.584  loss_mask_6: 1.224  loss_dice_6: 4.387  loss_ce_7: 3.821  loss_mask_7: 1.17  loss_dice_7: 4.358  loss_ce_8: 3.734  loss_mask_8: 1.321  loss_dice_8: 4.238  loss_mars: 0.5309    time: 5.6496  last_time: 5.0156  data_time: 0.0030  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 13:10:09] d2.utils.events INFO:  eta: 9:43:26  iter: 9359  total_loss: 91.9  loss_ce: 3.068  loss_mask: 1.434  loss_dice: 4.288  loss_ce_0: 3.53  loss_mask_0: 1.101  loss_dice_0: 4.117  loss_ce_1: 3.177  loss_mask_1: 1.141  loss_dice_1: 4.172  loss_ce_2: 3.319  loss_mask_2: 1.171  loss_dice_2: 4.305  loss_ce_3: 2.765  loss_mask_3: 1.358  loss_dice_3: 3.971  loss_ce_4: 2.802  loss_mask_4: 1.376  loss_dice_4: 3.985  loss_ce_5: 2.721  loss_mask_5: 1.358  loss_dice_5: 4.104  loss_ce_6: 2.967  loss_mask_6: 1.29  loss_dice_6: 3.99  loss_ce_7: 3.331  loss_mask_7: 1.129  loss_dice_7: 4.212  loss_ce_8: 3.35  loss_mask_8: 1.201  loss_dice_8: 4.067  loss_mars: 0.6324    time: 5.6496  last_time: 5.0944  data_time: 0.0032  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/19 13:11:44] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0009377.pth
[10/19 13:11:44] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 13:11:44] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 13:11:44] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 13:11:44] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 13:11:55] d2.utils.events INFO:  eta: 9:40:35  iter: 9379  total_loss: 90.72  loss_ce: 3.137  loss_mask: 0.956  loss_dice: 4.117  loss_ce_0: 3.569  loss_mask_0: 1.11  loss_dice_0: 4.297  loss_ce_1: 3.142  loss_mask_1: 0.9566  loss_dice_1: 4.27  loss_ce_2: 3.344  loss_mask_2: 0.9356  loss_dice_2: 4.112  loss_ce_3: 2.567  loss_mask_3: 1.415  loss_dice_3: 4.32  loss_ce_4: 2.552  loss_mask_4: 1.292  loss_dice_4: 4.17  loss_ce_5: 2.699  loss_mask_5: 1.129  loss_dice_5: 4.122  loss_ce_6: 2.69  loss_mask_6: 1.098  loss_dice_6: 4.203  loss_ce_7: 3.06  loss_mask_7: 1.039  loss_dice_7: 4.137  loss_ce_8: 2.962  loss_mask_8: 1.004  loss_dice_8: 4.131  loss_mars: 0.6612    time: 5.6487  last_time: 5.8104  data_time: 0.0028  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 13:13:42] d2.utils.events INFO:  eta: 9:39:42  iter: 9399  total_loss: 89.41  loss_ce: 3.246  loss_mask: 1.39  loss_dice: 4.35  loss_ce_0: 3.709  loss_mask_0: 1.141  loss_dice_0: 4.327  loss_ce_1: 3.256  loss_mask_1: 1.402  loss_dice_1: 4.275  loss_ce_2: 3.34  loss_mask_2: 1.167  loss_dice_2: 4.201  loss_ce_3: 2.762  loss_mask_3: 1.246  loss_dice_3: 4.321  loss_ce_4: 3.077  loss_mask_4: 1.494  loss_dice_4: 4.177  loss_ce_5: 2.909  loss_mask_5: 1.541  loss_dice_5: 4.289  loss_ce_6: 3.079  loss_mask_6: 1.451  loss_dice_6: 4.352  loss_ce_7: 3.215  loss_mask_7: 1.458  loss_dice_7: 4.36  loss_ce_8: 3.243  loss_mask_8: 1.409  loss_dice_8: 4.318  loss_mars: 0.2838    time: 5.6479  last_time: 5.2369  data_time: 0.0022  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 13:15:33] d2.utils.events INFO:  eta: 9:37:51  iter: 9419  total_loss: 92.95  loss_ce: 2.504  loss_mask: 1.566  loss_dice: 4.209  loss_ce_0: 2.677  loss_mask_0: 1.099  loss_dice_0: 4.347  loss_ce_1: 2.524  loss_mask_1: 1.425  loss_dice_1: 4.201  loss_ce_2: 2.508  loss_mask_2: 1.564  loss_dice_2: 4.249  loss_ce_3: 2.337  loss_mask_3: 1.505  loss_dice_3: 4.299  loss_ce_4: 2.402  loss_mask_4: 1.541  loss_dice_4: 4.213  loss_ce_5: 2.376  loss_mask_5: 1.21  loss_dice_5: 4.34  loss_ce_6: 2.834  loss_mask_6: 1.2  loss_dice_6: 4.159  loss_ce_7: 2.532  loss_mask_7: 1.269  loss_dice_7: 4.326  loss_ce_8: 2.504  loss_mask_8: 1.446  loss_dice_8: 4.334  loss_mars: 0.5655    time: 5.6477  last_time: 6.9121  data_time: 0.0037  last_data_time: 0.0085   lr: 0.0001  max_mem: 0M
[10/19 13:17:22] d2.utils.events INFO:  eta: 9:35:30  iter: 9439  total_loss: 98.24  loss_ce: 4.188  loss_mask: 1.305  loss_dice: 4.503  loss_ce_0: 3.955  loss_mask_0: 1.412  loss_dice_0: 4.219  loss_ce_1: 3.627  loss_mask_1: 1.244  loss_dice_1: 4.397  loss_ce_2: 3.965  loss_mask_2: 1.561  loss_dice_2: 4.406  loss_ce_3: 3.021  loss_mask_3: 1.771  loss_dice_3: 4.478  loss_ce_4: 3.339  loss_mask_4: 1.578  loss_dice_4: 4.439  loss_ce_5: 3.245  loss_mask_5: 1.732  loss_dice_5: 4.297  loss_ce_6: 3.258  loss_mask_6: 1.389  loss_dice_6: 4.387  loss_ce_7: 3.598  loss_mask_7: 1.509  loss_dice_7: 4.323  loss_ce_8: 3.997  loss_mask_8: 1.666  loss_dice_8: 4.354  loss_mars: 0.2667    time: 5.6471  last_time: 5.7018  data_time: 0.0029  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 13:19:12] d2.utils.events INFO:  eta: 9:31:19  iter: 9459  total_loss: 87.04  loss_ce: 2.639  loss_mask: 1.268  loss_dice: 4.176  loss_ce_0: 2.642  loss_mask_0: 1.177  loss_dice_0: 4.031  loss_ce_1: 2.642  loss_mask_1: 1.406  loss_dice_1: 4.143  loss_ce_2: 2.478  loss_mask_2: 1.853  loss_dice_2: 4.07  loss_ce_3: 2.094  loss_mask_3: 1.601  loss_dice_3: 4.287  loss_ce_4: 2.2  loss_mask_4: 1.871  loss_dice_4: 4.018  loss_ce_5: 2.117  loss_mask_5: 1.771  loss_dice_5: 4.123  loss_ce_6: 2.181  loss_mask_6: 1.363  loss_dice_6: 4.153  loss_ce_7: 2.57  loss_mask_7: 1.266  loss_dice_7: 4.178  loss_ce_8: 2.638  loss_mask_8: 1.398  loss_dice_8: 4.214  loss_mars: 0.4752    time: 5.6468  last_time: 5.6796  data_time: 0.0028  last_data_time: 0.0084   lr: 0.0001  max_mem: 0M
[10/19 13:21:12] d2.utils.events INFO:  eta: 9:30:41  iter: 9479  total_loss: 99.69  loss_ce: 3.434  loss_mask: 1.49  loss_dice: 4.546  loss_ce_0: 3.017  loss_mask_0: 1.485  loss_dice_0: 4.332  loss_ce_1: 2.972  loss_mask_1: 1.799  loss_dice_1: 4.735  loss_ce_2: 2.9  loss_mask_2: 2.348  loss_dice_2: 4.805  loss_ce_3: 2.51  loss_mask_3: 1.841  loss_dice_3: 4.414  loss_ce_4: 2.584  loss_mask_4: 1.308  loss_dice_4: 4.263  loss_ce_5: 2.941  loss_mask_5: 1.471  loss_dice_5: 4.372  loss_ce_6: 3.287  loss_mask_6: 1.407  loss_dice_6: 4.343  loss_ce_7: 3.344  loss_mask_7: 1.239  loss_dice_7: 4.279  loss_ce_8: 3.563  loss_mask_8: 1.329  loss_dice_8: 4.294  loss_mars: 0.4491    time: 5.6476  last_time: 5.0371  data_time: 0.0032  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 13:22:54] d2.utils.events INFO:  eta: 9:25:00  iter: 9499  total_loss: 98.29  loss_ce: 4.274  loss_mask: 1.377  loss_dice: 4.333  loss_ce_0: 4.036  loss_mask_0: 1.166  loss_dice_0: 4.345  loss_ce_1: 4.031  loss_mask_1: 1.355  loss_dice_1: 4.368  loss_ce_2: 3.585  loss_mask_2: 1.49  loss_dice_2: 4.321  loss_ce_3: 3.147  loss_mask_3: 1.854  loss_dice_3: 4.359  loss_ce_4: 3.329  loss_mask_4: 1.633  loss_dice_4: 4.521  loss_ce_5: 3.402  loss_mask_5: 1.761  loss_dice_5: 4.437  loss_ce_6: 3.739  loss_mask_6: 1.742  loss_dice_6: 4.552  loss_ce_7: 4.154  loss_mask_7: 1.475  loss_dice_7: 4.411  loss_ce_8: 3.809  loss_mask_8: 1.48  loss_dice_8: 4.35  loss_mars: 0.6198    time: 5.6463  last_time: 5.4298  data_time: 0.0029  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 13:24:45] d2.utils.events INFO:  eta: 9:25:45  iter: 9519  total_loss: 89.89  loss_ce: 3.784  loss_mask: 1.327  loss_dice: 4.272  loss_ce_0: 3.552  loss_mask_0: 1.426  loss_dice_0: 4.19  loss_ce_1: 3.584  loss_mask_1: 1.349  loss_dice_1: 4.086  loss_ce_2: 3.509  loss_mask_2: 1.481  loss_dice_2: 4.443  loss_ce_3: 2.937  loss_mask_3: 1.503  loss_dice_3: 4.193  loss_ce_4: 3.008  loss_mask_4: 1.625  loss_dice_4: 4.282  loss_ce_5: 3.092  loss_mask_5: 1.759  loss_dice_5: 4.274  loss_ce_6: 3.3  loss_mask_6: 1.684  loss_dice_6: 4.329  loss_ce_7: 3.21  loss_mask_7: 1.451  loss_dice_7: 4.275  loss_ce_8: 3.783  loss_mask_8: 1.446  loss_dice_8: 4.331  loss_mars: 0.6166    time: 5.6460  last_time: 5.6526  data_time: 0.0025  last_data_time: 0.0082   lr: 0.0001  max_mem: 0M
[10/19 13:26:34] d2.utils.events INFO:  eta: 9:22:42  iter: 9539  total_loss: 103.2  loss_ce: 3.691  loss_mask: 0.9637  loss_dice: 4.537  loss_ce_0: 3.986  loss_mask_0: 1.051  loss_dice_0: 4.306  loss_ce_1: 3.583  loss_mask_1: 1.016  loss_dice_1: 4.395  loss_ce_2: 3.263  loss_mask_2: 1.121  loss_dice_2: 4.378  loss_ce_3: 3.844  loss_mask_3: 1.631  loss_dice_3: 4.624  loss_ce_4: 3.515  loss_mask_4: 2.404  loss_dice_4: 4.507  loss_ce_5: 3.698  loss_mask_5: 1.476  loss_dice_5: 4.445  loss_ce_6: 3.53  loss_mask_6: 1.249  loss_dice_6: 4.425  loss_ce_7: 4.198  loss_mask_7: 0.9809  loss_dice_7: 4.465  loss_ce_8: 3.817  loss_mask_8: 0.9924  loss_dice_8: 4.281  loss_mars: 0.5175    time: 5.6455  last_time: 5.1150  data_time: 0.0033  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 13:28:24] d2.utils.events INFO:  eta: 9:20:02  iter: 9559  total_loss: 91.5  loss_ce: 3.598  loss_mask: 1.094  loss_dice: 4.428  loss_ce_0: 3.251  loss_mask_0: 0.8513  loss_dice_0: 4.523  loss_ce_1: 3.286  loss_mask_1: 0.9241  loss_dice_1: 4.44  loss_ce_2: 3.078  loss_mask_2: 0.9856  loss_dice_2: 4.491  loss_ce_3: 2.939  loss_mask_3: 1.018  loss_dice_3: 4.447  loss_ce_4: 2.783  loss_mask_4: 1.074  loss_dice_4: 4.465  loss_ce_5: 2.753  loss_mask_5: 1.074  loss_dice_5: 4.371  loss_ce_6: 2.794  loss_mask_6: 0.9999  loss_dice_6: 4.456  loss_ce_7: 3.697  loss_mask_7: 1.214  loss_dice_7: 4.472  loss_ce_8: 3.583  loss_mask_8: 0.8955  loss_dice_8: 4.536  loss_mars: 0.1563    time: 5.6451  last_time: 5.0074  data_time: 0.0030  last_data_time: 0.0080   lr: 0.0001  max_mem: 0M
[10/19 13:30:13] d2.utils.events INFO:  eta: 9:17:37  iter: 9579  total_loss: 91.65  loss_ce: 3.722  loss_mask: 0.9611  loss_dice: 4.597  loss_ce_0: 3.69  loss_mask_0: 0.9645  loss_dice_0: 4.462  loss_ce_1: 3.833  loss_mask_1: 1.058  loss_dice_1: 4.54  loss_ce_2: 3.141  loss_mask_2: 0.9249  loss_dice_2: 4.411  loss_ce_3: 3.17  loss_mask_3: 0.9707  loss_dice_3: 4.4  loss_ce_4: 3.104  loss_mask_4: 0.9702  loss_dice_4: 4.375  loss_ce_5: 3.311  loss_mask_5: 1.046  loss_dice_5: 4.445  loss_ce_6: 3.191  loss_mask_6: 1.209  loss_dice_6: 4.539  loss_ce_7: 3.822  loss_mask_7: 1.254  loss_dice_7: 4.496  loss_ce_8: 3.636  loss_mask_8: 1.151  loss_dice_8: 4.392  loss_mars: 0.578    time: 5.6447  last_time: 5.1116  data_time: 0.0026  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 13:32:04] d2.utils.events INFO:  eta: 9:16:20  iter: 9599  total_loss: 93.02  loss_ce: 4.07  loss_mask: 1.018  loss_dice: 4.441  loss_ce_0: 4.242  loss_mask_0: 1.15  loss_dice_0: 4.55  loss_ce_1: 4.137  loss_mask_1: 1.077  loss_dice_1: 4.325  loss_ce_2: 3.681  loss_mask_2: 1.185  loss_dice_2: 4.383  loss_ce_3: 3.473  loss_mask_3: 1.09  loss_dice_3: 4.438  loss_ce_4: 3.387  loss_mask_4: 1.071  loss_dice_4: 4.355  loss_ce_5: 3.612  loss_mask_5: 1.292  loss_dice_5: 4.422  loss_ce_6: 3.352  loss_mask_6: 1.163  loss_dice_6: 4.666  loss_ce_7: 4.041  loss_mask_7: 1.211  loss_dice_7: 4.558  loss_ce_8: 4.055  loss_mask_8: 1.072  loss_dice_8: 4.357  loss_mars: 0.6691    time: 5.6444  last_time: 5.6494  data_time: 0.0027  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 13:33:53] d2.utils.events INFO:  eta: 9:13:59  iter: 9619  total_loss: 91.72  loss_ce: 3.319  loss_mask: 1.406  loss_dice: 4.36  loss_ce_0: 3.024  loss_mask_0: 1.293  loss_dice_0: 4.262  loss_ce_1: 2.904  loss_mask_1: 1.335  loss_dice_1: 4.327  loss_ce_2: 2.68  loss_mask_2: 1.356  loss_dice_2: 4.346  loss_ce_3: 2.552  loss_mask_3: 1.487  loss_dice_3: 4.436  loss_ce_4: 2.444  loss_mask_4: 1.412  loss_dice_4: 4.401  loss_ce_5: 2.661  loss_mask_5: 1.675  loss_dice_5: 4.359  loss_ce_6: 2.898  loss_mask_6: 1.636  loss_dice_6: 4.422  loss_ce_7: 3.258  loss_mask_7: 1.581  loss_dice_7: 4.372  loss_ce_8: 2.999  loss_mask_8: 1.381  loss_dice_8: 4.333  loss_mars: 0.588    time: 5.6440  last_time: 3.8068  data_time: 0.0029  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 13:35:42] d2.utils.events INFO:  eta: 9:09:02  iter: 9639  total_loss: 101  loss_ce: 4.078  loss_mask: 1.144  loss_dice: 4.16  loss_ce_0: 4.417  loss_mask_0: 1.125  loss_dice_0: 4.229  loss_ce_1: 3.758  loss_mask_1: 1.303  loss_dice_1: 4.34  loss_ce_2: 3.542  loss_mask_2: 1.213  loss_dice_2: 4.329  loss_ce_3: 3.824  loss_mask_3: 1.488  loss_dice_3: 4.253  loss_ce_4: 4.054  loss_mask_4: 1.693  loss_dice_4: 4.31  loss_ce_5: 4.243  loss_mask_5: 1.522  loss_dice_5: 4.34  loss_ce_6: 4.044  loss_mask_6: 1.382  loss_dice_6: 4.406  loss_ce_7: 4.136  loss_mask_7: 1.302  loss_dice_7: 4.252  loss_ce_8: 4.077  loss_mask_8: 1.25  loss_dice_8: 4.163  loss_mars: 0.5579    time: 5.6434  last_time: 5.2940  data_time: 0.0023  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 13:37:37] d2.utils.events INFO:  eta: 9:09:59  iter: 9659  total_loss: 98.72  loss_ce: 4.121  loss_mask: 0.6567  loss_dice: 4.175  loss_ce_0: 4.547  loss_mask_0: 0.6459  loss_dice_0: 4.178  loss_ce_1: 3.927  loss_mask_1: 0.6015  loss_dice_1: 4.222  loss_ce_2: 3.783  loss_mask_2: 0.6681  loss_dice_2: 4.047  loss_ce_3: 3.766  loss_mask_3: 0.6148  loss_dice_3: 4.181  loss_ce_4: 3.986  loss_mask_4: 0.6515  loss_dice_4: 4.091  loss_ce_5: 4.258  loss_mask_5: 0.681  loss_dice_5: 4.193  loss_ce_6: 4.106  loss_mask_6: 0.681  loss_dice_6: 4.114  loss_ce_7: 4.133  loss_mask_7: 1.032  loss_dice_7: 4.408  loss_ce_8: 4.131  loss_mask_8: 1.031  loss_dice_8: 4.306  loss_mars: 0.7226    time: 5.6437  last_time: 5.9472  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 13:39:26] d2.utils.events INFO:  eta: 9:06:01  iter: 9679  total_loss: 92.43  loss_ce: 3.048  loss_mask: 1.889  loss_dice: 4.341  loss_ce_0: 3.195  loss_mask_0: 2.03  loss_dice_0: 4.073  loss_ce_1: 3.006  loss_mask_1: 2.003  loss_dice_1: 4.342  loss_ce_2: 3.012  loss_mask_2: 2.084  loss_dice_2: 4.297  loss_ce_3: 2.6  loss_mask_3: 2.25  loss_dice_3: 4.444  loss_ce_4: 2.805  loss_mask_4: 2.056  loss_dice_4: 4.179  loss_ce_5: 3.003  loss_mask_5: 2.236  loss_dice_5: 4.078  loss_ce_6: 2.798  loss_mask_6: 2.336  loss_dice_6: 4.336  loss_ce_7: 2.844  loss_mask_7: 2.487  loss_dice_7: 4.194  loss_ce_8: 2.915  loss_mask_8: 2.078  loss_dice_8: 4.217  loss_mars: 0.671    time: 5.6432  last_time: 5.6815  data_time: 0.0028  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 13:41:18] d2.utils.events INFO:  eta: 9:05:41  iter: 9699  total_loss: 86.55  loss_ce: 2.834  loss_mask: 1.314  loss_dice: 4.239  loss_ce_0: 2.948  loss_mask_0: 1.405  loss_dice_0: 4.364  loss_ce_1: 2.935  loss_mask_1: 1.46  loss_dice_1: 4.161  loss_ce_2: 2.968  loss_mask_2: 1.518  loss_dice_2: 4.189  loss_ce_3: 2.772  loss_mask_3: 1.726  loss_dice_3: 4.448  loss_ce_4: 2.883  loss_mask_4: 1.504  loss_dice_4: 4.32  loss_ce_5: 3.2  loss_mask_5: 1.428  loss_dice_5: 4.365  loss_ce_6: 3.031  loss_mask_6: 1.791  loss_dice_6: 4.496  loss_ce_7: 3.003  loss_mask_7: 1.622  loss_dice_7: 4.298  loss_ce_8: 2.973  loss_mask_8: 1.49  loss_dice_8: 4.283  loss_mars: 0.4988    time: 5.6431  last_time: 5.3616  data_time: 0.0030  last_data_time: 0.0087   lr: 0.0001  max_mem: 0M
[10/19 13:43:11] d2.utils.events INFO:  eta: 9:05:37  iter: 9719  total_loss: 96.5  loss_ce: 3.543  loss_mask: 1.161  loss_dice: 4.345  loss_ce_0: 3.714  loss_mask_0: 1.087  loss_dice_0: 4.055  loss_ce_1: 3.431  loss_mask_1: 1.174  loss_dice_1: 4.314  loss_ce_2: 3.713  loss_mask_2: 1.21  loss_dice_2: 4.087  loss_ce_3: 3.628  loss_mask_3: 1.161  loss_dice_3: 4.113  loss_ce_4: 3.448  loss_mask_4: 1.138  loss_dice_4: 4.347  loss_ce_5: 3.678  loss_mask_5: 1.168  loss_dice_5: 4.002  loss_ce_6: 3.531  loss_mask_6: 1.228  loss_dice_6: 4.371  loss_ce_7: 3.602  loss_mask_7: 1.202  loss_dice_7: 4.248  loss_ce_8: 3.685  loss_mask_8: 1.083  loss_dice_8: 4.136  loss_mars: 0.5956    time: 5.6432  last_time: 5.6789  data_time: 0.0032  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 13:45:07] d2.utils.events INFO:  eta: 9:06:32  iter: 9739  total_loss: 89.01  loss_ce: 3.12  loss_mask: 0.8367  loss_dice: 4.337  loss_ce_0: 3.72  loss_mask_0: 0.7572  loss_dice_0: 4.37  loss_ce_1: 3.707  loss_mask_1: 0.7347  loss_dice_1: 4.255  loss_ce_2: 3.498  loss_mask_2: 0.6351  loss_dice_2: 4.373  loss_ce_3: 3.507  loss_mask_3: 0.8508  loss_dice_3: 4.258  loss_ce_4: 3.089  loss_mask_4: 0.983  loss_dice_4: 4.52  loss_ce_5: 3.531  loss_mask_5: 0.823  loss_dice_5: 4.448  loss_ce_6: 3.377  loss_mask_6: 0.7106  loss_dice_6: 4.539  loss_ce_7: 3.618  loss_mask_7: 0.8444  loss_dice_7: 4.352  loss_ce_8: 3.436  loss_mask_8: 0.797  loss_dice_8: 4.436  loss_mars: 0.7152    time: 5.6435  last_time: 6.1811  data_time: 0.0033  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 13:46:55] d2.utils.events INFO:  eta: 9:05:15  iter: 9759  total_loss: 92.02  loss_ce: 3.069  loss_mask: 1.024  loss_dice: 4.486  loss_ce_0: 3.357  loss_mask_0: 0.9224  loss_dice_0: 4.637  loss_ce_1: 3.299  loss_mask_1: 0.9173  loss_dice_1: 4.389  loss_ce_2: 3.289  loss_mask_2: 0.9488  loss_dice_2: 4.38  loss_ce_3: 3.188  loss_mask_3: 1.071  loss_dice_3: 4.524  loss_ce_4: 3.124  loss_mask_4: 0.8119  loss_dice_4: 4.29  loss_ce_5: 3.299  loss_mask_5: 0.997  loss_dice_5: 4.415  loss_ce_6: 3.275  loss_mask_6: 1.068  loss_dice_6: 4.362  loss_ce_7: 3.474  loss_mask_7: 0.9979  loss_dice_7: 4.296  loss_ce_8: 3.332  loss_mask_8: 0.9176  loss_dice_8: 4.237  loss_mars: 0.6167    time: 5.6429  last_time: 5.3778  data_time: 0.0027  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 13:48:48] d2.utils.events INFO:  eta: 9:03:52  iter: 9779  total_loss: 85.25  loss_ce: 2.598  loss_mask: 0.9104  loss_dice: 4.535  loss_ce_0: 3.395  loss_mask_0: 0.956  loss_dice_0: 4.522  loss_ce_1: 2.896  loss_mask_1: 0.9807  loss_dice_1: 4.512  loss_ce_2: 2.321  loss_mask_2: 1.054  loss_dice_2: 4.581  loss_ce_3: 2.507  loss_mask_3: 1.069  loss_dice_3: 4.507  loss_ce_4: 2.625  loss_mask_4: 0.8682  loss_dice_4: 4.528  loss_ce_5: 2.946  loss_mask_5: 1.073  loss_dice_5: 4.51  loss_ce_6: 2.728  loss_mask_6: 0.9461  loss_dice_6: 4.755  loss_ce_7: 3.209  loss_mask_7: 0.9243  loss_dice_7: 4.437  loss_ce_8: 2.684  loss_mask_8: 0.8999  loss_dice_8: 4.484  loss_mars: 0.5724    time: 5.6429  last_time: 5.0653  data_time: 0.0029  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 13:50:40] d2.utils.events INFO:  eta: 9:00:10  iter: 9799  total_loss: 94  loss_ce: 3.199  loss_mask: 0.9666  loss_dice: 4.198  loss_ce_0: 3.784  loss_mask_0: 0.9819  loss_dice_0: 4.286  loss_ce_1: 3.721  loss_mask_1: 1.073  loss_dice_1: 4.492  loss_ce_2: 3.028  loss_mask_2: 0.9691  loss_dice_2: 4.236  loss_ce_3: 3.3  loss_mask_3: 1.066  loss_dice_3: 4.302  loss_ce_4: 3.139  loss_mask_4: 1.169  loss_dice_4: 4.398  loss_ce_5: 3.533  loss_mask_5: 1.064  loss_dice_5: 4.208  loss_ce_6: 3.476  loss_mask_6: 0.982  loss_dice_6: 4.205  loss_ce_7: 3.387  loss_mask_7: 1.147  loss_dice_7: 4.27  loss_ce_8: 3.044  loss_mask_8: 1.097  loss_dice_8: 4.27  loss_mars: 0.7367    time: 5.6429  last_time: 5.1958  data_time: 0.0034  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 13:52:32] d2.utils.events INFO:  eta: 8:58:39  iter: 9819  total_loss: 95.55  loss_ce: 3.286  loss_mask: 1.126  loss_dice: 4.349  loss_ce_0: 3.659  loss_mask_0: 0.8851  loss_dice_0: 4.043  loss_ce_1: 3.254  loss_mask_1: 0.9199  loss_dice_1: 4.397  loss_ce_2: 3.489  loss_mask_2: 0.9156  loss_dice_2: 4.031  loss_ce_3: 3.575  loss_mask_3: 0.8854  loss_dice_3: 4.111  loss_ce_4: 3.649  loss_mask_4: 0.9113  loss_dice_4: 3.991  loss_ce_5: 3.668  loss_mask_5: 0.8837  loss_dice_5: 4.286  loss_ce_6: 3.612  loss_mask_6: 0.926  loss_dice_6: 4.409  loss_ce_7: 3.348  loss_mask_7: 1.015  loss_dice_7: 4.441  loss_ce_8: 3.227  loss_mask_8: 1.18  loss_dice_8: 4.4  loss_mars: 0.7558    time: 5.6427  last_time: 5.1504  data_time: 0.0027  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/19 13:54:22] d2.utils.events INFO:  eta: 8:55:40  iter: 9839  total_loss: 90.97  loss_ce: 2.771  loss_mask: 1.601  loss_dice: 4.242  loss_ce_0: 2.984  loss_mask_0: 1.393  loss_dice_0: 4.312  loss_ce_1: 2.812  loss_mask_1: 1.33  loss_dice_1: 4.328  loss_ce_2: 2.698  loss_mask_2: 1.567  loss_dice_2: 4.272  loss_ce_3: 2.856  loss_mask_3: 1.475  loss_dice_3: 4.356  loss_ce_4: 2.91  loss_mask_4: 2.016  loss_dice_4: 4.239  loss_ce_5: 3.46  loss_mask_5: 1.338  loss_dice_5: 4.286  loss_ce_6: 3.517  loss_mask_6: 1.483  loss_dice_6: 4.318  loss_ce_7: 2.8  loss_mask_7: 1.542  loss_dice_7: 4.278  loss_ce_8: 2.74  loss_mask_8: 1.515  loss_dice_8: 4.241  loss_mars: 0.5016    time: 5.6423  last_time: 6.1063  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 13:56:10] d2.utils.events INFO:  eta: 8:50:30  iter: 9859  total_loss: 98.23  loss_ce: 2.819  loss_mask: 2.878  loss_dice: 4.145  loss_ce_0: 3.008  loss_mask_0: 1.693  loss_dice_0: 4.131  loss_ce_1: 2.862  loss_mask_1: 1.646  loss_dice_1: 4.186  loss_ce_2: 2.794  loss_mask_2: 1.665  loss_dice_2: 4.18  loss_ce_3: 3.286  loss_mask_3: 1.763  loss_dice_3: 4.076  loss_ce_4: 2.918  loss_mask_4: 1.823  loss_dice_4: 4.286  loss_ce_5: 2.997  loss_mask_5: 1.573  loss_dice_5: 3.95  loss_ce_6: 2.766  loss_mask_6: 1.974  loss_dice_6: 4.144  loss_ce_7: 2.92  loss_mask_7: 1.847  loss_dice_7: 4.087  loss_ce_8: 2.825  loss_mask_8: 2.732  loss_dice_8: 4.124  loss_mars: 0.5739    time: 5.6418  last_time: 5.1377  data_time: 0.0027  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 13:58:07] d2.utils.events INFO:  eta: 8:51:57  iter: 9879  total_loss: 92.86  loss_ce: 3  loss_mask: 1.644  loss_dice: 4.31  loss_ce_0: 3.073  loss_mask_0: 1.123  loss_dice_0: 4.474  loss_ce_1: 2.829  loss_mask_1: 1.183  loss_dice_1: 4.582  loss_ce_2: 2.921  loss_mask_2: 1.291  loss_dice_2: 4.663  loss_ce_3: 3.377  loss_mask_3: 1.336  loss_dice_3: 4.601  loss_ce_4: 3.09  loss_mask_4: 1.261  loss_dice_4: 4.616  loss_ce_5: 3.044  loss_mask_5: 1.246  loss_dice_5: 4.481  loss_ce_6: 2.725  loss_mask_6: 1.235  loss_dice_6: 4.452  loss_ce_7: 2.867  loss_mask_7: 1.733  loss_dice_7: 4.349  loss_ce_8: 2.867  loss_mask_8: 1.814  loss_dice_8: 4.278  loss_mars: 0.2323    time: 5.6423  last_time: 5.7886  data_time: 0.0030  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 14:00:01] d2.utils.events INFO:  eta: 8:46:49  iter: 9899  total_loss: 96.37  loss_ce: 3.896  loss_mask: 1.028  loss_dice: 4.563  loss_ce_0: 4.303  loss_mask_0: 1.092  loss_dice_0: 4.598  loss_ce_1: 3.709  loss_mask_1: 0.9593  loss_dice_1: 4.519  loss_ce_2: 3.957  loss_mask_2: 1.001  loss_dice_2: 4.624  loss_ce_3: 4.051  loss_mask_3: 1.096  loss_dice_3: 4.508  loss_ce_4: 3.935  loss_mask_4: 0.9882  loss_dice_4: 4.607  loss_ce_5: 3.611  loss_mask_5: 0.9752  loss_dice_5: 4.621  loss_ce_6: 4.186  loss_mask_6: 1.084  loss_dice_6: 4.526  loss_ce_7: 3.419  loss_mask_7: 1.038  loss_dice_7: 4.645  loss_ce_8: 3.828  loss_mask_8: 1.067  loss_dice_8: 4.589  loss_mars: 0.06947    time: 5.6425  last_time: 6.0277  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 14:01:56] d2.utils.events INFO:  eta: 8:44:33  iter: 9919  total_loss: 88.9  loss_ce: 2.731  loss_mask: 1.44  loss_dice: 4.038  loss_ce_0: 2.897  loss_mask_0: 1.282  loss_dice_0: 4.338  loss_ce_1: 2.681  loss_mask_1: 1.339  loss_dice_1: 3.995  loss_ce_2: 2.861  loss_mask_2: 1.4  loss_dice_2: 3.939  loss_ce_3: 2.89  loss_mask_3: 1.404  loss_dice_3: 3.918  loss_ce_4: 2.984  loss_mask_4: 1.38  loss_dice_4: 3.978  loss_ce_5: 3.071  loss_mask_5: 1.357  loss_dice_5: 3.989  loss_ce_6: 2.921  loss_mask_6: 1.423  loss_dice_6: 3.993  loss_ce_7: 2.583  loss_mask_7: 1.476  loss_dice_7: 4.04  loss_ce_8: 2.726  loss_mask_8: 1.406  loss_dice_8: 3.889  loss_mars: 0.8162    time: 5.6427  last_time: 3.8248  data_time: 0.0025  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 14:03:52] d2.utils.events INFO:  eta: 8:44:22  iter: 9939  total_loss: 96.89  loss_ce: 3.716  loss_mask: 1.484  loss_dice: 4.439  loss_ce_0: 3.593  loss_mask_0: 1.236  loss_dice_0: 4.389  loss_ce_1: 3.307  loss_mask_1: 1.404  loss_dice_1: 4.547  loss_ce_2: 3.319  loss_mask_2: 1.323  loss_dice_2: 4.369  loss_ce_3: 3.315  loss_mask_3: 1.214  loss_dice_3: 4.292  loss_ce_4: 3.17  loss_mask_4: 1.225  loss_dice_4: 4.398  loss_ce_5: 3.687  loss_mask_5: 1.314  loss_dice_5: 4.182  loss_ce_6: 3.77  loss_mask_6: 1.25  loss_dice_6: 4.215  loss_ce_7: 3.577  loss_mask_7: 1.428  loss_dice_7: 4.378  loss_ce_8: 3.758  loss_mask_8: 1.397  loss_dice_8: 4.302  loss_mars: 0.5384    time: 5.6430  last_time: 5.0415  data_time: 0.0026  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 14:05:43] d2.utils.events INFO:  eta: 8:41:18  iter: 9959  total_loss: 90.3  loss_ce: 3.156  loss_mask: 1.105  loss_dice: 4.513  loss_ce_0: 3.528  loss_mask_0: 0.9061  loss_dice_0: 3.954  loss_ce_1: 2.905  loss_mask_1: 0.9479  loss_dice_1: 4.15  loss_ce_2: 2.721  loss_mask_2: 1.045  loss_dice_2: 4.352  loss_ce_3: 2.634  loss_mask_3: 1.568  loss_dice_3: 4.373  loss_ce_4: 3.276  loss_mask_4: 1.361  loss_dice_4: 4.443  loss_ce_5: 3.258  loss_mask_5: 1.231  loss_dice_5: 4.11  loss_ce_6: 3.318  loss_mask_6: 1.248  loss_dice_6: 4.519  loss_ce_7: 3.098  loss_mask_7: 1.003  loss_dice_7: 4.427  loss_ce_8: 3.304  loss_mask_8: 1.133  loss_dice_8: 4.469  loss_mars: 0.8368    time: 5.6428  last_time: 4.3526  data_time: 0.0028  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 14:07:38] d2.utils.events INFO:  eta: 8:39:21  iter: 9979  total_loss: 88.1  loss_ce: 3.221  loss_mask: 0.6893  loss_dice: 4.416  loss_ce_0: 3.565  loss_mask_0: 0.6179  loss_dice_0: 4.351  loss_ce_1: 3.26  loss_mask_1: 0.5699  loss_dice_1: 4.559  loss_ce_2: 3.342  loss_mask_2: 0.7179  loss_dice_2: 4.433  loss_ce_3: 3.145  loss_mask_3: 0.9178  loss_dice_3: 4.504  loss_ce_4: 3.068  loss_mask_4: 0.9029  loss_dice_4: 4.505  loss_ce_5: 3.198  loss_mask_5: 0.693  loss_dice_5: 4.42  loss_ce_6: 3.254  loss_mask_6: 0.6382  loss_dice_6: 4.39  loss_ce_7: 3.307  loss_mask_7: 0.692  loss_dice_7: 4.424  loss_ce_8: 3.195  loss_mask_8: 0.6907  loss_dice_8: 4.4  loss_mars: 0.7992    time: 5.6431  last_time: 5.0931  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 14:09:29] d2.utils.events INFO:  eta: 8:37:37  iter: 9999  total_loss: 92.91  loss_ce: 2.944  loss_mask: 1.651  loss_dice: 4.22  loss_ce_0: 2.975  loss_mask_0: 1.532  loss_dice_0: 4.137  loss_ce_1: 2.765  loss_mask_1: 1.726  loss_dice_1: 4.199  loss_ce_2: 2.716  loss_mask_2: 1.511  loss_dice_2: 4.324  loss_ce_3: 2.754  loss_mask_3: 1.537  loss_dice_3: 4.397  loss_ce_4: 2.775  loss_mask_4: 1.848  loss_dice_4: 4.241  loss_ce_5: 3.024  loss_mask_5: 1.967  loss_dice_5: 4.183  loss_ce_6: 3.326  loss_mask_6: 1.639  loss_dice_6: 4.152  loss_ce_7: 2.908  loss_mask_7: 1.803  loss_dice_7: 4.212  loss_ce_8: 2.883  loss_mask_8: 1.8  loss_dice_8: 4.356  loss_mars: 0.1655    time: 5.6428  last_time: 4.2920  data_time: 0.0022  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 14:11:20] d2.utils.events INFO:  eta: 8:35:21  iter: 10019  total_loss: 87.76  loss_ce: 2.394  loss_mask: 1.496  loss_dice: 4.562  loss_ce_0: 2.613  loss_mask_0: 1.466  loss_dice_0: 4.432  loss_ce_1: 2.335  loss_mask_1: 1.378  loss_dice_1: 4.617  loss_ce_2: 2.261  loss_mask_2: 1.418  loss_dice_2: 4.543  loss_ce_3: 2.253  loss_mask_3: 1.4  loss_dice_3: 4.482  loss_ce_4: 2.316  loss_mask_4: 1.26  loss_dice_4: 4.473  loss_ce_5: 2.499  loss_mask_5: 1.377  loss_dice_5: 4.538  loss_ce_6: 2.637  loss_mask_6: 1.319  loss_dice_6: 4.439  loss_ce_7: 2.343  loss_mask_7: 1.463  loss_dice_7: 4.58  loss_ce_8: 2.32  loss_mask_8: 1.581  loss_dice_8: 4.566  loss_mars: 0.3536    time: 5.6427  last_time: 5.7895  data_time: 0.0027  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 14:13:11] d2.utils.events INFO:  eta: 8:30:07  iter: 10039  total_loss: 82.49  loss_ce: 2.454  loss_mask: 1.288  loss_dice: 4.249  loss_ce_0: 3.008  loss_mask_0: 1.039  loss_dice_0: 4.413  loss_ce_1: 2.671  loss_mask_1: 1.188  loss_dice_1: 4.325  loss_ce_2: 2.503  loss_mask_2: 1.185  loss_dice_2: 4.387  loss_ce_3: 2.501  loss_mask_3: 1.167  loss_dice_3: 4.198  loss_ce_4: 2.722  loss_mask_4: 1.094  loss_dice_4: 4.273  loss_ce_5: 2.901  loss_mask_5: 1.157  loss_dice_5: 4.362  loss_ce_6: 3.28  loss_mask_6: 1.349  loss_dice_6: 4.342  loss_ce_7: 2.577  loss_mask_7: 1.316  loss_dice_7: 4.345  loss_ce_8: 2.506  loss_mask_8: 1.344  loss_dice_8: 4.379  loss_mars: 0.1716    time: 5.6424  last_time: 5.1653  data_time: 0.0029  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 14:15:03] d2.utils.events INFO:  eta: 8:27:57  iter: 10059  total_loss: 92.49  loss_ce: 3.224  loss_mask: 1.451  loss_dice: 4.285  loss_ce_0: 3.708  loss_mask_0: 1.308  loss_dice_0: 4.355  loss_ce_1: 3.773  loss_mask_1: 1.447  loss_dice_1: 4.344  loss_ce_2: 3.346  loss_mask_2: 1.36  loss_dice_2: 4.444  loss_ce_3: 3.571  loss_mask_3: 1.441  loss_dice_3: 4.341  loss_ce_4: 3.548  loss_mask_4: 1.556  loss_dice_4: 4.219  loss_ce_5: 3.347  loss_mask_5: 1.599  loss_dice_5: 4.441  loss_ce_6: 3.444  loss_mask_6: 1.668  loss_dice_6: 4.568  loss_ce_7: 3.224  loss_mask_7: 1.377  loss_dice_7: 4.544  loss_ce_8: 3.105  loss_mask_8: 1.354  loss_dice_8: 4.285  loss_mars: 0.4837    time: 5.6423  last_time: 5.5067  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 14:16:52] d2.utils.events INFO:  eta: 8:28:40  iter: 10079  total_loss: 99.36  loss_ce: 3.471  loss_mask: 1.274  loss_dice: 4.535  loss_ce_0: 4.355  loss_mask_0: 1.065  loss_dice_0: 4.509  loss_ce_1: 4.034  loss_mask_1: 1.192  loss_dice_1: 4.46  loss_ce_2: 3.548  loss_mask_2: 1.069  loss_dice_2: 4.386  loss_ce_3: 3.814  loss_mask_3: 1.225  loss_dice_3: 4.589  loss_ce_4: 3.886  loss_mask_4: 1.133  loss_dice_4: 4.516  loss_ce_5: 3.983  loss_mask_5: 1.075  loss_dice_5: 4.474  loss_ce_6: 3.761  loss_mask_6: 1.27  loss_dice_6: 4.446  loss_ce_7: 3.718  loss_mask_7: 1.328  loss_dice_7: 4.559  loss_ce_8: 3.592  loss_mask_8: 1.298  loss_dice_8: 4.565  loss_mars: 0.6959    time: 5.6419  last_time: 5.3855  data_time: 0.0040  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 14:18:44] d2.utils.events INFO:  eta: 8:26:50  iter: 10099  total_loss: 92.48  loss_ce: 3.696  loss_mask: 0.9485  loss_dice: 4.319  loss_ce_0: 3.994  loss_mask_0: 0.884  loss_dice_0: 4.286  loss_ce_1: 3.643  loss_mask_1: 0.8544  loss_dice_1: 4.316  loss_ce_2: 3.327  loss_mask_2: 0.8443  loss_dice_2: 4.285  loss_ce_3: 3.728  loss_mask_3: 0.8371  loss_dice_3: 4.389  loss_ce_4: 3.806  loss_mask_4: 0.8446  loss_dice_4: 4.187  loss_ce_5: 3.661  loss_mask_5: 0.8105  loss_dice_5: 4.32  loss_ce_6: 3.524  loss_mask_6: 0.9278  loss_dice_6: 4.272  loss_ce_7: 3.7  loss_mask_7: 0.8039  loss_dice_7: 4.318  loss_ce_8: 3.699  loss_mask_8: 0.878  loss_dice_8: 4.271  loss_mars: 0.8832    time: 5.6417  last_time: 5.1578  data_time: 0.0034  last_data_time: 0.0092   lr: 0.0001  max_mem: 0M
[10/19 14:20:28] d2.utils.events INFO:  eta: 8:23:51  iter: 10119  total_loss: 93.77  loss_ce: 2.582  loss_mask: 2.046  loss_dice: 4.053  loss_ce_0: 2.958  loss_mask_0: 1.615  loss_dice_0: 4.359  loss_ce_1: 2.847  loss_mask_1: 1.627  loss_dice_1: 4.138  loss_ce_2: 2.699  loss_mask_2: 1.807  loss_dice_2: 4.406  loss_ce_3: 2.708  loss_mask_3: 1.819  loss_dice_3: 4.154  loss_ce_4: 2.695  loss_mask_4: 1.655  loss_dice_4: 4.122  loss_ce_5: 3.089  loss_mask_5: 1.473  loss_dice_5: 4.411  loss_ce_6: 3.074  loss_mask_6: 1.256  loss_dice_6: 4.181  loss_ce_7: 2.636  loss_mask_7: 1.969  loss_dice_7: 4.479  loss_ce_8: 2.599  loss_mask_8: 2.114  loss_dice_8: 4.089  loss_mars: 0.5457    time: 5.6407  last_time: 5.6456  data_time: 0.0025  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 14:22:15] d2.utils.events INFO:  eta: 8:21:07  iter: 10139  total_loss: 88.16  loss_ce: 2.831  loss_mask: 1.131  loss_dice: 4.401  loss_ce_0: 3.272  loss_mask_0: 1.149  loss_dice_0: 4.244  loss_ce_1: 3.12  loss_mask_1: 1.304  loss_dice_1: 4.303  loss_ce_2: 2.652  loss_mask_2: 1.324  loss_dice_2: 4.409  loss_ce_3: 2.607  loss_mask_3: 1.048  loss_dice_3: 4.258  loss_ce_4: 2.925  loss_mask_4: 1.007  loss_dice_4: 4.246  loss_ce_5: 3.057  loss_mask_5: 1.231  loss_dice_5: 4.142  loss_ce_6: 3.049  loss_mask_6: 1.245  loss_dice_6: 4.134  loss_ce_7: 2.567  loss_mask_7: 1.243  loss_dice_7: 4.527  loss_ce_8: 2.855  loss_mask_8: 1.173  loss_dice_8: 4.268  loss_mars: 0.5206    time: 5.6400  last_time: 5.6087  data_time: 0.0028  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 14:23:59] d2.utils.events INFO:  eta: 8:18:50  iter: 10159  total_loss: 99.28  loss_ce: 3.475  loss_mask: 1.276  loss_dice: 4.21  loss_ce_0: 3.166  loss_mask_0: 1.325  loss_dice_0: 4.453  loss_ce_1: 2.772  loss_mask_1: 1.424  loss_dice_1: 4.346  loss_ce_2: 2.725  loss_mask_2: 1.093  loss_dice_2: 4.402  loss_ce_3: 2.638  loss_mask_3: 1.083  loss_dice_3: 4.187  loss_ce_4: 2.695  loss_mask_4: 1.421  loss_dice_4: 4.162  loss_ce_5: 3.15  loss_mask_5: 1.135  loss_dice_5: 4.137  loss_ce_6: 3.404  loss_mask_6: 1.224  loss_dice_6: 4.313  loss_ce_7: 3.091  loss_mask_7: 1.954  loss_dice_7: 4.505  loss_ce_8: 3.561  loss_mask_8: 1.383  loss_dice_8: 4.286  loss_mars: 0.3631    time: 5.6390  last_time: 5.2097  data_time: 0.0024  last_data_time: 0.0035   lr: 0.0001  max_mem: 0M
[10/19 14:25:51] d2.utils.events INFO:  eta: 8:14:44  iter: 10179  total_loss: 90.53  loss_ce: 2.941  loss_mask: 1.808  loss_dice: 4.248  loss_ce_0: 2.878  loss_mask_0: 1.912  loss_dice_0: 3.998  loss_ce_1: 2.621  loss_mask_1: 1.753  loss_dice_1: 3.962  loss_ce_2: 2.439  loss_mask_2: 1.673  loss_dice_2: 3.883  loss_ce_3: 2.329  loss_mask_3: 1.585  loss_dice_3: 4.014  loss_ce_4: 2.475  loss_mask_4: 1.748  loss_dice_4: 3.928  loss_ce_5: 2.529  loss_mask_5: 1.699  loss_dice_5: 4.007  loss_ce_6: 3.596  loss_mask_6: 1.641  loss_dice_6: 4.082  loss_ce_7: 3.61  loss_mask_7: 1.609  loss_dice_7: 3.912  loss_ce_8: 2.849  loss_mask_8: 1.711  loss_dice_8: 4.018  loss_mars: 0.4669    time: 5.6389  last_time: 6.3378  data_time: 0.0032  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 14:27:42] d2.utils.events INFO:  eta: 8:11:19  iter: 10199  total_loss: 85.61  loss_ce: 2.69  loss_mask: 1.487  loss_dice: 4.262  loss_ce_0: 2.975  loss_mask_0: 1.56  loss_dice_0: 4.275  loss_ce_1: 2.536  loss_mask_1: 1.702  loss_dice_1: 4.197  loss_ce_2: 2.433  loss_mask_2: 1.63  loss_dice_2: 4.221  loss_ce_3: 2.361  loss_mask_3: 1.413  loss_dice_3: 4.236  loss_ce_4: 2.465  loss_mask_4: 1.383  loss_dice_4: 3.986  loss_ce_5: 2.849  loss_mask_5: 1.777  loss_dice_5: 4.139  loss_ce_6: 3.023  loss_mask_6: 1.425  loss_dice_6: 4.174  loss_ce_7: 2.644  loss_mask_7: 1.533  loss_dice_7: 4.27  loss_ce_8: 2.723  loss_mask_8: 1.564  loss_dice_8: 4.241  loss_mars: 0.4726    time: 5.6387  last_time: 4.3257  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 14:29:31] d2.utils.events INFO:  eta: 8:10:19  iter: 10219  total_loss: 92.17  loss_ce: 3.883  loss_mask: 0.8688  loss_dice: 4.256  loss_ce_0: 4.121  loss_mask_0: 0.9062  loss_dice_0: 4.375  loss_ce_1: 3.407  loss_mask_1: 1.004  loss_dice_1: 4.33  loss_ce_2: 3.148  loss_mask_2: 0.8986  loss_dice_2: 4.147  loss_ce_3: 3.219  loss_mask_3: 1.236  loss_dice_3: 4.445  loss_ce_4: 3.617  loss_mask_4: 1.049  loss_dice_4: 4.24  loss_ce_5: 3.236  loss_mask_5: 1.017  loss_dice_5: 4.238  loss_ce_6: 3.685  loss_mask_6: 0.9253  loss_dice_6: 4.281  loss_ce_7: 4.089  loss_mask_7: 0.9606  loss_dice_7: 4.25  loss_ce_8: 3.979  loss_mask_8: 0.9625  loss_dice_8: 4.243  loss_mars: 0.4566    time: 5.6383  last_time: 6.1801  data_time: 0.0028  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 14:31:21] d2.utils.events INFO:  eta: 8:07:22  iter: 10239  total_loss: 89.63  loss_ce: 3.003  loss_mask: 1.184  loss_dice: 4.202  loss_ce_0: 3.568  loss_mask_0: 1.282  loss_dice_0: 4.295  loss_ce_1: 3.222  loss_mask_1: 1.222  loss_dice_1: 4.263  loss_ce_2: 2.778  loss_mask_2: 1.354  loss_dice_2: 4.172  loss_ce_3: 3.061  loss_mask_3: 1.542  loss_dice_3: 4.257  loss_ce_4: 3.005  loss_mask_4: 1.18  loss_dice_4: 4.207  loss_ce_5: 2.985  loss_mask_5: 1.11  loss_dice_5: 4.064  loss_ce_6: 3.192  loss_mask_6: 0.9859  loss_dice_6: 4.149  loss_ce_7: 3.201  loss_mask_7: 1.16  loss_dice_7: 3.98  loss_ce_8: 3.022  loss_mask_8: 1.22  loss_dice_8: 4.18  loss_mars: 0.3116    time: 5.6379  last_time: 4.9226  data_time: 0.0028  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 14:33:15] d2.utils.events INFO:  eta: 8:07:17  iter: 10259  total_loss: 89.35  loss_ce: 2.669  loss_mask: 1.93  loss_dice: 4.2  loss_ce_0: 2.979  loss_mask_0: 2.196  loss_dice_0: 3.888  loss_ce_1: 2.65  loss_mask_1: 2.185  loss_dice_1: 3.995  loss_ce_2: 2.41  loss_mask_2: 2.114  loss_dice_2: 4.06  loss_ce_3: 2.468  loss_mask_3: 1.941  loss_dice_3: 4.141  loss_ce_4: 2.541  loss_mask_4: 2.16  loss_dice_4: 4.125  loss_ce_5: 2.395  loss_mask_5: 2.183  loss_dice_5: 4.158  loss_ce_6: 2.67  loss_mask_6: 2.171  loss_dice_6: 4.285  loss_ce_7: 2.786  loss_mask_7: 2.192  loss_dice_7: 4.41  loss_ce_8: 2.749  loss_mask_8: 1.982  loss_dice_8: 4.317  loss_mars: 0.6997    time: 5.6381  last_time: 6.1359  data_time: 0.0029  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 14:35:06] d2.utils.events INFO:  eta: 8:04:05  iter: 10279  total_loss: 108.3  loss_ce: 3.85  loss_mask: 1.344  loss_dice: 4.77  loss_ce_0: 4.496  loss_mask_0: 0.9179  loss_dice_0: 4.563  loss_ce_1: 3.664  loss_mask_1: 1.004  loss_dice_1: 4.532  loss_ce_2: 3.307  loss_mask_2: 1.195  loss_dice_2: 4.591  loss_ce_3: 3.379  loss_mask_3: 1.185  loss_dice_3: 4.575  loss_ce_4: 3.586  loss_mask_4: 1.132  loss_dice_4: 4.582  loss_ce_5: 3.752  loss_mask_5: 0.9643  loss_dice_5: 4.56  loss_ce_6: 3.706  loss_mask_6: 1.312  loss_dice_6: 4.802  loss_ce_7: 3.856  loss_mask_7: 1.001  loss_dice_7: 4.653  loss_ce_8: 3.497  loss_mask_8: 1.334  loss_dice_8: 4.739  loss_mars: 0.3149    time: 5.6379  last_time: 5.0084  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 14:36:57] d2.utils.events INFO:  eta: 8:06:01  iter: 10299  total_loss: 101.9  loss_ce: 3.428  loss_mask: 1.268  loss_dice: 4.507  loss_ce_0: 3.254  loss_mask_0: 0.8286  loss_dice_0: 4.515  loss_ce_1: 3.051  loss_mask_1: 0.9516  loss_dice_1: 4.459  loss_ce_2: 2.877  loss_mask_2: 1.116  loss_dice_2: 4.47  loss_ce_3: 2.919  loss_mask_3: 1.076  loss_dice_3: 4.475  loss_ce_4: 2.98  loss_mask_4: 1.154  loss_dice_4: 4.311  loss_ce_5: 2.904  loss_mask_5: 1.326  loss_dice_5: 4.26  loss_ce_6: 3  loss_mask_6: 1.414  loss_dice_6: 4.557  loss_ce_7: 3.341  loss_mask_7: 1.1  loss_dice_7: 4.483  loss_ce_8: 3.098  loss_mask_8: 1.257  loss_dice_8: 4.537  loss_mars: 0.4811    time: 5.6376  last_time: 5.7721  data_time: 0.0028  last_data_time: 0.0034   lr: 0.0001  max_mem: 0M
[10/19 14:38:47] d2.utils.events INFO:  eta: 8:02:01  iter: 10319  total_loss: 88.64  loss_ce: 3.226  loss_mask: 1.691  loss_dice: 4.154  loss_ce_0: 2.698  loss_mask_0: 1.597  loss_dice_0: 4.012  loss_ce_1: 2.804  loss_mask_1: 1.541  loss_dice_1: 3.973  loss_ce_2: 2.669  loss_mask_2: 1.535  loss_dice_2: 4.125  loss_ce_3: 2.816  loss_mask_3: 1.522  loss_dice_3: 3.96  loss_ce_4: 2.729  loss_mask_4: 1.512  loss_dice_4: 4.144  loss_ce_5: 2.866  loss_mask_5: 1.635  loss_dice_5: 4.144  loss_ce_6: 2.911  loss_mask_6: 1.637  loss_dice_6: 4.194  loss_ce_7: 3.025  loss_mask_7: 1.661  loss_dice_7: 4.149  loss_ce_8: 2.827  loss_mask_8: 1.778  loss_dice_8: 4.154  loss_mars: 0.6897    time: 5.6373  last_time: 5.2033  data_time: 0.0024  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 14:40:32] d2.utils.events INFO:  eta: 8:01:58  iter: 10339  total_loss: 83.72  loss_ce: 2.271  loss_mask: 2.301  loss_dice: 3.76  loss_ce_0: 2.24  loss_mask_0: 2.333  loss_dice_0: 3.943  loss_ce_1: 2.52  loss_mask_1: 2.442  loss_dice_1: 4.383  loss_ce_2: 2.199  loss_mask_2: 2.382  loss_dice_2: 4.263  loss_ce_3: 2.169  loss_mask_3: 2.247  loss_dice_3: 4.003  loss_ce_4: 2.096  loss_mask_4: 2.46  loss_dice_4: 3.928  loss_ce_5: 2.21  loss_mask_5: 2.096  loss_dice_5: 3.794  loss_ce_6: 2.224  loss_mask_6: 2.211  loss_dice_6: 3.884  loss_ce_7: 2.298  loss_mask_7: 2.399  loss_dice_7: 3.731  loss_ce_8: 2.159  loss_mask_8: 2.413  loss_dice_8: 3.853  loss_mars: 0.6904    time: 5.6365  last_time: 5.0470  data_time: 0.0029  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 14:42:19] d2.utils.events INFO:  eta: 7:58:12  iter: 10359  total_loss: 92.35  loss_ce: 3.345  loss_mask: 1.42  loss_dice: 4.295  loss_ce_0: 3.525  loss_mask_0: 1.059  loss_dice_0: 4.038  loss_ce_1: 3.579  loss_mask_1: 0.8995  loss_dice_1: 4.043  loss_ce_2: 3.782  loss_mask_2: 0.93  loss_dice_2: 4.006  loss_ce_3: 3.645  loss_mask_3: 0.8611  loss_dice_3: 4.069  loss_ce_4: 3.679  loss_mask_4: 1.001  loss_dice_4: 3.901  loss_ce_5: 3.581  loss_mask_5: 0.9362  loss_dice_5: 3.887  loss_ce_6: 3.438  loss_mask_6: 1.005  loss_dice_6: 4.036  loss_ce_7: 3.272  loss_mask_7: 1.164  loss_dice_7: 4.137  loss_ce_8: 3.275  loss_mask_8: 1.386  loss_dice_8: 4.479  loss_mars: 0.7465    time: 5.6358  last_time: 4.9670  data_time: 0.0025  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 14:44:09] d2.utils.events INFO:  eta: 7:55:48  iter: 10379  total_loss: 90.44  loss_ce: 2.715  loss_mask: 1.263  loss_dice: 4.387  loss_ce_0: 3.127  loss_mask_0: 1.35  loss_dice_0: 4.359  loss_ce_1: 3.151  loss_mask_1: 1.149  loss_dice_1: 4.368  loss_ce_2: 2.991  loss_mask_2: 1.343  loss_dice_2: 4.306  loss_ce_3: 2.907  loss_mask_3: 1.394  loss_dice_3: 4.254  loss_ce_4: 2.81  loss_mask_4: 1.245  loss_dice_4: 4.5  loss_ce_5: 2.777  loss_mask_5: 1.252  loss_dice_5: 4.33  loss_ce_6: 2.867  loss_mask_6: 1.319  loss_dice_6: 4.286  loss_ce_7: 2.759  loss_mask_7: 1.399  loss_dice_7: 4.307  loss_ce_8: 2.543  loss_mask_8: 1.31  loss_dice_8: 4.394  loss_mars: 0.4586    time: 5.6354  last_time: 5.7756  data_time: 0.0028  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 14:45:56] d2.utils.events INFO:  eta: 7:53:12  iter: 10399  total_loss: 89.92  loss_ce: 2.933  loss_mask: 1.406  loss_dice: 4.139  loss_ce_0: 3.143  loss_mask_0: 1.402  loss_dice_0: 3.979  loss_ce_1: 3.115  loss_mask_1: 1.26  loss_dice_1: 3.933  loss_ce_2: 3.263  loss_mask_2: 1.38  loss_dice_2: 3.96  loss_ce_3: 3.265  loss_mask_3: 1.509  loss_dice_3: 3.932  loss_ce_4: 3.224  loss_mask_4: 1.466  loss_dice_4: 4.005  loss_ce_5: 3.256  loss_mask_5: 1.323  loss_dice_5: 4.031  loss_ce_6: 3.116  loss_mask_6: 1.279  loss_dice_6: 3.967  loss_ce_7: 2.993  loss_mask_7: 1.51  loss_dice_7: 3.83  loss_ce_8: 2.891  loss_mask_8: 1.392  loss_dice_8: 3.792  loss_mars: 0.5289    time: 5.6348  last_time: 5.1399  data_time: 0.0028  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 14:47:47] d2.utils.events INFO:  eta: 7:50:53  iter: 10419  total_loss: 83.72  loss_ce: 3.015  loss_mask: 1.905  loss_dice: 4.303  loss_ce_0: 2.784  loss_mask_0: 1.364  loss_dice_0: 4.086  loss_ce_1: 3.091  loss_mask_1: 1.31  loss_dice_1: 4.014  loss_ce_2: 2.804  loss_mask_2: 1.29  loss_dice_2: 3.98  loss_ce_3: 2.859  loss_mask_3: 1.348  loss_dice_3: 4.117  loss_ce_4: 2.915  loss_mask_4: 1.439  loss_dice_4: 4.096  loss_ce_5: 2.786  loss_mask_5: 1.478  loss_dice_5: 4.079  loss_ce_6: 2.884  loss_mask_6: 1.523  loss_dice_6: 4.059  loss_ce_7: 2.55  loss_mask_7: 1.579  loss_dice_7: 4.158  loss_ce_8: 2.833  loss_mask_8: 2.524  loss_dice_8: 4.469  loss_mars: 0.527    time: 5.6347  last_time: 6.2393  data_time: 0.0021  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 14:49:38] d2.utils.events INFO:  eta: 7:50:56  iter: 10439  total_loss: 101.2  loss_ce: 3.964  loss_mask: 1.015  loss_dice: 4.567  loss_ce_0: 3.589  loss_mask_0: 1.098  loss_dice_0: 4.514  loss_ce_1: 3.663  loss_mask_1: 1.132  loss_dice_1: 4.507  loss_ce_2: 3.34  loss_mask_2: 1.133  loss_dice_2: 4.574  loss_ce_3: 3.396  loss_mask_3: 1.117  loss_dice_3: 4.584  loss_ce_4: 3.463  loss_mask_4: 0.9174  loss_dice_4: 4.542  loss_ce_5: 3.437  loss_mask_5: 1.156  loss_dice_5: 4.555  loss_ce_6: 3.554  loss_mask_6: 1.556  loss_dice_6: 4.529  loss_ce_7: 3.579  loss_mask_7: 1.017  loss_dice_7: 4.578  loss_ce_8: 3.914  loss_mask_8: 1.027  loss_dice_8: 4.483  loss_mars: 0.606    time: 5.6345  last_time: 5.7264  data_time: 0.0030  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 14:51:32] d2.utils.events INFO:  eta: 7:49:07  iter: 10459  total_loss: 96.5  loss_ce: 3.612  loss_mask: 1.814  loss_dice: 4.559  loss_ce_0: 3.796  loss_mask_0: 1.611  loss_dice_0: 4.404  loss_ce_1: 4.176  loss_mask_1: 1.441  loss_dice_1: 4.208  loss_ce_2: 4.124  loss_mask_2: 1.526  loss_dice_2: 4.503  loss_ce_3: 3.654  loss_mask_3: 1.567  loss_dice_3: 4.476  loss_ce_4: 3.823  loss_mask_4: 1.922  loss_dice_4: 4.375  loss_ce_5: 3.878  loss_mask_5: 2.084  loss_dice_5: 4.336  loss_ce_6: 3.814  loss_mask_6: 1.741  loss_dice_6: 4.366  loss_ce_7: 3.859  loss_mask_7: 2.206  loss_dice_7: 4.553  loss_ce_8: 3.888  loss_mask_8: 1.895  loss_dice_8: 4.456  loss_mars: 0.4783    time: 5.6346  last_time: 6.0872  data_time: 0.0032  last_data_time: 0.0089   lr: 0.0001  max_mem: 0M
[10/19 14:53:17] d2.utils.events INFO:  eta: 7:45:27  iter: 10479  total_loss: 109.8  loss_ce: 5.043  loss_mask: 1.115  loss_dice: 4.543  loss_ce_0: 5.97  loss_mask_0: 1.172  loss_dice_0: 4.576  loss_ce_1: 5.038  loss_mask_1: 1.092  loss_dice_1: 4.627  loss_ce_2: 5.168  loss_mask_2: 1.283  loss_dice_2: 4.62  loss_ce_3: 5.216  loss_mask_3: 1.216  loss_dice_3: 4.517  loss_ce_4: 5.063  loss_mask_4: 1.164  loss_dice_4: 4.69  loss_ce_5: 5.116  loss_mask_5: 1.194  loss_dice_5: 4.57  loss_ce_6: 5.318  loss_mask_6: 1.125  loss_dice_6: 4.568  loss_ce_7: 5.082  loss_mask_7: 1.092  loss_dice_7: 4.697  loss_ce_8: 5.271  loss_mask_8: 1.077  loss_dice_8: 4.522  loss_mars: 0.8494    time: 5.6338  last_time: 5.0140  data_time: 0.0030  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 14:55:08] d2.utils.events INFO:  eta: 7:45:29  iter: 10499  total_loss: 95.19  loss_ce: 3.542  loss_mask: 0.9396  loss_dice: 4.542  loss_ce_0: 3.777  loss_mask_0: 0.8669  loss_dice_0: 4.559  loss_ce_1: 3.603  loss_mask_1: 0.801  loss_dice_1: 4.582  loss_ce_2: 3.746  loss_mask_2: 0.8261  loss_dice_2: 4.593  loss_ce_3: 3.619  loss_mask_3: 0.895  loss_dice_3: 4.544  loss_ce_4: 3.904  loss_mask_4: 0.9172  loss_dice_4: 4.507  loss_ce_5: 3.604  loss_mask_5: 0.8762  loss_dice_5: 4.389  loss_ce_6: 3.754  loss_mask_6: 0.8265  loss_dice_6: 4.532  loss_ce_7: 3.551  loss_mask_7: 0.9196  loss_dice_7: 4.524  loss_ce_8: 3.701  loss_mask_8: 0.9645  loss_dice_8: 4.566  loss_mars: 0.526    time: 5.6336  last_time: 8.0130  data_time: 0.0031  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 14:56:56] d2.utils.events INFO:  eta: 7:40:59  iter: 10519  total_loss: 100.9  loss_ce: 4.053  loss_mask: 1.587  loss_dice: 4.477  loss_ce_0: 3.487  loss_mask_0: 1.274  loss_dice_0: 4.512  loss_ce_1: 3.763  loss_mask_1: 1.536  loss_dice_1: 4.504  loss_ce_2: 3.796  loss_mask_2: 1.135  loss_dice_2: 4.472  loss_ce_3: 3.945  loss_mask_3: 1.593  loss_dice_3: 4.499  loss_ce_4: 3.792  loss_mask_4: 1.659  loss_dice_4: 4.412  loss_ce_5: 3.694  loss_mask_5: 1.695  loss_dice_5: 4.542  loss_ce_6: 3.808  loss_mask_6: 1.545  loss_dice_6: 4.472  loss_ce_7: 4.138  loss_mask_7: 1.422  loss_dice_7: 4.463  loss_ce_8: 4.185  loss_mask_8: 1.357  loss_dice_8: 4.379  loss_mars: 0.3428    time: 5.6330  last_time: 5.1227  data_time: 0.0026  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 14:58:45] d2.utils.events INFO:  eta: 7:40:05  iter: 10539  total_loss: 94.56  loss_ce: 2.937  loss_mask: 1.077  loss_dice: 4.249  loss_ce_0: 3.573  loss_mask_0: 1.261  loss_dice_0: 4.312  loss_ce_1: 3.575  loss_mask_1: 1.245  loss_dice_1: 4.309  loss_ce_2: 3.521  loss_mask_2: 1.196  loss_dice_2: 4.383  loss_ce_3: 3.177  loss_mask_3: 1.104  loss_dice_3: 4.185  loss_ce_4: 3.072  loss_mask_4: 1.216  loss_dice_4: 4.185  loss_ce_5: 3.095  loss_mask_5: 1.138  loss_dice_5: 4.171  loss_ce_6: 2.977  loss_mask_6: 1.08  loss_dice_6: 4.206  loss_ce_7: 2.788  loss_mask_7: 1.124  loss_dice_7: 4.255  loss_ce_8: 2.947  loss_mask_8: 1.111  loss_dice_8: 4.179  loss_mars: 0.3291    time: 5.6326  last_time: 5.7871  data_time: 0.0037  last_data_time: 0.0082   lr: 0.0001  max_mem: 0M
[10/19 15:00:40] d2.utils.events INFO:  eta: 7:40:13  iter: 10559  total_loss: 93.41  loss_ce: 3.042  loss_mask: 1.833  loss_dice: 4.432  loss_ce_0: 3.116  loss_mask_0: 1.75  loss_dice_0: 4.2  loss_ce_1: 3.524  loss_mask_1: 1.522  loss_dice_1: 4.251  loss_ce_2: 3.148  loss_mask_2: 1.614  loss_dice_2: 4.331  loss_ce_3: 2.914  loss_mask_3: 1.413  loss_dice_3: 4.339  loss_ce_4: 2.649  loss_mask_4: 1.634  loss_dice_4: 4.427  loss_ce_5: 2.856  loss_mask_5: 1.818  loss_dice_5: 4.132  loss_ce_6: 2.835  loss_mask_6: 1.884  loss_dice_6: 4.329  loss_ce_7: 2.771  loss_mask_7: 1.863  loss_dice_7: 4.504  loss_ce_8: 2.864  loss_mask_8: 1.603  loss_dice_8: 4.29  loss_mars: 0.4566    time: 5.6328  last_time: 5.5755  data_time: 0.0028  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 15:02:25] d2.utils.events INFO:  eta: 7:37:39  iter: 10579  total_loss: 90.26  loss_ce: 2.568  loss_mask: 1.272  loss_dice: 4.663  loss_ce_0: 3.058  loss_mask_0: 1.021  loss_dice_0: 4.41  loss_ce_1: 2.745  loss_mask_1: 1.003  loss_dice_1: 4.107  loss_ce_2: 2.427  loss_mask_2: 0.9517  loss_dice_2: 4.173  loss_ce_3: 2.34  loss_mask_3: 1.066  loss_dice_3: 4.51  loss_ce_4: 2.135  loss_mask_4: 1.243  loss_dice_4: 4.583  loss_ce_5: 2.232  loss_mask_5: 1.072  loss_dice_5: 4.25  loss_ce_6: 2.559  loss_mask_6: 1.057  loss_dice_6: 4.428  loss_ce_7: 2.485  loss_mask_7: 1.272  loss_dice_7: 4.714  loss_ce_8: 2.366  loss_mask_8: 1.169  loss_dice_8: 4.685  loss_mars: 0.6237    time: 5.6320  last_time: 4.9890  data_time: 0.0025  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 15:04:14] d2.utils.events INFO:  eta: 7:36:35  iter: 10599  total_loss: 80.78  loss_ce: 2.404  loss_mask: 0.9313  loss_dice: 3.785  loss_ce_0: 2.67  loss_mask_0: 0.9097  loss_dice_0: 3.819  loss_ce_1: 2.413  loss_mask_1: 0.8744  loss_dice_1: 3.872  loss_ce_2: 2.248  loss_mask_2: 0.902  loss_dice_2: 4.159  loss_ce_3: 2.307  loss_mask_3: 0.8465  loss_dice_3: 3.83  loss_ce_4: 2.357  loss_mask_4: 0.8642  loss_dice_4: 3.716  loss_ce_5: 2.343  loss_mask_5: 0.9338  loss_dice_5: 4.029  loss_ce_6: 2.372  loss_mask_6: 0.922  loss_dice_6: 3.833  loss_ce_7: 2.446  loss_mask_7: 0.8892  loss_dice_7: 3.866  loss_ce_8: 2.413  loss_mask_8: 0.8818  loss_dice_8: 3.794  loss_mars: 0.8442    time: 5.6316  last_time: 7.3329  data_time: 0.0028  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 15:06:07] d2.utils.events INFO:  eta: 7:36:49  iter: 10619  total_loss: 89.72  loss_ce: 3.121  loss_mask: 1.614  loss_dice: 4.278  loss_ce_0: 3.553  loss_mask_0: 1.755  loss_dice_0: 4.31  loss_ce_1: 2.995  loss_mask_1: 1.325  loss_dice_1: 4.371  loss_ce_2: 2.81  loss_mask_2: 1.338  loss_dice_2: 4.352  loss_ce_3: 2.82  loss_mask_3: 1.358  loss_dice_3: 4.291  loss_ce_4: 2.73  loss_mask_4: 1.286  loss_dice_4: 4.234  loss_ce_5: 2.846  loss_mask_5: 1.454  loss_dice_5: 4.238  loss_ce_6: 2.966  loss_mask_6: 1.474  loss_dice_6: 4.165  loss_ce_7: 2.922  loss_mask_7: 1.409  loss_dice_7: 4.231  loss_ce_8: 2.962  loss_mask_8: 1.484  loss_dice_8: 4.341  loss_mars: 0.6293    time: 5.6317  last_time: 5.7467  data_time: 0.0024  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 15:07:54] d2.utils.events INFO:  eta: 7:36:51  iter: 10639  total_loss: 91.13  loss_ce: 3.48  loss_mask: 0.8075  loss_dice: 4.263  loss_ce_0: 3.905  loss_mask_0: 0.7936  loss_dice_0: 4.331  loss_ce_1: 3.316  loss_mask_1: 0.7471  loss_dice_1: 4.365  loss_ce_2: 3.386  loss_mask_2: 0.7744  loss_dice_2: 4.284  loss_ce_3: 3.51  loss_mask_3: 0.7598  loss_dice_3: 4.172  loss_ce_4: 3.325  loss_mask_4: 0.8086  loss_dice_4: 4.317  loss_ce_5: 3.409  loss_mask_5: 0.8679  loss_dice_5: 4.198  loss_ce_6: 3.518  loss_mask_6: 0.7461  loss_dice_6: 4.327  loss_ce_7: 3.49  loss_mask_7: 0.8016  loss_dice_7: 4.261  loss_ce_8: 3.52  loss_mask_8: 0.7812  loss_dice_8: 4.301  loss_mars: 0.3356    time: 5.6310  last_time: 5.2195  data_time: 0.0033  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 15:09:49] d2.utils.events INFO:  eta: 7:36:35  iter: 10659  total_loss: 99.21  loss_ce: 3.739  loss_mask: 1.399  loss_dice: 4.472  loss_ce_0: 4.358  loss_mask_0: 1.339  loss_dice_0: 4.319  loss_ce_1: 3.768  loss_mask_1: 1.209  loss_dice_1: 4.436  loss_ce_2: 3.81  loss_mask_2: 1.246  loss_dice_2: 4.494  loss_ce_3: 3.826  loss_mask_3: 1.305  loss_dice_3: 4.465  loss_ce_4: 3.596  loss_mask_4: 1.401  loss_dice_4: 4.514  loss_ce_5: 3.792  loss_mask_5: 1.219  loss_dice_5: 4.533  loss_ce_6: 3.731  loss_mask_6: 1.259  loss_dice_6: 4.467  loss_ce_7: 3.677  loss_mask_7: 1.314  loss_dice_7: 4.483  loss_ce_8: 3.719  loss_mask_8: 1.273  loss_dice_8: 4.576  loss_mars: 0.5844    time: 5.6313  last_time: 3.8003  data_time: 0.0028  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 15:11:38] d2.utils.events INFO:  eta: 7:34:44  iter: 10679  total_loss: 78.54  loss_ce: 2.447  loss_mask: 0.9876  loss_dice: 4.542  loss_ce_0: 2.141  loss_mask_0: 0.8888  loss_dice_0: 4.31  loss_ce_1: 2.26  loss_mask_1: 0.8866  loss_dice_1: 4.592  loss_ce_2: 2.204  loss_mask_2: 1.02  loss_dice_2: 4.51  loss_ce_3: 2.31  loss_mask_3: 0.8923  loss_dice_3: 4.413  loss_ce_4: 2.205  loss_mask_4: 0.872  loss_dice_4: 4.375  loss_ce_5: 2.348  loss_mask_5: 0.9454  loss_dice_5: 4.484  loss_ce_6: 2.252  loss_mask_6: 1.531  loss_dice_6: 4.648  loss_ce_7: 2.396  loss_mask_7: 1.304  loss_dice_7: 4.526  loss_ce_8: 2.405  loss_mask_8: 1.005  loss_dice_8: 4.516  loss_mars: 0.5241    time: 5.6309  last_time: 5.8592  data_time: 0.0027  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 15:13:29] d2.utils.events INFO:  eta: 7:31:21  iter: 10699  total_loss: 102.9  loss_ce: 4.855  loss_mask: 0.7433  loss_dice: 4.277  loss_ce_0: 4.259  loss_mask_0: 0.7858  loss_dice_0: 4.582  loss_ce_1: 5.05  loss_mask_1: 0.9357  loss_dice_1: 4.287  loss_ce_2: 4.889  loss_mask_2: 0.719  loss_dice_2: 4.441  loss_ce_3: 4.868  loss_mask_3: 0.9356  loss_dice_3: 4.46  loss_ce_4: 4.443  loss_mask_4: 0.7606  loss_dice_4: 4.651  loss_ce_5: 4.398  loss_mask_5: 0.7436  loss_dice_5: 4.714  loss_ce_6: 4.668  loss_mask_6: 0.6862  loss_dice_6: 4.601  loss_ce_7: 4.907  loss_mask_7: 0.797  loss_dice_7: 4.371  loss_ce_8: 5.038  loss_mask_8: 0.7187  loss_dice_8: 4.243  loss_mars: 0.4266    time: 5.6307  last_time: 4.9480  data_time: 0.0043  last_data_time: 0.0047   lr: 0.0001  max_mem: 0M
[10/19 15:15:19] d2.utils.events INFO:  eta: 7:28:02  iter: 10719  total_loss: 84.21  loss_ce: 2.971  loss_mask: 1.035  loss_dice: 4.296  loss_ce_0: 2.814  loss_mask_0: 0.7881  loss_dice_0: 4.246  loss_ce_1: 2.972  loss_mask_1: 1.11  loss_dice_1: 3.951  loss_ce_2: 3.016  loss_mask_2: 0.8692  loss_dice_2: 3.875  loss_ce_3: 3.101  loss_mask_3: 1.039  loss_dice_3: 4.111  loss_ce_4: 2.964  loss_mask_4: 1.121  loss_dice_4: 3.961  loss_ce_5: 2.877  loss_mask_5: 1.042  loss_dice_5: 4.282  loss_ce_6: 3.1  loss_mask_6: 0.8308  loss_dice_6: 4.157  loss_ce_7: 3.026  loss_mask_7: 0.9407  loss_dice_7: 3.868  loss_ce_8: 3  loss_mask_8: 1.144  loss_dice_8: 4.1  loss_mars: 0.6809    time: 5.6304  last_time: 5.8982  data_time: 0.0025  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 15:17:10] d2.utils.events INFO:  eta: 7:23:51  iter: 10739  total_loss: 88.5  loss_ce: 3.008  loss_mask: 1.413  loss_dice: 4.263  loss_ce_0: 2.823  loss_mask_0: 1.323  loss_dice_0: 4.322  loss_ce_1: 3.055  loss_mask_1: 1.268  loss_dice_1: 4.267  loss_ce_2: 2.961  loss_mask_2: 1.357  loss_dice_2: 4.189  loss_ce_3: 2.793  loss_mask_3: 1.345  loss_dice_3: 4.209  loss_ce_4: 2.664  loss_mask_4: 1.245  loss_dice_4: 4.222  loss_ce_5: 2.804  loss_mask_5: 1.333  loss_dice_5: 4.219  loss_ce_6: 2.897  loss_mask_6: 1.316  loss_dice_6: 4.324  loss_ce_7: 3.058  loss_mask_7: 1.373  loss_dice_7: 4.345  loss_ce_8: 3.041  loss_mask_8: 1.437  loss_dice_8: 4.25  loss_mars: 0.6314    time: 5.6302  last_time: 5.0678  data_time: 0.0027  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 15:19:02] d2.utils.events INFO:  eta: 7:22:10  iter: 10759  total_loss: 88.71  loss_ce: 3.111  loss_mask: 1.361  loss_dice: 4.451  loss_ce_0: 3.217  loss_mask_0: 1.269  loss_dice_0: 4.304  loss_ce_1: 3.71  loss_mask_1: 1.165  loss_dice_1: 4.331  loss_ce_2: 3.317  loss_mask_2: 1.231  loss_dice_2: 4.352  loss_ce_3: 3.121  loss_mask_3: 1.324  loss_dice_3: 4.388  loss_ce_4: 3.091  loss_mask_4: 1.331  loss_dice_4: 4.368  loss_ce_5: 3.267  loss_mask_5: 1.391  loss_dice_5: 4.384  loss_ce_6: 3.201  loss_mask_6: 1.359  loss_dice_6: 4.317  loss_ce_7: 3.172  loss_mask_7: 1.278  loss_dice_7: 4.481  loss_ce_8: 3.267  loss_mask_8: 1.412  loss_dice_8: 4.459  loss_mars: 0.7596    time: 5.6302  last_time: 5.9522  data_time: 0.0031  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 15:20:49] d2.utils.events INFO:  eta: 7:19:31  iter: 10779  total_loss: 91.61  loss_ce: 3.788  loss_mask: 0.7003  loss_dice: 4.424  loss_ce_0: 4.521  loss_mask_0: 0.5817  loss_dice_0: 4.347  loss_ce_1: 4.015  loss_mask_1: 0.5872  loss_dice_1: 4.408  loss_ce_2: 3.671  loss_mask_2: 0.6405  loss_dice_2: 4.567  loss_ce_3: 3.777  loss_mask_3: 0.7301  loss_dice_3: 4.536  loss_ce_4: 3.696  loss_mask_4: 0.5768  loss_dice_4: 4.413  loss_ce_5: 4.027  loss_mask_5: 0.6323  loss_dice_5: 4.39  loss_ce_6: 3.901  loss_mask_6: 0.6071  loss_dice_6: 4.533  loss_ce_7: 3.618  loss_mask_7: 0.7154  loss_dice_7: 4.48  loss_ce_8: 3.515  loss_mask_8: 0.7635  loss_dice_8: 4.533  loss_mars: 0.5608    time: 5.6296  last_time: 5.2241  data_time: 0.0028  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 15:22:41] d2.utils.events INFO:  eta: 7:18:14  iter: 10799  total_loss: 95.2  loss_ce: 3.044  loss_mask: 1.439  loss_dice: 3.857  loss_ce_0: 2.979  loss_mask_0: 1.107  loss_dice_0: 3.753  loss_ce_1: 3.16  loss_mask_1: 1.109  loss_dice_1: 3.836  loss_ce_2: 2.947  loss_mask_2: 1.287  loss_dice_2: 3.888  loss_ce_3: 2.858  loss_mask_3: 1.588  loss_dice_3: 4.107  loss_ce_4: 2.898  loss_mask_4: 1.445  loss_dice_4: 3.696  loss_ce_5: 3.044  loss_mask_5: 1.377  loss_dice_5: 3.982  loss_ce_6: 3.093  loss_mask_6: 1.173  loss_dice_6: 4.105  loss_ce_7: 2.947  loss_mask_7: 1.45  loss_dice_7: 4.165  loss_ce_8: 2.954  loss_mask_8: 1.664  loss_dice_8: 4.037  loss_mars: 0.6171    time: 5.6295  last_time: 5.4877  data_time: 0.0028  last_data_time: 0.0086   lr: 0.0001  max_mem: 0M
[10/19 15:24:33] d2.utils.events INFO:  eta: 7:16:25  iter: 10819  total_loss: 96.19  loss_ce: 4.148  loss_mask: 1.428  loss_dice: 4.321  loss_ce_0: 2.889  loss_mask_0: 1.245  loss_dice_0: 4.262  loss_ce_1: 2.814  loss_mask_1: 1.413  loss_dice_1: 4.238  loss_ce_2: 2.732  loss_mask_2: 1.426  loss_dice_2: 4.387  loss_ce_3: 3.197  loss_mask_3: 1.79  loss_dice_3: 4.489  loss_ce_4: 2.963  loss_mask_4: 2.062  loss_dice_4: 4.63  loss_ce_5: 2.981  loss_mask_5: 1.723  loss_dice_5: 4.468  loss_ce_6: 3.153  loss_mask_6: 1.303  loss_dice_6: 4.514  loss_ce_7: 3.339  loss_mask_7: 1.327  loss_dice_7: 4.533  loss_ce_8: 4.124  loss_mask_8: 1.584  loss_dice_8: 4.39  loss_mars: 0.1624    time: 5.6295  last_time: 5.9713  data_time: 0.0031  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 15:26:21] d2.utils.events INFO:  eta: 7:14:48  iter: 10839  total_loss: 91.36  loss_ce: 3.698  loss_mask: 1.082  loss_dice: 4.469  loss_ce_0: 3.296  loss_mask_0: 0.7794  loss_dice_0: 4.462  loss_ce_1: 2.982  loss_mask_1: 0.9775  loss_dice_1: 4.629  loss_ce_2: 2.79  loss_mask_2: 1.203  loss_dice_2: 4.611  loss_ce_3: 3.089  loss_mask_3: 1.052  loss_dice_3: 4.522  loss_ce_4: 3.236  loss_mask_4: 0.9858  loss_dice_4: 4.606  loss_ce_5: 3.405  loss_mask_5: 1.067  loss_dice_5: 4.558  loss_ce_6: 3.713  loss_mask_6: 1.051  loss_dice_6: 4.591  loss_ce_7: 3.939  loss_mask_7: 0.9836  loss_dice_7: 4.552  loss_ce_8: 3.735  loss_mask_8: 0.9983  loss_dice_8: 4.589  loss_mars: 0.5323    time: 5.6289  last_time: 5.9150  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 15:28:13] d2.utils.events INFO:  eta: 7:15:35  iter: 10859  total_loss: 91.9  loss_ce: 4.062  loss_mask: 0.7666  loss_dice: 4.422  loss_ce_0: 3.546  loss_mask_0: 1.012  loss_dice_0: 4.282  loss_ce_1: 3.403  loss_mask_1: 1.346  loss_dice_1: 4.483  loss_ce_2: 3.27  loss_mask_2: 1.076  loss_dice_2: 4.425  loss_ce_3: 3.322  loss_mask_3: 1.061  loss_dice_3: 4.348  loss_ce_4: 3.507  loss_mask_4: 1.101  loss_dice_4: 4.383  loss_ce_5: 3.58  loss_mask_5: 1.267  loss_dice_5: 4.4  loss_ce_6: 3.969  loss_mask_6: 0.8738  loss_dice_6: 4.281  loss_ce_7: 3.954  loss_mask_7: 0.7804  loss_dice_7: 4.46  loss_ce_8: 4.008  loss_mask_8: 0.7307  loss_dice_8: 4.472  loss_mars: 0.5136    time: 5.6289  last_time: 5.6604  data_time: 0.0027  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 15:30:00] d2.utils.events INFO:  eta: 7:10:45  iter: 10879  total_loss: 94.93  loss_ce: 3.416  loss_mask: 1.125  loss_dice: 4.304  loss_ce_0: 3.868  loss_mask_0: 1.162  loss_dice_0: 4.212  loss_ce_1: 3.325  loss_mask_1: 1.086  loss_dice_1: 4.539  loss_ce_2: 3.202  loss_mask_2: 1.19  loss_dice_2: 4.425  loss_ce_3: 3.624  loss_mask_3: 1.156  loss_dice_3: 4.39  loss_ce_4: 3.672  loss_mask_4: 1.222  loss_dice_4: 4.352  loss_ce_5: 3.787  loss_mask_5: 1.136  loss_dice_5: 4.312  loss_ce_6: 3.72  loss_mask_6: 1.131  loss_dice_6: 4.305  loss_ce_7: 3.388  loss_mask_7: 1.068  loss_dice_7: 4.441  loss_ce_8: 3.355  loss_mask_8: 1.119  loss_dice_8: 4.39  loss_mars: 0.4625    time: 5.6283  last_time: 5.0392  data_time: 0.0029  last_data_time: 0.0054   lr: 0.0001  max_mem: 0M
[10/19 15:31:52] d2.utils.events INFO:  eta: 7:10:05  iter: 10899  total_loss: 96.77  loss_ce: 3.348  loss_mask: 2.632  loss_dice: 4.535  loss_ce_0: 3.127  loss_mask_0: 1.263  loss_dice_0: 4.253  loss_ce_1: 2.783  loss_mask_1: 1.263  loss_dice_1: 4.369  loss_ce_2: 2.758  loss_mask_2: 1.37  loss_dice_2: 4.367  loss_ce_3: 2.901  loss_mask_3: 1.324  loss_dice_3: 4.218  loss_ce_4: 3.018  loss_mask_4: 1.292  loss_dice_4: 4.158  loss_ce_5: 2.922  loss_mask_5: 1.194  loss_dice_5: 4.262  loss_ce_6: 2.949  loss_mask_6: 1.465  loss_dice_6: 4.477  loss_ce_7: 3.167  loss_mask_7: 2.646  loss_dice_7: 4.503  loss_ce_8: 3.173  loss_mask_8: 1.98  loss_dice_8: 4.461  loss_mars: 0.693    time: 5.6282  last_time: 5.2569  data_time: 0.0029  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 15:33:37] d2.utils.events INFO:  eta: 7:06:35  iter: 10919  total_loss: 99.02  loss_ce: 4.134  loss_mask: 0.9695  loss_dice: 4.568  loss_ce_0: 4.134  loss_mask_0: 0.8626  loss_dice_0: 4.559  loss_ce_1: 3.302  loss_mask_1: 0.8919  loss_dice_1: 4.559  loss_ce_2: 3.88  loss_mask_2: 0.8176  loss_dice_2: 4.696  loss_ce_3: 4.123  loss_mask_3: 0.9149  loss_dice_3: 4.618  loss_ce_4: 4.13  loss_mask_4: 0.9299  loss_dice_4: 4.69  loss_ce_5: 3.801  loss_mask_5: 0.9063  loss_dice_5: 4.705  loss_ce_6: 4.077  loss_mask_6: 0.9225  loss_dice_6: 4.611  loss_ce_7: 4.002  loss_mask_7: 0.9267  loss_dice_7: 4.634  loss_ce_8: 3.969  loss_mask_8: 0.9728  loss_dice_8: 4.605  loss_mars: 0.1726    time: 5.6274  last_time: 3.7533  data_time: 0.0028  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 15:35:24] d2.utils.events INFO:  eta: 7:03:32  iter: 10939  total_loss: 93.85  loss_ce: 3.302  loss_mask: 1.875  loss_dice: 4.599  loss_ce_0: 3.289  loss_mask_0: 1.374  loss_dice_0: 4.337  loss_ce_1: 3.399  loss_mask_1: 1.384  loss_dice_1: 4.283  loss_ce_2: 3.134  loss_mask_2: 1.333  loss_dice_2: 4.564  loss_ce_3: 3.313  loss_mask_3: 1.318  loss_dice_3: 4.496  loss_ce_4: 3.319  loss_mask_4: 1.407  loss_dice_4: 4.61  loss_ce_5: 3.534  loss_mask_5: 1.639  loss_dice_5: 4.588  loss_ce_6: 3.453  loss_mask_6: 1.442  loss_dice_6: 4.365  loss_ce_7: 3.448  loss_mask_7: 1.472  loss_dice_7: 4.537  loss_ce_8: 3.249  loss_mask_8: 1.754  loss_dice_8: 4.466  loss_mars: 0.1324    time: 5.6268  last_time: 5.3317  data_time: 0.0021  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 15:35:29] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0010940.pth
[10/19 15:35:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 15:35:29] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 15:35:29] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 15:35:29] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 15:37:13] d2.utils.events INFO:  eta: 7:02:03  iter: 10959  total_loss: 95.87  loss_ce: 3.509  loss_mask: 1.238  loss_dice: 4.607  loss_ce_0: 3.367  loss_mask_0: 0.9483  loss_dice_0: 4.44  loss_ce_1: 3.534  loss_mask_1: 0.6812  loss_dice_1: 4.53  loss_ce_2: 3.24  loss_mask_2: 0.9595  loss_dice_2: 4.604  loss_ce_3: 2.963  loss_mask_3: 1.277  loss_dice_3: 4.57  loss_ce_4: 2.97  loss_mask_4: 1.742  loss_dice_4: 4.668  loss_ce_5: 3.191  loss_mask_5: 1.937  loss_dice_5: 4.628  loss_ce_6: 3.23  loss_mask_6: 1.859  loss_dice_6: 4.668  loss_ce_7: 3.929  loss_mask_7: 1.595  loss_dice_7: 4.659  loss_ce_8: 3.63  loss_mask_8: 1.838  loss_dice_8: 4.581  loss_mars: 0.2798    time: 5.6265  last_time: 6.0904  data_time: 0.0028  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 15:39:09] d2.utils.events INFO:  eta: 7:01:44  iter: 10979  total_loss: 101.9  loss_ce: 4.308  loss_mask: 1.639  loss_dice: 4.462  loss_ce_0: 4.211  loss_mask_0: 1.075  loss_dice_0: 4.319  loss_ce_1: 4.427  loss_mask_1: 1.169  loss_dice_1: 4.292  loss_ce_2: 3.978  loss_mask_2: 1.116  loss_dice_2: 4.342  loss_ce_3: 4.124  loss_mask_3: 1.184  loss_dice_3: 4.477  loss_ce_4: 4.113  loss_mask_4: 1.152  loss_dice_4: 4.488  loss_ce_5: 4.312  loss_mask_5: 1.134  loss_dice_5: 4.377  loss_ce_6: 4.439  loss_mask_6: 1.255  loss_dice_6: 4.412  loss_ce_7: 4.48  loss_mask_7: 1.154  loss_dice_7: 4.368  loss_ce_8: 4.406  loss_mask_8: 1.245  loss_dice_8: 4.356  loss_mars: 0.5471    time: 5.6268  last_time: 5.1581  data_time: 0.0033  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 15:40:59] d2.utils.events INFO:  eta: 6:59:20  iter: 10999  total_loss: 101.6  loss_ce: 4.151  loss_mask: 2.076  loss_dice: 4.547  loss_ce_0: 4.032  loss_mask_0: 1.244  loss_dice_0: 4.137  loss_ce_1: 3.93  loss_mask_1: 1.637  loss_dice_1: 4.19  loss_ce_2: 3.917  loss_mask_2: 1.465  loss_dice_2: 4.421  loss_ce_3: 3.831  loss_mask_3: 1.49  loss_dice_3: 4.233  loss_ce_4: 3.738  loss_mask_4: 1.357  loss_dice_4: 4.226  loss_ce_5: 3.767  loss_mask_5: 1.252  loss_dice_5: 4.12  loss_ce_6: 3.77  loss_mask_6: 1.548  loss_dice_6: 4.123  loss_ce_7: 3.685  loss_mask_7: 1.768  loss_dice_7: 4.565  loss_ce_8: 3.883  loss_mask_8: 2.015  loss_dice_8: 4.388  loss_mars: 0.6978    time: 5.6265  last_time: 5.2987  data_time: 0.0030  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 15:42:47] d2.utils.events INFO:  eta: 6:57:57  iter: 11019  total_loss: 102.9  loss_ce: 3.48  loss_mask: 1.635  loss_dice: 4.839  loss_ce_0: 3.79  loss_mask_0: 1.157  loss_dice_0: 4.459  loss_ce_1: 3.853  loss_mask_1: 1.616  loss_dice_1: 4.521  loss_ce_2: 3.491  loss_mask_2: 1.615  loss_dice_2: 4.832  loss_ce_3: 3.691  loss_mask_3: 1.203  loss_dice_3: 4.584  loss_ce_4: 3.654  loss_mask_4: 1.214  loss_dice_4: 4.635  loss_ce_5: 3.998  loss_mask_5: 1.546  loss_dice_5: 4.588  loss_ce_6: 3.914  loss_mask_6: 1.473  loss_dice_6: 4.479  loss_ce_7: 3.65  loss_mask_7: 1.74  loss_dice_7: 4.662  loss_ce_8: 3.45  loss_mask_8: 2.133  loss_dice_8: 4.924  loss_mars: 0.6539    time: 5.6260  last_time: 5.6074  data_time: 0.0026  last_data_time: 0.0016   lr: 0.0001  max_mem: 0M
[10/19 15:44:34] d2.utils.events INFO:  eta: 6:56:00  iter: 11039  total_loss: 92.63  loss_ce: 3.032  loss_mask: 2.069  loss_dice: 4.289  loss_ce_0: 2.894  loss_mask_0: 1.413  loss_dice_0: 4.102  loss_ce_1: 2.925  loss_mask_1: 1.535  loss_dice_1: 4.174  loss_ce_2: 3.078  loss_mask_2: 1.524  loss_dice_2: 4.219  loss_ce_3: 2.9  loss_mask_3: 1.583  loss_dice_3: 4.29  loss_ce_4: 2.747  loss_mask_4: 1.607  loss_dice_4: 4.197  loss_ce_5: 2.691  loss_mask_5: 1.539  loss_dice_5: 4.22  loss_ce_6: 2.643  loss_mask_6: 1.561  loss_dice_6: 4.139  loss_ce_7: 2.789  loss_mask_7: 1.462  loss_dice_7: 4.258  loss_ce_8: 2.891  loss_mask_8: 1.488  loss_dice_8: 4.325  loss_mars: 0.8954    time: 5.6255  last_time: 8.1072  data_time: 0.0032  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 15:46:20] d2.utils.events INFO:  eta: 6:54:12  iter: 11059  total_loss: 84.14  loss_ce: 3.091  loss_mask: 1.581  loss_dice: 4.337  loss_ce_0: 2.81  loss_mask_0: 1.182  loss_dice_0: 4.348  loss_ce_1: 3.002  loss_mask_1: 1.422  loss_dice_1: 4.299  loss_ce_2: 3.051  loss_mask_2: 1.158  loss_dice_2: 4.317  loss_ce_3: 3.071  loss_mask_3: 1.165  loss_dice_3: 4.33  loss_ce_4: 3.21  loss_mask_4: 1.113  loss_dice_4: 4.313  loss_ce_5: 2.895  loss_mask_5: 1.067  loss_dice_5: 4.326  loss_ce_6: 2.956  loss_mask_6: 1.246  loss_dice_6: 4.329  loss_ce_7: 3.053  loss_mask_7: 1.096  loss_dice_7: 4.335  loss_ce_8: 2.98  loss_mask_8: 1.156  loss_dice_8: 4.329  loss_mars: 0.5013    time: 5.6248  last_time: 5.2226  data_time: 0.0026  last_data_time: 0.0083   lr: 0.0001  max_mem: 0M
[10/19 15:48:11] d2.utils.events INFO:  eta: 6:52:23  iter: 11079  total_loss: 87.77  loss_ce: 2.963  loss_mask: 1.295  loss_dice: 4.077  loss_ce_0: 2.834  loss_mask_0: 1.319  loss_dice_0: 4.06  loss_ce_1: 3.481  loss_mask_1: 1.717  loss_dice_1: 4.076  loss_ce_2: 3.444  loss_mask_2: 1.685  loss_dice_2: 4.099  loss_ce_3: 3.541  loss_mask_3: 1.368  loss_dice_3: 4.083  loss_ce_4: 3.452  loss_mask_4: 1.297  loss_dice_4: 4.125  loss_ce_5: 2.978  loss_mask_5: 1.347  loss_dice_5: 4.095  loss_ce_6: 2.99  loss_mask_6: 1.322  loss_dice_6: 4.088  loss_ce_7: 3.002  loss_mask_7: 1.332  loss_dice_7: 4.173  loss_ce_8: 2.885  loss_mask_8: 1.292  loss_dice_8: 4.099  loss_mars: 0.8172    time: 5.6246  last_time: 5.7577  data_time: 0.0023  last_data_time: 0.0063   lr: 0.0001  max_mem: 0M
[10/19 15:50:01] d2.utils.events INFO:  eta: 6:50:02  iter: 11099  total_loss: 93.11  loss_ce: 3.003  loss_mask: 1.677  loss_dice: 4.389  loss_ce_0: 3.16  loss_mask_0: 1.394  loss_dice_0: 4.227  loss_ce_1: 3.176  loss_mask_1: 1.419  loss_dice_1: 4.172  loss_ce_2: 3.283  loss_mask_2: 1.496  loss_dice_2: 4.151  loss_ce_3: 3.406  loss_mask_3: 1.492  loss_dice_3: 4.245  loss_ce_4: 3.298  loss_mask_4: 1.424  loss_dice_4: 4.241  loss_ce_5: 3.103  loss_mask_5: 1.819  loss_dice_5: 4.175  loss_ce_6: 2.911  loss_mask_6: 1.867  loss_dice_6: 4.316  loss_ce_7: 2.741  loss_mask_7: 1.713  loss_dice_7: 4.301  loss_ce_8: 2.852  loss_mask_8: 1.765  loss_dice_8: 4.415  loss_mars: 0.7908    time: 5.6244  last_time: 6.0867  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 15:51:55] d2.utils.events INFO:  eta: 6:52:15  iter: 11119  total_loss: 92.56  loss_ce: 3.932  loss_mask: 1.067  loss_dice: 4.239  loss_ce_0: 3.254  loss_mask_0: 0.6279  loss_dice_0: 4.245  loss_ce_1: 4.285  loss_mask_1: 0.631  loss_dice_1: 4.295  loss_ce_2: 4.249  loss_mask_2: 0.6876  loss_dice_2: 4.301  loss_ce_3: 4.003  loss_mask_3: 0.697  loss_dice_3: 4.435  loss_ce_4: 4.023  loss_mask_4: 0.7333  loss_dice_4: 4.225  loss_ce_5: 3.66  loss_mask_5: 0.8837  loss_dice_5: 4.263  loss_ce_6: 3.542  loss_mask_6: 1.353  loss_dice_6: 4.353  loss_ce_7: 3.584  loss_mask_7: 1.364  loss_dice_7: 4.252  loss_ce_8: 3.696  loss_mask_8: 1.283  loss_dice_8: 4.273  loss_mars: 0.5792    time: 5.6245  last_time: 5.2186  data_time: 0.0033  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 15:53:48] d2.utils.events INFO:  eta: 6:53:09  iter: 11139  total_loss: 89.4  loss_ce: 3.722  loss_mask: 1.324  loss_dice: 4.292  loss_ce_0: 3.646  loss_mask_0: 1.325  loss_dice_0: 4.188  loss_ce_1: 3.419  loss_mask_1: 1.213  loss_dice_1: 4.326  loss_ce_2: 3.498  loss_mask_2: 1.208  loss_dice_2: 4.303  loss_ce_3: 3.33  loss_mask_3: 1.27  loss_dice_3: 4.3  loss_ce_4: 3.237  loss_mask_4: 1.322  loss_dice_4: 4.187  loss_ce_5: 3.279  loss_mask_5: 1.396  loss_dice_5: 4.206  loss_ce_6: 3.223  loss_mask_6: 1.434  loss_dice_6: 4.255  loss_ce_7: 3.325  loss_mask_7: 1.45  loss_dice_7: 4.202  loss_ce_8: 3.666  loss_mask_8: 1.368  loss_dice_8: 4.344  loss_mars: 0.786    time: 5.6245  last_time: 4.7268  data_time: 0.0025  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 15:55:42] d2.utils.events INFO:  eta: 6:53:35  iter: 11159  total_loss: 83.81  loss_ce: 3.126  loss_mask: 0.6548  loss_dice: 4.283  loss_ce_0: 2.739  loss_mask_0: 0.5555  loss_dice_0: 4.209  loss_ce_1: 2.798  loss_mask_1: 0.6925  loss_dice_1: 4.409  loss_ce_2: 2.751  loss_mask_2: 0.6589  loss_dice_2: 4.342  loss_ce_3: 2.612  loss_mask_3: 0.7948  loss_dice_3: 4.292  loss_ce_4: 2.439  loss_mask_4: 0.831  loss_dice_4: 4.324  loss_ce_5: 2.553  loss_mask_5: 0.736  loss_dice_5: 4.447  loss_ce_6: 2.618  loss_mask_6: 0.7085  loss_dice_6: 4.406  loss_ce_7: 2.708  loss_mask_7: 0.821  loss_dice_7: 4.292  loss_ce_8: 2.828  loss_mask_8: 0.6601  loss_dice_8: 4.311  loss_mars: 0.6506    time: 5.6247  last_time: 6.5349  data_time: 0.0029  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 15:57:36] d2.utils.events INFO:  eta: 6:52:25  iter: 11179  total_loss: 95.34  loss_ce: 3.683  loss_mask: 1.06  loss_dice: 4.22  loss_ce_0: 3.755  loss_mask_0: 0.9823  loss_dice_0: 4.322  loss_ce_1: 3.524  loss_mask_1: 0.9628  loss_dice_1: 4.534  loss_ce_2: 3.139  loss_mask_2: 1.032  loss_dice_2: 4.35  loss_ce_3: 3.059  loss_mask_3: 1.137  loss_dice_3: 4.498  loss_ce_4: 2.931  loss_mask_4: 1.077  loss_dice_4: 4.344  loss_ce_5: 3.238  loss_mask_5: 0.9844  loss_dice_5: 4.501  loss_ce_6: 3.388  loss_mask_6: 0.9698  loss_dice_6: 4.515  loss_ce_7: 3.697  loss_mask_7: 1.086  loss_dice_7: 4.242  loss_ce_8: 3.789  loss_mask_8: 1.097  loss_dice_8: 4.254  loss_mars: 0.5238    time: 5.6248  last_time: 5.3512  data_time: 0.0032  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 15:59:24] d2.utils.events INFO:  eta: 6:50:57  iter: 11199  total_loss: 88.49  loss_ce: 3.271  loss_mask: 0.8925  loss_dice: 4.23  loss_ce_0: 3.065  loss_mask_0: 0.8833  loss_dice_0: 4.456  loss_ce_1: 3.203  loss_mask_1: 0.9217  loss_dice_1: 4.419  loss_ce_2: 2.898  loss_mask_2: 1.044  loss_dice_2: 4.373  loss_ce_3: 2.792  loss_mask_3: 0.9397  loss_dice_3: 4.489  loss_ce_4: 2.887  loss_mask_4: 0.9395  loss_dice_4: 4.448  loss_ce_5: 3.257  loss_mask_5: 0.8967  loss_dice_5: 4.364  loss_ce_6: 3.262  loss_mask_6: 0.8909  loss_dice_6: 4.407  loss_ce_7: 3.389  loss_mask_7: 0.827  loss_dice_7: 4.324  loss_ce_8: 3.377  loss_mask_8: 0.8941  loss_dice_8: 4.287  loss_mars: 0.677    time: 5.6243  last_time: 5.1244  data_time: 0.0026  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 16:01:13] d2.utils.events INFO:  eta: 6:49:05  iter: 11219  total_loss: 91.86  loss_ce: 3.36  loss_mask: 0.6498  loss_dice: 4.135  loss_ce_0: 2.876  loss_mask_0: 0.6797  loss_dice_0: 3.988  loss_ce_1: 2.912  loss_mask_1: 0.6948  loss_dice_1: 4.437  loss_ce_2: 3.727  loss_mask_2: 0.7281  loss_dice_2: 4.465  loss_ce_3: 3.163  loss_mask_3: 0.6673  loss_dice_3: 4.277  loss_ce_4: 3.357  loss_mask_4: 0.6629  loss_dice_4: 4.119  loss_ce_5: 3.093  loss_mask_5: 0.8141  loss_dice_5: 4.542  loss_ce_6: 3.001  loss_mask_6: 0.6481  loss_dice_6: 4.231  loss_ce_7: 3.157  loss_mask_7: 0.637  loss_dice_7: 4.369  loss_ce_8: 3.268  loss_mask_8: 0.6537  loss_dice_8: 4.09  loss_mars: 0.85    time: 5.6240  last_time: 4.6060  data_time: 0.0036  last_data_time: 0.0051   lr: 0.0001  max_mem: 0M
[10/19 16:03:04] d2.utils.events INFO:  eta: 6:47:38  iter: 11239  total_loss: 94.04  loss_ce: 3.379  loss_mask: 0.9879  loss_dice: 4.32  loss_ce_0: 3.448  loss_mask_0: 1.11  loss_dice_0: 4.276  loss_ce_1: 3.63  loss_mask_1: 1.14  loss_dice_1: 4.508  loss_ce_2: 3.288  loss_mask_2: 1.056  loss_dice_2: 4.402  loss_ce_3: 3.254  loss_mask_3: 1.002  loss_dice_3: 4.32  loss_ce_4: 3.348  loss_mask_4: 1.206  loss_dice_4: 4.399  loss_ce_5: 3.585  loss_mask_5: 1.065  loss_dice_5: 4.272  loss_ce_6: 3.619  loss_mask_6: 1.048  loss_dice_6: 4.365  loss_ce_7: 3.61  loss_mask_7: 1.075  loss_dice_7: 4.355  loss_ce_8: 3.478  loss_mask_8: 0.9972  loss_dice_8: 4.318  loss_mars: 0.7565    time: 5.6238  last_time: 7.8822  data_time: 0.0027  last_data_time: 0.0087   lr: 0.0001  max_mem: 0M
[10/19 16:04:48] d2.utils.events INFO:  eta: 6:43:40  iter: 11259  total_loss: 90.04  loss_ce: 3.637  loss_mask: 0.89  loss_dice: 4.353  loss_ce_0: 3.591  loss_mask_0: 0.9633  loss_dice_0: 4.293  loss_ce_1: 3.711  loss_mask_1: 0.9133  loss_dice_1: 4.304  loss_ce_2: 3.444  loss_mask_2: 0.9354  loss_dice_2: 4.342  loss_ce_3: 3.332  loss_mask_3: 1.025  loss_dice_3: 4.388  loss_ce_4: 3.405  loss_mask_4: 0.9988  loss_dice_4: 4.315  loss_ce_5: 3.475  loss_mask_5: 0.9635  loss_dice_5: 4.669  loss_ce_6: 3.643  loss_mask_6: 0.8708  loss_dice_6: 4.418  loss_ce_7: 3.531  loss_mask_7: 0.9463  loss_dice_7: 4.349  loss_ce_8: 3.632  loss_mask_8: 0.9175  loss_dice_8: 4.357  loss_mars: 0.8349    time: 5.6229  last_time: 5.6828  data_time: 0.0032  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 16:06:37] d2.utils.events INFO:  eta: 6:42:51  iter: 11279  total_loss: 87.65  loss_ce: 2.755  loss_mask: 1.524  loss_dice: 4.288  loss_ce_0: 2.736  loss_mask_0: 1.509  loss_dice_0: 4.055  loss_ce_1: 2.854  loss_mask_1: 1.485  loss_dice_1: 4.19  loss_ce_2: 2.454  loss_mask_2: 1.447  loss_dice_2: 4.18  loss_ce_3: 2.315  loss_mask_3: 1.413  loss_dice_3: 4.029  loss_ce_4: 2.316  loss_mask_4: 1.416  loss_dice_4: 4.194  loss_ce_5: 2.539  loss_mask_5: 1.408  loss_dice_5: 4.246  loss_ce_6: 2.904  loss_mask_6: 1.371  loss_dice_6: 4.289  loss_ce_7: 2.906  loss_mask_7: 1.408  loss_dice_7: 4.277  loss_ce_8: 2.714  loss_mask_8: 1.399  loss_dice_8: 4.214  loss_mars: 0.8445    time: 5.6226  last_time: 5.8456  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 16:08:26] d2.utils.events INFO:  eta: 6:41:00  iter: 11299  total_loss: 91.1  loss_ce: 2.86  loss_mask: 1.151  loss_dice: 4.373  loss_ce_0: 3.061  loss_mask_0: 1.052  loss_dice_0: 4.155  loss_ce_1: 3.416  loss_mask_1: 1.049  loss_dice_1: 4.495  loss_ce_2: 3.14  loss_mask_2: 1.283  loss_dice_2: 4.522  loss_ce_3: 3.128  loss_mask_3: 1.426  loss_dice_3: 4.501  loss_ce_4: 3.291  loss_mask_4: 1.097  loss_dice_4: 4.338  loss_ce_5: 3.253  loss_mask_5: 1.031  loss_dice_5: 4.53  loss_ce_6: 3.479  loss_mask_6: 1.13  loss_dice_6: 4.317  loss_ce_7: 3.333  loss_mask_7: 1.201  loss_dice_7: 4.348  loss_ce_8: 2.87  loss_mask_8: 1.044  loss_dice_8: 4.417  loss_mars: 0.7403    time: 5.6223  last_time: 5.8130  data_time: 0.0028  last_data_time: 0.0024   lr: 0.0001  max_mem: 0M
[10/19 16:10:21] d2.utils.events INFO:  eta: 6:40:12  iter: 11319  total_loss: 87.24  loss_ce: 3.329  loss_mask: 1.27  loss_dice: 3.999  loss_ce_0: 2.947  loss_mask_0: 1.081  loss_dice_0: 4.015  loss_ce_1: 3.057  loss_mask_1: 1.112  loss_dice_1: 4.087  loss_ce_2: 3.033  loss_mask_2: 1.171  loss_dice_2: 4.073  loss_ce_3: 3.119  loss_mask_3: 1.047  loss_dice_3: 3.981  loss_ce_4: 3.109  loss_mask_4: 1.119  loss_dice_4: 3.966  loss_ce_5: 3.148  loss_mask_5: 1.149  loss_dice_5: 3.897  loss_ce_6: 3.369  loss_mask_6: 1.184  loss_dice_6: 3.658  loss_ce_7: 3.301  loss_mask_7: 1.21  loss_dice_7: 3.877  loss_ce_8: 3.291  loss_mask_8: 1.208  loss_dice_8: 3.899  loss_mars: 0.957    time: 5.6225  last_time: 6.4787  data_time: 0.0025  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 16:12:15] d2.utils.events INFO:  eta: 6:39:25  iter: 11339  total_loss: 93.51  loss_ce: 3.955  loss_mask: 0.9102  loss_dice: 4.379  loss_ce_0: 3.855  loss_mask_0: 0.9054  loss_dice_0: 4.387  loss_ce_1: 3.979  loss_mask_1: 0.8381  loss_dice_1: 4.314  loss_ce_2: 3.793  loss_mask_2: 0.8573  loss_dice_2: 4.231  loss_ce_3: 3.918  loss_mask_3: 0.8937  loss_dice_3: 4.295  loss_ce_4: 3.92  loss_mask_4: 0.9321  loss_dice_4: 4.399  loss_ce_5: 3.983  loss_mask_5: 0.8763  loss_dice_5: 4.379  loss_ce_6: 4.116  loss_mask_6: 0.8518  loss_dice_6: 4.388  loss_ce_7: 4.096  loss_mask_7: 0.8786  loss_dice_7: 4.423  loss_ce_8: 4.012  loss_mask_8: 0.8261  loss_dice_8: 4.358  loss_mars: 0.736    time: 5.6227  last_time: 5.7235  data_time: 0.0032  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 16:14:04] d2.utils.events INFO:  eta: 6:37:02  iter: 11359  total_loss: 92.35  loss_ce: 3.814  loss_mask: 0.948  loss_dice: 4.182  loss_ce_0: 3.814  loss_mask_0: 0.9637  loss_dice_0: 4.091  loss_ce_1: 3.697  loss_mask_1: 0.8488  loss_dice_1: 3.993  loss_ce_2: 3.798  loss_mask_2: 1.048  loss_dice_2: 4.214  loss_ce_3: 3.65  loss_mask_3: 1.102  loss_dice_3: 4.218  loss_ce_4: 3.781  loss_mask_4: 1.165  loss_dice_4: 4.327  loss_ce_5: 3.717  loss_mask_5: 1.157  loss_dice_5: 4.391  loss_ce_6: 3.785  loss_mask_6: 0.9104  loss_dice_6: 4.15  loss_ce_7: 3.995  loss_mask_7: 1.221  loss_dice_7: 4.27  loss_ce_8: 3.807  loss_mask_8: 1.133  loss_dice_8: 4.184  loss_mars: 0.787    time: 5.6223  last_time: 5.5929  data_time: 0.0027  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 16:15:54] d2.utils.events INFO:  eta: 6:34:37  iter: 11379  total_loss: 93.09  loss_ce: 2.991  loss_mask: 0.9754  loss_dice: 4.635  loss_ce_0: 3.132  loss_mask_0: 0.8565  loss_dice_0: 4.452  loss_ce_1: 3.2  loss_mask_1: 0.8158  loss_dice_1: 4.532  loss_ce_2: 3.183  loss_mask_2: 0.8668  loss_dice_2: 4.534  loss_ce_3: 3.156  loss_mask_3: 0.8528  loss_dice_3: 4.516  loss_ce_4: 2.93  loss_mask_4: 1.015  loss_dice_4: 4.567  loss_ce_5: 3.061  loss_mask_5: 1.022  loss_dice_5: 4.516  loss_ce_6: 3.426  loss_mask_6: 0.9485  loss_dice_6: 4.584  loss_ce_7: 3.199  loss_mask_7: 1.041  loss_dice_7: 4.542  loss_ce_8: 2.836  loss_mask_8: 1.012  loss_dice_8: 4.661  loss_mars: 0.4222    time: 5.6221  last_time: 5.1249  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 16:17:46] d2.utils.events INFO:  eta: 6:33:54  iter: 11399  total_loss: 88.41  loss_ce: 3.376  loss_mask: 1.298  loss_dice: 4.313  loss_ce_0: 3.416  loss_mask_0: 1.304  loss_dice_0: 4.273  loss_ce_1: 3.243  loss_mask_1: 1.205  loss_dice_1: 4.421  loss_ce_2: 2.983  loss_mask_2: 1.239  loss_dice_2: 4.293  loss_ce_3: 2.791  loss_mask_3: 1.323  loss_dice_3: 4.294  loss_ce_4: 2.729  loss_mask_4: 1.46  loss_dice_4: 4.276  loss_ce_5: 2.906  loss_mask_5: 1.304  loss_dice_5: 4.242  loss_ce_6: 3.048  loss_mask_6: 1.256  loss_dice_6: 4.213  loss_ce_7: 3.178  loss_mask_7: 1.304  loss_dice_7: 4.344  loss_ce_8: 3.331  loss_mask_8: 1.38  loss_dice_8: 4.345  loss_mars: 0.6873    time: 5.6220  last_time: 6.6017  data_time: 0.0021  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 16:19:37] d2.utils.events INFO:  eta: 6:32:51  iter: 11419  total_loss: 87.69  loss_ce: 3.181  loss_mask: 0.882  loss_dice: 4.382  loss_ce_0: 3.658  loss_mask_0: 0.8798  loss_dice_0: 4.21  loss_ce_1: 3.324  loss_mask_1: 0.8901  loss_dice_1: 4.399  loss_ce_2: 3.174  loss_mask_2: 0.9113  loss_dice_2: 4.547  loss_ce_3: 2.842  loss_mask_3: 0.9258  loss_dice_3: 4.475  loss_ce_4: 2.616  loss_mask_4: 1.036  loss_dice_4: 4.331  loss_ce_5: 3.066  loss_mask_5: 0.8328  loss_dice_5: 4.344  loss_ce_6: 3.08  loss_mask_6: 1.104  loss_dice_6: 4.232  loss_ce_7: 3.202  loss_mask_7: 0.9268  loss_dice_7: 4.302  loss_ce_8: 3.247  loss_mask_8: 0.9041  loss_dice_8: 4.34  loss_mars: 0.609    time: 5.6219  last_time: 5.7087  data_time: 0.0031  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 16:21:27] d2.utils.events INFO:  eta: 6:30:20  iter: 11439  total_loss: 89.44  loss_ce: 3.23  loss_mask: 1.306  loss_dice: 4.136  loss_ce_0: 3.543  loss_mask_0: 1.282  loss_dice_0: 4.256  loss_ce_1: 3.244  loss_mask_1: 1.434  loss_dice_1: 4.244  loss_ce_2: 3.235  loss_mask_2: 1.444  loss_dice_2: 4.151  loss_ce_3: 3.233  loss_mask_3: 1.482  loss_dice_3: 4.247  loss_ce_4: 3.056  loss_mask_4: 1.512  loss_dice_4: 4.249  loss_ce_5: 3.098  loss_mask_5: 1.869  loss_dice_5: 4.339  loss_ce_6: 3.248  loss_mask_6: 1.333  loss_dice_6: 4.394  loss_ce_7: 3.348  loss_mask_7: 1.331  loss_dice_7: 4.211  loss_ce_8: 3.027  loss_mask_8: 1.389  loss_dice_8: 4.175  loss_mars: 0.654    time: 5.6216  last_time: 5.8870  data_time: 0.0028  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 16:23:18] d2.utils.events INFO:  eta: 6:27:43  iter: 11459  total_loss: 94.56  loss_ce: 3.626  loss_mask: 0.71  loss_dice: 4.535  loss_ce_0: 3.56  loss_mask_0: 0.7357  loss_dice_0: 4.447  loss_ce_1: 3.711  loss_mask_1: 0.6933  loss_dice_1: 4.471  loss_ce_2: 3.661  loss_mask_2: 0.7926  loss_dice_2: 4.395  loss_ce_3: 3.52  loss_mask_3: 0.8702  loss_dice_3: 4.417  loss_ce_4: 3.413  loss_mask_4: 0.8309  loss_dice_4: 4.367  loss_ce_5: 3.522  loss_mask_5: 0.8032  loss_dice_5: 4.583  loss_ce_6: 3.431  loss_mask_6: 0.7506  loss_dice_6: 4.423  loss_ce_7: 3.617  loss_mask_7: 0.7561  loss_dice_7: 4.447  loss_ce_8: 3.652  loss_mask_8: 0.769  loss_dice_8: 4.508  loss_mars: 0.6276    time: 5.6214  last_time: 5.8497  data_time: 0.0026  last_data_time: 0.0067   lr: 0.0001  max_mem: 0M
[10/19 16:25:11] d2.utils.events INFO:  eta: 6:26:26  iter: 11479  total_loss: 90.27  loss_ce: 3.297  loss_mask: 1.229  loss_dice: 4.364  loss_ce_0: 4.016  loss_mask_0: 1.135  loss_dice_0: 4.295  loss_ce_1: 3.818  loss_mask_1: 1.056  loss_dice_1: 4.229  loss_ce_2: 3.612  loss_mask_2: 1.122  loss_dice_2: 4.217  loss_ce_3: 3.457  loss_mask_3: 1.082  loss_dice_3: 4.311  loss_ce_4: 3.513  loss_mask_4: 1.007  loss_dice_4: 4.292  loss_ce_5: 3.398  loss_mask_5: 1.08  loss_dice_5: 4.329  loss_ce_6: 3.297  loss_mask_6: 1.221  loss_dice_6: 4.371  loss_ce_7: 3.298  loss_mask_7: 1.157  loss_dice_7: 4.295  loss_ce_8: 3.306  loss_mask_8: 1.154  loss_dice_8: 4.268  loss_mars: 0.7084    time: 5.6215  last_time: 4.8972  data_time: 0.0025  last_data_time: 0.0032   lr: 0.0001  max_mem: 0M
[10/19 16:27:04] d2.utils.events INFO:  eta: 6:24:00  iter: 11499  total_loss: 86.74  loss_ce: 1.981  loss_mask: 1.841  loss_dice: 3.802  loss_ce_0: 2.179  loss_mask_0: 1.784  loss_dice_0: 3.654  loss_ce_1: 2.284  loss_mask_1: 1.586  loss_dice_1: 3.993  loss_ce_2: 2.281  loss_mask_2: 1.727  loss_dice_2: 3.965  loss_ce_3: 2.011  loss_mask_3: 1.914  loss_dice_3: 4.407  loss_ce_4: 1.751  loss_mask_4: 1.908  loss_dice_4: 4.483  loss_ce_5: 2.19  loss_mask_5: 1.811  loss_dice_5: 3.559  loss_ce_6: 2.392  loss_mask_6: 1.703  loss_dice_6: 3.827  loss_ce_7: 1.896  loss_mask_7: 2.222  loss_dice_7: 4.119  loss_ce_8: 1.906  loss_mask_8: 2.043  loss_dice_8: 3.64  loss_mars: 0.5906    time: 5.6216  last_time: 5.9780  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 16:28:52] d2.utils.events INFO:  eta: 6:22:42  iter: 11519  total_loss: 83.78  loss_ce: 2.912  loss_mask: 0.9218  loss_dice: 4.3  loss_ce_0: 3.485  loss_mask_0: 0.8285  loss_dice_0: 4.335  loss_ce_1: 3.352  loss_mask_1: 0.8243  loss_dice_1: 4.323  loss_ce_2: 3.213  loss_mask_2: 0.8757  loss_dice_2: 4.261  loss_ce_3: 2.887  loss_mask_3: 0.8882  loss_dice_3: 4.346  loss_ce_4: 2.756  loss_mask_4: 0.8287  loss_dice_4: 4.293  loss_ce_5: 2.956  loss_mask_5: 0.9778  loss_dice_5: 4.21  loss_ce_6: 2.99  loss_mask_6: 0.8911  loss_dice_6: 4.157  loss_ce_7: 2.935  loss_mask_7: 1.094  loss_dice_7: 4.232  loss_ce_8: 2.925  loss_mask_8: 0.9103  loss_dice_8: 4.285  loss_mars: 0.9136    time: 5.6212  last_time: 5.2620  data_time: 0.0025  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 16:30:44] d2.utils.events INFO:  eta: 6:21:46  iter: 11539  total_loss: 91.54  loss_ce: 3.034  loss_mask: 1.322  loss_dice: 4.062  loss_ce_0: 3.649  loss_mask_0: 1.203  loss_dice_0: 4.108  loss_ce_1: 3.578  loss_mask_1: 1.26  loss_dice_1: 3.931  loss_ce_2: 3.099  loss_mask_2: 1.331  loss_dice_2: 4.112  loss_ce_3: 2.769  loss_mask_3: 1.416  loss_dice_3: 4.114  loss_ce_4: 2.748  loss_mask_4: 1.318  loss_dice_4: 4.141  loss_ce_5: 2.925  loss_mask_5: 1.365  loss_dice_5: 4.236  loss_ce_6: 3.099  loss_mask_6: 1.225  loss_dice_6: 3.995  loss_ce_7: 3.157  loss_mask_7: 1.234  loss_dice_7: 4.127  loss_ce_8: 3.061  loss_mask_8: 1.249  loss_dice_8: 4.03  loss_mars: 0.8596    time: 5.6210  last_time: 5.7611  data_time: 0.0023  last_data_time: 0.0026   lr: 0.0001  max_mem: 0M
[10/19 16:32:37] d2.utils.events INFO:  eta: 6:19:55  iter: 11559  total_loss: 99.81  loss_ce: 4.785  loss_mask: 0.738  loss_dice: 4.404  loss_ce_0: 4.971  loss_mask_0: 0.6256  loss_dice_0: 4.543  loss_ce_1: 4.798  loss_mask_1: 0.6893  loss_dice_1: 4.455  loss_ce_2: 4.851  loss_mask_2: 0.7108  loss_dice_2: 4.488  loss_ce_3: 5.076  loss_mask_3: 0.7921  loss_dice_3: 4.31  loss_ce_4: 4.941  loss_mask_4: 0.7067  loss_dice_4: 4.47  loss_ce_5: 4.807  loss_mask_5: 0.7969  loss_dice_5: 4.471  loss_ce_6: 4.559  loss_mask_6: 0.7909  loss_dice_6: 4.369  loss_ce_7: 4.561  loss_mask_7: 0.7689  loss_dice_7: 4.454  loss_ce_8: 4.612  loss_mask_8: 0.7585  loss_dice_8: 4.405  loss_mars: 0.9386    time: 5.6211  last_time: 5.2128  data_time: 0.0033  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 16:34:21] d2.utils.events INFO:  eta: 6:18:02  iter: 11579  total_loss: 99.81  loss_ce: 3.696  loss_mask: 1.578  loss_dice: 4.183  loss_ce_0: 4.241  loss_mask_0: 1.499  loss_dice_0: 4.185  loss_ce_1: 4.044  loss_mask_1: 1.496  loss_dice_1: 4.045  loss_ce_2: 3.973  loss_mask_2: 1.641  loss_dice_2: 3.95  loss_ce_3: 3.746  loss_mask_3: 1.572  loss_dice_3: 4.069  loss_ce_4: 3.645  loss_mask_4: 1.514  loss_dice_4: 4.132  loss_ce_5: 3.543  loss_mask_5: 1.544  loss_dice_5: 4.105  loss_ce_6: 3.441  loss_mask_6: 1.598  loss_dice_6: 3.98  loss_ce_7: 3.527  loss_mask_7: 1.505  loss_dice_7: 4.098  loss_ce_8: 3.634  loss_mask_8: 1.672  loss_dice_8: 4.213  loss_mars: 0.9186    time: 5.6203  last_time: 5.1373  data_time: 0.0037  last_data_time: 0.0084   lr: 0.0001  max_mem: 0M
[10/19 16:36:11] d2.utils.events INFO:  eta: 6:15:15  iter: 11599  total_loss: 90.78  loss_ce: 2.709  loss_mask: 1.492  loss_dice: 4.41  loss_ce_0: 3.776  loss_mask_0: 1.437  loss_dice_0: 4.27  loss_ce_1: 3.356  loss_mask_1: 1.374  loss_dice_1: 4.277  loss_ce_2: 3.212  loss_mask_2: 1.371  loss_dice_2: 4.2  loss_ce_3: 3.204  loss_mask_3: 1.394  loss_dice_3: 4.228  loss_ce_4: 2.922  loss_mask_4: 1.464  loss_dice_4: 4.23  loss_ce_5: 2.756  loss_mask_5: 1.519  loss_dice_5: 4.308  loss_ce_6: 2.984  loss_mask_6: 1.404  loss_dice_6: 4.328  loss_ce_7: 2.672  loss_mask_7: 1.361  loss_dice_7: 4.19  loss_ce_8: 2.829  loss_mask_8: 1.516  loss_dice_8: 4.435  loss_mars: 0.8741    time: 5.6200  last_time: 3.7577  data_time: 0.0023  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 16:37:57] d2.utils.events INFO:  eta: 6:11:12  iter: 11619  total_loss: 88.77  loss_ce: 2.402  loss_mask: 1.569  loss_dice: 3.99  loss_ce_0: 2.922  loss_mask_0: 1.713  loss_dice_0: 4.037  loss_ce_1: 2.669  loss_mask_1: 1.535  loss_dice_1: 4.12  loss_ce_2: 2.786  loss_mask_2: 1.876  loss_dice_2: 4.115  loss_ce_3: 2.768  loss_mask_3: 1.715  loss_dice_3: 4.137  loss_ce_4: 2.677  loss_mask_4: 1.821  loss_dice_4: 3.917  loss_ce_5: 2.468  loss_mask_5: 1.638  loss_dice_5: 4.054  loss_ce_6: 2.575  loss_mask_6: 1.646  loss_dice_6: 3.956  loss_ce_7: 2.293  loss_mask_7: 1.642  loss_dice_7: 4.052  loss_ce_8: 2.468  loss_mask_8: 1.464  loss_dice_8: 3.963  loss_mars: 0.8004    time: 5.6194  last_time: 5.8757  data_time: 0.0026  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 16:39:49] d2.utils.events INFO:  eta: 6:09:29  iter: 11639  total_loss: 96.8  loss_ce: 3.964  loss_mask: 1.07  loss_dice: 4.25  loss_ce_0: 4.02  loss_mask_0: 0.9508  loss_dice_0: 4.297  loss_ce_1: 3.804  loss_mask_1: 1.033  loss_dice_1: 4.183  loss_ce_2: 3.748  loss_mask_2: 1.071  loss_dice_2: 4.376  loss_ce_3: 3.663  loss_mask_3: 1.08  loss_dice_3: 4.328  loss_ce_4: 3.684  loss_mask_4: 1.149  loss_dice_4: 4.221  loss_ce_5: 3.734  loss_mask_5: 0.8657  loss_dice_5: 4.223  loss_ce_6: 3.639  loss_mask_6: 1.05  loss_dice_6: 4.204  loss_ce_7: 3.926  loss_mask_7: 1.101  loss_dice_7: 4.275  loss_ce_8: 3.896  loss_mask_8: 0.9936  loss_dice_8: 4.285  loss_mars: 0.5203    time: 5.6194  last_time: 5.7972  data_time: 0.0027  last_data_time: 0.0025   lr: 0.0001  max_mem: 0M
[10/19 16:41:39] d2.utils.events INFO:  eta: 6:05:58  iter: 11659  total_loss: 99.74  loss_ce: 3.442  loss_mask: 1.002  loss_dice: 4.278  loss_ce_0: 4.008  loss_mask_0: 0.9294  loss_dice_0: 4.391  loss_ce_1: 3.639  loss_mask_1: 0.8103  loss_dice_1: 4.309  loss_ce_2: 3.7  loss_mask_2: 0.965  loss_dice_2: 4.356  loss_ce_3: 3.605  loss_mask_3: 0.8409  loss_dice_3: 4.51  loss_ce_4: 3.542  loss_mask_4: 0.9258  loss_dice_4: 4.335  loss_ce_5: 3.546  loss_mask_5: 0.977  loss_dice_5: 4.299  loss_ce_6: 3.548  loss_mask_6: 1.137  loss_dice_6: 4.403  loss_ce_7: 3.655  loss_mask_7: 0.9697  loss_dice_7: 4.284  loss_ce_8: 3.474  loss_mask_8: 1.009  loss_dice_8: 4.39  loss_mars: 0.7418    time: 5.6191  last_time: 6.1375  data_time: 0.0033  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 16:43:30] d2.utils.events INFO:  eta: 6:04:08  iter: 11679  total_loss: 95.91  loss_ce: 4.252  loss_mask: 1.015  loss_dice: 4.522  loss_ce_0: 4.686  loss_mask_0: 0.9208  loss_dice_0: 4.406  loss_ce_1: 4.359  loss_mask_1: 0.9849  loss_dice_1: 4.486  loss_ce_2: 4.003  loss_mask_2: 0.9949  loss_dice_2: 4.445  loss_ce_3: 4.106  loss_mask_3: 0.8717  loss_dice_3: 4.491  loss_ce_4: 3.911  loss_mask_4: 0.9346  loss_dice_4: 4.427  loss_ce_5: 3.762  loss_mask_5: 0.9199  loss_dice_5: 4.434  loss_ce_6: 3.961  loss_mask_6: 0.9382  loss_dice_6: 4.424  loss_ce_7: 4.109  loss_mask_7: 0.9907  loss_dice_7: 4.45  loss_ce_8: 4.21  loss_mask_8: 1.107  loss_dice_8: 4.358  loss_mars: 0.8003    time: 5.6190  last_time: 5.7640  data_time: 0.0027  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 16:45:18] d2.utils.events INFO:  eta: 6:01:42  iter: 11699  total_loss: 92.64  loss_ce: 3.675  loss_mask: 1.465  loss_dice: 4.214  loss_ce_0: 4.325  loss_mask_0: 1.489  loss_dice_0: 4.084  loss_ce_1: 3.901  loss_mask_1: 1.494  loss_dice_1: 4.132  loss_ce_2: 4.03  loss_mask_2: 1.628  loss_dice_2: 4.227  loss_ce_3: 3.925  loss_mask_3: 1.47  loss_dice_3: 4.169  loss_ce_4: 3.708  loss_mask_4: 1.439  loss_dice_4: 3.967  loss_ce_5: 3.509  loss_mask_5: 1.67  loss_dice_5: 4.265  loss_ce_6: 3.466  loss_mask_6: 1.636  loss_dice_6: 4.218  loss_ce_7: 3.682  loss_mask_7: 1.469  loss_dice_7: 4.133  loss_ce_8: 3.541  loss_mask_8: 1.465  loss_dice_8: 4.069  loss_mars: 0.6358    time: 5.6186  last_time: 5.8912  data_time: 0.0034  last_data_time: 0.0083   lr: 0.0001  max_mem: 0M
[10/19 16:47:07] d2.utils.events INFO:  eta: 5:57:38  iter: 11719  total_loss: 91.93  loss_ce: 3.482  loss_mask: 1.078  loss_dice: 4.021  loss_ce_0: 3.594  loss_mask_0: 1.001  loss_dice_0: 4.241  loss_ce_1: 3.435  loss_mask_1: 0.9132  loss_dice_1: 4.515  loss_ce_2: 3.189  loss_mask_2: 1.008  loss_dice_2: 4.488  loss_ce_3: 3.328  loss_mask_3: 0.9787  loss_dice_3: 4.004  loss_ce_4: 3.341  loss_mask_4: 0.9283  loss_dice_4: 4.318  loss_ce_5: 3.375  loss_mask_5: 1.144  loss_dice_5: 4.042  loss_ce_6: 3.358  loss_mask_6: 0.983  loss_dice_6: 4.147  loss_ce_7: 3.401  loss_mask_7: 1.042  loss_dice_7: 4.011  loss_ce_8: 3.593  loss_mask_8: 1.139  loss_dice_8: 4.044  loss_mars: 0.5854    time: 5.6182  last_time: 3.9383  data_time: 0.0025  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 16:48:59] d2.utils.events INFO:  eta: 5:57:14  iter: 11739  total_loss: 80.34  loss_ce: 2.412  loss_mask: 1.312  loss_dice: 3.787  loss_ce_0: 2.487  loss_mask_0: 1.244  loss_dice_0: 3.956  loss_ce_1: 2.841  loss_mask_1: 1.317  loss_dice_1: 3.747  loss_ce_2: 2.373  loss_mask_2: 1.399  loss_dice_2: 3.796  loss_ce_3: 2.422  loss_mask_3: 1.302  loss_dice_3: 3.587  loss_ce_4: 2.61  loss_mask_4: 1.514  loss_dice_4: 3.831  loss_ce_5: 2.501  loss_mask_5: 1.205  loss_dice_5: 3.66  loss_ce_6: 2.52  loss_mask_6: 1.181  loss_dice_6: 3.767  loss_ce_7: 2.352  loss_mask_7: 1.315  loss_dice_7: 4.254  loss_ce_8: 2.307  loss_mask_8: 1.443  loss_dice_8: 3.76  loss_mars: 0.4994    time: 5.6182  last_time: 5.7867  data_time: 0.0034  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 16:50:50] d2.utils.events INFO:  eta: 5:55:56  iter: 11759  total_loss: 91.96  loss_ce: 3.168  loss_mask: 1.434  loss_dice: 4.263  loss_ce_0: 3.194  loss_mask_0: 1.373  loss_dice_0: 4.216  loss_ce_1: 3.439  loss_mask_1: 1.444  loss_dice_1: 4.025  loss_ce_2: 3.024  loss_mask_2: 1.308  loss_dice_2: 4.129  loss_ce_3: 3.218  loss_mask_3: 1.449  loss_dice_3: 4.172  loss_ce_4: 3.273  loss_mask_4: 1.395  loss_dice_4: 4.181  loss_ce_5: 3.177  loss_mask_5: 1.406  loss_dice_5: 4.277  loss_ce_6: 3.181  loss_mask_6: 1.307  loss_dice_6: 4.068  loss_ce_7: 3.11  loss_mask_7: 1.477  loss_dice_7: 4.095  loss_ce_8: 3.095  loss_mask_8: 1.493  loss_dice_8: 4.268  loss_mars: 0.79    time: 5.6180  last_time: 3.7963  data_time: 0.0027  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 16:52:38] d2.utils.events INFO:  eta: 5:53:14  iter: 11779  total_loss: 89.37  loss_ce: 3.291  loss_mask: 1.495  loss_dice: 4.358  loss_ce_0: 3.691  loss_mask_0: 1.445  loss_dice_0: 4.211  loss_ce_1: 3.511  loss_mask_1: 1.347  loss_dice_1: 4.186  loss_ce_2: 3.457  loss_mask_2: 1.41  loss_dice_2: 4.426  loss_ce_3: 3.387  loss_mask_3: 1.321  loss_dice_3: 4.374  loss_ce_4: 3.393  loss_mask_4: 1.304  loss_dice_4: 4.269  loss_ce_5: 3.334  loss_mask_5: 1.322  loss_dice_5: 4.065  loss_ce_6: 3.282  loss_mask_6: 1.373  loss_dice_6: 4.178  loss_ce_7: 3.242  loss_mask_7: 1.381  loss_dice_7: 4.104  loss_ce_8: 3.207  loss_mask_8: 1.341  loss_dice_8: 4.185  loss_mars: 0.6712    time: 5.6176  last_time: 6.1637  data_time: 0.0030  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/19 16:54:30] d2.utils.events INFO:  eta: 5:51:43  iter: 11799  total_loss: 97.27  loss_ce: 4.577  loss_mask: 0.7903  loss_dice: 4.312  loss_ce_0: 4.626  loss_mask_0: 0.7384  loss_dice_0: 4.29  loss_ce_1: 4.126  loss_mask_1: 0.7562  loss_dice_1: 4.4  loss_ce_2: 4.304  loss_mask_2: 0.7699  loss_dice_2: 4.504  loss_ce_3: 4.709  loss_mask_3: 0.7667  loss_dice_3: 4.466  loss_ce_4: 4.35  loss_mask_4: 0.7371  loss_dice_4: 4.439  loss_ce_5: 4.377  loss_mask_5: 0.7693  loss_dice_5: 4.4  loss_ce_6: 4.419  loss_mask_6: 0.7818  loss_dice_6: 4.299  loss_ce_7: 4.398  loss_mask_7: 0.7778  loss_dice_7: 4.338  loss_ce_8: 4.154  loss_mask_8: 0.7991  loss_dice_8: 4.319  loss_mars: 0.8448    time: 5.6176  last_time: 5.0991  data_time: 0.0030  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/19 16:56:25] d2.utils.events INFO:  eta: 5:51:36  iter: 11819  total_loss: 80.23  loss_ce: 2.752  loss_mask: 1.044  loss_dice: 4.183  loss_ce_0: 3.043  loss_mask_0: 0.999  loss_dice_0: 4.226  loss_ce_1: 2.826  loss_mask_1: 0.9945  loss_dice_1: 4.096  loss_ce_2: 2.805  loss_mask_2: 1.187  loss_dice_2: 4.099  loss_ce_3: 2.909  loss_mask_3: 1.09  loss_dice_3: 4.029  loss_ce_4: 2.813  loss_mask_4: 0.9775  loss_dice_4: 3.779  loss_ce_5: 2.746  loss_mask_5: 1.001  loss_dice_5: 3.926  loss_ce_6: 2.762  loss_mask_6: 1.065  loss_dice_6: 3.741  loss_ce_7: 2.693  loss_mask_7: 0.9493  loss_dice_7: 3.764  loss_ce_8: 2.802  loss_mask_8: 0.9991  loss_dice_8: 4.238  loss_mars: 0.8686    time: 5.6179  last_time: 5.1763  data_time: 0.0024  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 16:58:18] d2.utils.events INFO:  eta: 5:50:48  iter: 11839  total_loss: 91.4  loss_ce: 3.16  loss_mask: 1.19  loss_dice: 4.381  loss_ce_0: 3.292  loss_mask_0: 1.24  loss_dice_0: 4.274  loss_ce_1: 3.01  loss_mask_1: 1.207  loss_dice_1: 4.239  loss_ce_2: 3.047  loss_mask_2: 1.165  loss_dice_2: 4.313  loss_ce_3: 3.208  loss_mask_3: 1.245  loss_dice_3: 4.25  loss_ce_4: 3.034  loss_mask_4: 1.121  loss_dice_4: 4.423  loss_ce_5: 3.084  loss_mask_5: 1.197  loss_dice_5: 4.234  loss_ce_6: 3.007  loss_mask_6: 1.193  loss_dice_6: 4.314  loss_ce_7: 3.129  loss_mask_7: 1.173  loss_dice_7: 4.312  loss_ce_8: 3.2  loss_mask_8: 1.13  loss_dice_8: 4.314  loss_mars: 0.7363    time: 5.6179  last_time: 5.9605  data_time: 0.0025  last_data_time: 0.0078   lr: 0.0001  max_mem: 0M
[10/19 17:00:07] d2.utils.events INFO:  eta: 5:47:28  iter: 11859  total_loss: 92.78  loss_ce: 3.39  loss_mask: 1.075  loss_dice: 4.004  loss_ce_0: 4.074  loss_mask_0: 1.141  loss_dice_0: 4.292  loss_ce_1: 3.452  loss_mask_1: 1.045  loss_dice_1: 4.212  loss_ce_2: 3.483  loss_mask_2: 1.096  loss_dice_2: 4.126  loss_ce_3: 3.601  loss_mask_3: 1.108  loss_dice_3: 4.045  loss_ce_4: 3.291  loss_mask_4: 1.073  loss_dice_4: 4.188  loss_ce_5: 3.509  loss_mask_5: 1.084  loss_dice_5: 4.115  loss_ce_6: 3.488  loss_mask_6: 1.081  loss_dice_6: 3.933  loss_ce_7: 3.536  loss_mask_7: 1.138  loss_dice_7: 4.07  loss_ce_8: 3.171  loss_mask_8: 1.162  loss_dice_8: 4.211  loss_mars: 0.8779    time: 5.6176  last_time: 5.2345  data_time: 0.0028  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 17:02:00] d2.utils.events INFO:  eta: 5:46:03  iter: 11879  total_loss: 91.54  loss_ce: 3.61  loss_mask: 1.211  loss_dice: 4.342  loss_ce_0: 3.966  loss_mask_0: 1.041  loss_dice_0: 4.403  loss_ce_1: 3.847  loss_mask_1: 1.05  loss_dice_1: 4.415  loss_ce_2: 3.657  loss_mask_2: 1.01  loss_dice_2: 4.466  loss_ce_3: 3.936  loss_mask_3: 0.9233  loss_dice_3: 4.452  loss_ce_4: 3.485  loss_mask_4: 0.9306  loss_dice_4: 4.456  loss_ce_5: 3.755  loss_mask_5: 1.185  loss_dice_5: 4.384  loss_ce_6: 3.532  loss_mask_6: 1.112  loss_dice_6: 4.386  loss_ce_7: 3.546  loss_mask_7: 1.043  loss_dice_7: 4.46  loss_ce_8: 3.537  loss_mask_8: 1.048  loss_dice_8: 4.385  loss_mars: 0.9083    time: 5.6176  last_time: 4.9420  data_time: 0.0024  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 17:03:49] d2.utils.events INFO:  eta: 5:43:47  iter: 11899  total_loss: 91.48  loss_ce: 3.818  loss_mask: 1.085  loss_dice: 3.893  loss_ce_0: 3.432  loss_mask_0: 1.047  loss_dice_0: 3.97  loss_ce_1: 3.53  loss_mask_1: 1.081  loss_dice_1: 3.771  loss_ce_2: 3.508  loss_mask_2: 1.106  loss_dice_2: 4.006  loss_ce_3: 3.662  loss_mask_3: 1.066  loss_dice_3: 3.76  loss_ce_4: 3.495  loss_mask_4: 1.086  loss_dice_4: 4.092  loss_ce_5: 3.463  loss_mask_5: 1.103  loss_dice_5: 3.948  loss_ce_6: 3.518  loss_mask_6: 1.154  loss_dice_6: 3.947  loss_ce_7: 3.564  loss_mask_7: 1.074  loss_dice_7: 4.062  loss_ce_8: 3.649  loss_mask_8: 1.184  loss_dice_8: 4.024  loss_mars: 0.7511    time: 5.6173  last_time: 5.9045  data_time: 0.0029  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 17:05:38] d2.utils.events INFO:  eta: 5:41:59  iter: 11919  total_loss: 88.69  loss_ce: 3.446  loss_mask: 0.8002  loss_dice: 4.284  loss_ce_0: 3.516  loss_mask_0: 0.7749  loss_dice_0: 4.201  loss_ce_1: 3.489  loss_mask_1: 0.7934  loss_dice_1: 4.242  loss_ce_2: 3.383  loss_mask_2: 1.089  loss_dice_2: 4.323  loss_ce_3: 3.578  loss_mask_3: 0.7991  loss_dice_3: 4.204  loss_ce_4: 3.532  loss_mask_4: 0.7682  loss_dice_4: 4.178  loss_ce_5: 3.419  loss_mask_5: 0.7992  loss_dice_5: 4.326  loss_ce_6: 3.571  loss_mask_6: 0.8416  loss_dice_6: 4.468  loss_ce_7: 3.674  loss_mask_7: 0.8105  loss_dice_7: 4.201  loss_ce_8: 3.506  loss_mask_8: 0.8027  loss_dice_8: 4.302  loss_mars: 0.6855    time: 5.6170  last_time: 5.1117  data_time: 0.0026  last_data_time: 0.0084   lr: 0.0001  max_mem: 0M
[10/19 17:07:27] d2.utils.events INFO:  eta: 5:40:08  iter: 11939  total_loss: 96.97  loss_ce: 3.493  loss_mask: 1.244  loss_dice: 3.81  loss_ce_0: 3.662  loss_mask_0: 1.365  loss_dice_0: 4.024  loss_ce_1: 3.423  loss_mask_1: 1.241  loss_dice_1: 3.927  loss_ce_2: 3.413  loss_mask_2: 1.233  loss_dice_2: 3.878  loss_ce_3: 3.655  loss_mask_3: 1.267  loss_dice_3: 3.811  loss_ce_4: 3.595  loss_mask_4: 1.28  loss_dice_4: 3.86  loss_ce_5: 3.617  loss_mask_5: 1.278  loss_dice_5: 3.806  loss_ce_6: 3.392  loss_mask_6: 1.387  loss_dice_6: 3.841  loss_ce_7: 3.495  loss_mask_7: 1.277  loss_dice_7: 3.833  loss_ce_8: 3.475  loss_mask_8: 1.326  loss_dice_8: 3.937  loss_mars: 0.8561    time: 5.6166  last_time: 5.1316  data_time: 0.0048  last_data_time: 0.0072   lr: 0.0001  max_mem: 0M
[10/19 17:09:21] d2.utils.events INFO:  eta: 5:38:39  iter: 11959  total_loss: 94.25  loss_ce: 3.655  loss_mask: 1.035  loss_dice: 4.363  loss_ce_0: 3.476  loss_mask_0: 1.013  loss_dice_0: 4.263  loss_ce_1: 3.554  loss_mask_1: 0.9587  loss_dice_1: 4.187  loss_ce_2: 3.507  loss_mask_2: 1.044  loss_dice_2: 4.126  loss_ce_3: 3.68  loss_mask_3: 1.136  loss_dice_3: 4.11  loss_ce_4: 3.793  loss_mask_4: 1.09  loss_dice_4: 4.104  loss_ce_5: 3.833  loss_mask_5: 1.032  loss_dice_5: 4.14  loss_ce_6: 3.685  loss_mask_6: 1.019  loss_dice_6: 4.236  loss_ce_7: 3.43  loss_mask_7: 1.075  loss_dice_7: 4.256  loss_ce_8: 3.51  loss_mask_8: 1.09  loss_dice_8: 4.291  loss_mars: 0.7397    time: 5.6168  last_time: 5.8095  data_time: 0.0036  last_data_time: 0.0022   lr: 0.0001  max_mem: 0M
[10/19 17:11:15] d2.utils.events INFO:  eta: 5:36:26  iter: 11979  total_loss: 93.93  loss_ce: 3.679  loss_mask: 1.114  loss_dice: 4.416  loss_ce_0: 3.871  loss_mask_0: 1.138  loss_dice_0: 4.426  loss_ce_1: 3.365  loss_mask_1: 1.105  loss_dice_1: 4.335  loss_ce_2: 3.21  loss_mask_2: 1.26  loss_dice_2: 4.488  loss_ce_3: 3.554  loss_mask_3: 1.106  loss_dice_3: 4.416  loss_ce_4: 3.742  loss_mask_4: 1.161  loss_dice_4: 4.357  loss_ce_5: 3.606  loss_mask_5: 1.076  loss_dice_5: 4.411  loss_ce_6: 3.388  loss_mask_6: 1.189  loss_dice_6: 4.536  loss_ce_7: 3.339  loss_mask_7: 1.251  loss_dice_7: 4.417  loss_ce_8: 3.273  loss_mask_8: 1.139  loss_dice_8: 4.523  loss_mars: 0.5035    time: 5.6170  last_time: 5.1783  data_time: 0.0030  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 17:13:09] d2.utils.events INFO:  eta: 5:34:59  iter: 11999  total_loss: 86.34  loss_ce: 3.073  loss_mask: 1.413  loss_dice: 4.114  loss_ce_0: 2.906  loss_mask_0: 1.367  loss_dice_0: 3.965  loss_ce_1: 2.918  loss_mask_1: 1.089  loss_dice_1: 4.109  loss_ce_2: 2.66  loss_mask_2: 1.396  loss_dice_2: 4.149  loss_ce_3: 2.939  loss_mask_3: 1.492  loss_dice_3: 4.012  loss_ce_4: 3.027  loss_mask_4: 1.384  loss_dice_4: 3.966  loss_ce_5: 2.901  loss_mask_5: 1.298  loss_dice_5: 4.24  loss_ce_6: 2.813  loss_mask_6: 1.285  loss_dice_6: 4.031  loss_ce_7: 2.988  loss_mask_7: 1.224  loss_dice_7: 4.12  loss_ce_8: 2.798  loss_mask_8: 1.329  loss_dice_8: 3.95  loss_mars: 0.5188    time: 5.6172  last_time: 5.9493  data_time: 0.0031  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 17:15:01] d2.utils.events INFO:  eta: 5:32:45  iter: 12019  total_loss: 89.8  loss_ce: 3.103  loss_mask: 1.191  loss_dice: 4.047  loss_ce_0: 3.657  loss_mask_0: 1.145  loss_dice_0: 4.297  loss_ce_1: 3.323  loss_mask_1: 1.232  loss_dice_1: 4.252  loss_ce_2: 3.228  loss_mask_2: 1.16  loss_dice_2: 4.103  loss_ce_3: 3.269  loss_mask_3: 1.22  loss_dice_3: 4.176  loss_ce_4: 3.339  loss_mask_4: 1.184  loss_dice_4: 4.173  loss_ce_5: 3.35  loss_mask_5: 1.178  loss_dice_5: 3.945  loss_ce_6: 3.076  loss_mask_6: 1.085  loss_dice_6: 4  loss_ce_7: 3.191  loss_mask_7: 1.224  loss_dice_7: 3.957  loss_ce_8: 3.059  loss_mask_8: 1.285  loss_dice_8: 4.056  loss_mars: 0.8244    time: 5.6171  last_time: 4.6768  data_time: 0.0023  last_data_time: 0.0028   lr: 0.0001  max_mem: 0M
[10/19 17:16:54] d2.utils.events INFO:  eta: 5:32:05  iter: 12039  total_loss: 99.22  loss_ce: 4.151  loss_mask: 0.8135  loss_dice: 4.346  loss_ce_0: 5.387  loss_mask_0: 0.7891  loss_dice_0: 4.405  loss_ce_1: 4.436  loss_mask_1: 0.8022  loss_dice_1: 4.44  loss_ce_2: 4.153  loss_mask_2: 0.7851  loss_dice_2: 4.408  loss_ce_3: 4.288  loss_mask_3: 0.8031  loss_dice_3: 4.38  loss_ce_4: 4.108  loss_mask_4: 0.7773  loss_dice_4: 4.366  loss_ce_5: 3.997  loss_mask_5: 0.8588  loss_dice_5: 4.298  loss_ce_6: 3.959  loss_mask_6: 0.8832  loss_dice_6: 4.292  loss_ce_7: 4.267  loss_mask_7: 0.8098  loss_dice_7: 4.286  loss_ce_8: 4.165  loss_mask_8: 0.7528  loss_dice_8: 4.321  loss_mars: 0.8896    time: 5.6171  last_time: 5.1646  data_time: 0.0037  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 17:18:44] d2.utils.events INFO:  eta: 5:30:25  iter: 12059  total_loss: 91.86  loss_ce: 2.921  loss_mask: 1.479  loss_dice: 4.152  loss_ce_0: 2.836  loss_mask_0: 1.309  loss_dice_0: 4.096  loss_ce_1: 2.997  loss_mask_1: 1.427  loss_dice_1: 4.119  loss_ce_2: 3.168  loss_mask_2: 1.353  loss_dice_2: 4.025  loss_ce_3: 2.84  loss_mask_3: 1.397  loss_dice_3: 4.136  loss_ce_4: 2.986  loss_mask_4: 1.445  loss_dice_4: 4.107  loss_ce_5: 2.817  loss_mask_5: 1.473  loss_dice_5: 4.037  loss_ce_6: 2.881  loss_mask_6: 1.467  loss_dice_6: 4.03  loss_ce_7: 2.775  loss_mask_7: 1.375  loss_dice_7: 4.252  loss_ce_8: 2.707  loss_mask_8: 1.533  loss_dice_8: 4.219  loss_mars: 0.8527    time: 5.6169  last_time: 6.5258  data_time: 0.0029  last_data_time: 0.0027   lr: 0.0001  max_mem: 0M
[10/19 17:20:38] d2.utils.events INFO:  eta: 5:28:41  iter: 12079  total_loss: 94.67  loss_ce: 3.217  loss_mask: 1.377  loss_dice: 4.198  loss_ce_0: 3.398  loss_mask_0: 1.148  loss_dice_0: 4.01  loss_ce_1: 3.63  loss_mask_1: 1.044  loss_dice_1: 4.063  loss_ce_2: 3.434  loss_mask_2: 1.252  loss_dice_2: 4.154  loss_ce_3: 3.488  loss_mask_3: 1.206  loss_dice_3: 4.205  loss_ce_4: 3.576  loss_mask_4: 1.362  loss_dice_4: 4.228  loss_ce_5: 3.557  loss_mask_5: 1.271  loss_dice_5: 4.145  loss_ce_6: 3.641  loss_mask_6: 1.271  loss_dice_6: 4.255  loss_ce_7: 3.291  loss_mask_7: 1.383  loss_dice_7: 4.239  loss_ce_8: 3.213  loss_mask_8: 1.309  loss_dice_8: 4.252  loss_mars: 0.8032    time: 5.6171  last_time: 5.8637  data_time: 0.0039  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 17:22:27] d2.utils.events INFO:  eta: 5:26:31  iter: 12099  total_loss: 89.61  loss_ce: 3.149  loss_mask: 0.7946  loss_dice: 4.256  loss_ce_0: 3.889  loss_mask_0: 0.8186  loss_dice_0: 4.149  loss_ce_1: 3.33  loss_mask_1: 0.8829  loss_dice_1: 3.962  loss_ce_2: 3.306  loss_mask_2: 0.828  loss_dice_2: 4.114  loss_ce_3: 3.492  loss_mask_3: 0.7793  loss_dice_3: 4.046  loss_ce_4: 3.387  loss_mask_4: 0.793  loss_dice_4: 3.955  loss_ce_5: 3.256  loss_mask_5: 0.8367  loss_dice_5: 4.141  loss_ce_6: 3.22  loss_mask_6: 0.7695  loss_dice_6: 4.142  loss_ce_7: 3.1  loss_mask_7: 0.8161  loss_dice_7: 4.159  loss_ce_8: 2.972  loss_mask_8: 0.8748  loss_dice_8: 4.398  loss_mars: 0.7944    time: 5.6167  last_time: 3.8434  data_time: 0.0029  last_data_time: 0.0033   lr: 0.0001  max_mem: 0M
[10/19 17:24:19] d2.utils.events INFO:  eta: 5:24:23  iter: 12119  total_loss: 95.01  loss_ce: 3.085  loss_mask: 1.012  loss_dice: 4.335  loss_ce_0: 3.694  loss_mask_0: 1.127  loss_dice_0: 4.241  loss_ce_1: 3.284  loss_mask_1: 1.006  loss_dice_1: 4.225  loss_ce_2: 3.056  loss_mask_2: 0.9674  loss_dice_2: 4.369  loss_ce_3: 3.048  loss_mask_3: 1.022  loss_dice_3: 4.401  loss_ce_4: 3.14  loss_mask_4: 1.015  loss_dice_4: 4.29  loss_ce_5: 3.135  loss_mask_5: 1.045  loss_dice_5: 4.362  loss_ce_6: 3.05  loss_mask_6: 1.012  loss_dice_6: 4.301  loss_ce_7: 3.062  loss_mask_7: 1.045  loss_dice_7: 4.299  loss_ce_8: 2.982  loss_mask_8: 1.097  loss_dice_8: 4.39  loss_mars: 0.8115    time: 5.6167  last_time: 3.8455  data_time: 0.0036  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 17:26:10] d2.utils.events INFO:  eta: 5:21:40  iter: 12139  total_loss: 89.96  loss_ce: 3.247  loss_mask: 0.9202  loss_dice: 4.146  loss_ce_0: 3.843  loss_mask_0: 1.149  loss_dice_0: 4.303  loss_ce_1: 3.491  loss_mask_1: 1.118  loss_dice_1: 4.112  loss_ce_2: 3.262  loss_mask_2: 0.9897  loss_dice_2: 4.06  loss_ce_3: 3.246  loss_mask_3: 1.036  loss_dice_3: 4.053  loss_ce_4: 3.347  loss_mask_4: 1.022  loss_dice_4: 4.103  loss_ce_5: 3.277  loss_mask_5: 1.022  loss_dice_5: 4.062  loss_ce_6: 3.163  loss_mask_6: 1.023  loss_dice_6: 4.12  loss_ce_7: 3.196  loss_mask_7: 0.9683  loss_dice_7: 4.199  loss_ce_8: 3.008  loss_mask_8: 1.138  loss_dice_8: 4.249  loss_mars: 0.8168    time: 5.6166  last_time: 6.0841  data_time: 0.0035  last_data_time: 0.0019   lr: 0.0001  max_mem: 0M
[10/19 17:28:02] d2.utils.events INFO:  eta: 5:20:05  iter: 12159  total_loss: 87.9  loss_ce: 3.282  loss_mask: 0.9896  loss_dice: 4.312  loss_ce_0: 3.322  loss_mask_0: 1.162  loss_dice_0: 4.168  loss_ce_1: 3.046  loss_mask_1: 1.069  loss_dice_1: 4.25  loss_ce_2: 2.968  loss_mask_2: 1.082  loss_dice_2: 4.361  loss_ce_3: 3.262  loss_mask_3: 1.089  loss_dice_3: 4.357  loss_ce_4: 3.175  loss_mask_4: 1.033  loss_dice_4: 4.32  loss_ce_5: 3.137  loss_mask_5: 1.028  loss_dice_5: 4.295  loss_ce_6: 3.137  loss_mask_6: 1.057  loss_dice_6: 4.269  loss_ce_7: 3.011  loss_mask_7: 1.102  loss_dice_7: 4.282  loss_ce_8: 3.106  loss_mask_8: 1.077  loss_dice_8: 4.325  loss_mars: 0.5899    time: 5.6166  last_time: 5.9203  data_time: 0.0032  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 17:29:55] d2.utils.events INFO:  eta: 5:17:33  iter: 12179  total_loss: 88.72  loss_ce: 3.275  loss_mask: 0.727  loss_dice: 4.027  loss_ce_0: 3.592  loss_mask_0: 0.6628  loss_dice_0: 4.094  loss_ce_1: 3.308  loss_mask_1: 0.6654  loss_dice_1: 4.087  loss_ce_2: 3.177  loss_mask_2: 0.7417  loss_dice_2: 4.147  loss_ce_3: 3.317  loss_mask_3: 0.67  loss_dice_3: 3.948  loss_ce_4: 3.377  loss_mask_4: 0.6335  loss_dice_4: 3.955  loss_ce_5: 3.366  loss_mask_5: 0.6563  loss_dice_5: 3.963  loss_ce_6: 3.23  loss_mask_6: 0.7157  loss_dice_6: 4.024  loss_ce_7: 3.342  loss_mask_7: 0.6874  loss_dice_7: 4.113  loss_ce_8: 3.229  loss_mask_8: 0.7738  loss_dice_8: 4.107  loss_mars: 0.7797    time: 5.6166  last_time: 5.2191  data_time: 0.0025  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 17:31:46] d2.utils.events INFO:  eta: 5:15:26  iter: 12199  total_loss: 90.12  loss_ce: 2.924  loss_mask: 0.7958  loss_dice: 4.42  loss_ce_0: 3.996  loss_mask_0: 0.822  loss_dice_0: 4.116  loss_ce_1: 2.827  loss_mask_1: 0.8939  loss_dice_1: 4.265  loss_ce_2: 3.122  loss_mask_2: 1.171  loss_dice_2: 4.341  loss_ce_3: 3.332  loss_mask_3: 1.19  loss_dice_3: 4.405  loss_ce_4: 3.537  loss_mask_4: 1.17  loss_dice_4: 4.421  loss_ce_5: 3.499  loss_mask_5: 0.9842  loss_dice_5: 4.362  loss_ce_6: 3.036  loss_mask_6: 1.058  loss_dice_6: 4.462  loss_ce_7: 2.934  loss_mask_7: 0.9101  loss_dice_7: 4.264  loss_ce_8: 2.796  loss_mask_8: 0.8566  loss_dice_8: 4.233  loss_mars: 0.1543    time: 5.6165  last_time: 5.6952  data_time: 0.0027  last_data_time: 0.0041   lr: 0.0001  max_mem: 0M
[10/19 17:33:33] d2.utils.events INFO:  eta: 5:13:52  iter: 12219  total_loss: 86.15  loss_ce: 2.938  loss_mask: 0.9692  loss_dice: 4.198  loss_ce_0: 2.836  loss_mask_0: 0.8666  loss_dice_0: 4.028  loss_ce_1: 3.064  loss_mask_1: 0.9155  loss_dice_1: 4.301  loss_ce_2: 2.776  loss_mask_2: 0.993  loss_dice_2: 4.226  loss_ce_3: 2.885  loss_mask_3: 0.9538  loss_dice_3: 4.136  loss_ce_4: 2.834  loss_mask_4: 0.9895  loss_dice_4: 4.201  loss_ce_5: 2.796  loss_mask_5: 1.037  loss_dice_5: 4.169  loss_ce_6: 2.88  loss_mask_6: 0.8717  loss_dice_6: 4.113  loss_ce_7: 3.135  loss_mask_7: 0.8233  loss_dice_7: 4.289  loss_ce_8: 3.026  loss_mask_8: 0.8972  loss_dice_8: 4.297  loss_mars: 0.2774    time: 5.6160  last_time: 5.2161  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 17:35:24] d2.utils.events INFO:  eta: 5:11:15  iter: 12239  total_loss: 84.34  loss_ce: 2.762  loss_mask: 1.214  loss_dice: 4.005  loss_ce_0: 2.899  loss_mask_0: 1.06  loss_dice_0: 3.996  loss_ce_1: 2.806  loss_mask_1: 1.17  loss_dice_1: 3.998  loss_ce_2: 2.507  loss_mask_2: 1.426  loss_dice_2: 4.14  loss_ce_3: 2.478  loss_mask_3: 1.167  loss_dice_3: 4.055  loss_ce_4: 2.604  loss_mask_4: 1.468  loss_dice_4: 4.01  loss_ce_5: 2.65  loss_mask_5: 1.311  loss_dice_5: 4.066  loss_ce_6: 2.62  loss_mask_6: 1.182  loss_dice_6: 4.101  loss_ce_7: 2.927  loss_mask_7: 1.102  loss_dice_7: 4.13  loss_ce_8: 2.716  loss_mask_8: 1.114  loss_dice_8: 4.18  loss_mars: 0.825    time: 5.6158  last_time: 5.9333  data_time: 0.0042  last_data_time: 0.0018   lr: 0.0001  max_mem: 0M
[10/19 17:37:14] d2.utils.events INFO:  eta: 5:09:54  iter: 12259  total_loss: 86.4  loss_ce: 2.629  loss_mask: 1.228  loss_dice: 3.987  loss_ce_0: 2.872  loss_mask_0: 1.183  loss_dice_0: 4.093  loss_ce_1: 2.646  loss_mask_1: 1.16  loss_dice_1: 4.112  loss_ce_2: 2.435  loss_mask_2: 1.26  loss_dice_2: 4.079  loss_ce_3: 2.585  loss_mask_3: 1.252  loss_dice_3: 4.102  loss_ce_4: 2.667  loss_mask_4: 1.271  loss_dice_4: 4.194  loss_ce_5: 2.717  loss_mask_5: 1.304  loss_dice_5: 4.132  loss_ce_6: 2.623  loss_mask_6: 1.226  loss_dice_6: 4.085  loss_ce_7: 2.631  loss_mask_7: 1.254  loss_dice_7: 4.07  loss_ce_8: 2.569  loss_mask_8: 1.197  loss_dice_8: 4.055  loss_mars: 0.7213    time: 5.6157  last_time: 5.9748  data_time: 0.0030  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/19 17:39:06] d2.utils.events INFO:  eta: 5:07:51  iter: 12279  total_loss: 90.69  loss_ce: 3.232  loss_mask: 1.119  loss_dice: 4.29  loss_ce_0: 3.497  loss_mask_0: 1.032  loss_dice_0: 4.258  loss_ce_1: 3.576  loss_mask_1: 1.161  loss_dice_1: 4.211  loss_ce_2: 3.243  loss_mask_2: 1.483  loss_dice_2: 4.25  loss_ce_3: 3.395  loss_mask_3: 1.109  loss_dice_3: 4.234  loss_ce_4: 3.468  loss_mask_4: 1.047  loss_dice_4: 4.301  loss_ce_5: 3.575  loss_mask_5: 1.047  loss_dice_5: 4.192  loss_ce_6: 3.595  loss_mask_6: 1.068  loss_dice_6: 3.897  loss_ce_7: 3.418  loss_mask_7: 1.208  loss_dice_7: 4.287  loss_ce_8: 3.287  loss_mask_8: 1.178  loss_dice_8: 4.255  loss_mars: 0.7665    time: 5.6156  last_time: 4.8159  data_time: 0.0023  last_data_time: 0.0030   lr: 0.0001  max_mem: 0M
[10/19 17:41:02] d2.utils.events INFO:  eta: 5:06:42  iter: 12299  total_loss: 90.3  loss_ce: 3.131  loss_mask: 1.141  loss_dice: 4.302  loss_ce_0: 3.418  loss_mask_0: 0.9486  loss_dice_0: 4.363  loss_ce_1: 3.268  loss_mask_1: 1.004  loss_dice_1: 4.37  loss_ce_2: 3.476  loss_mask_2: 1.051  loss_dice_2: 4.38  loss_ce_3: 3.516  loss_mask_3: 1.204  loss_dice_3: 4.351  loss_ce_4: 3.425  loss_mask_4: 1.119  loss_dice_4: 4.38  loss_ce_5: 3.385  loss_mask_5: 1.119  loss_dice_5: 4.361  loss_ce_6: 3.315  loss_mask_6: 1.141  loss_dice_6: 4.312  loss_ce_7: 3.1  loss_mask_7: 0.9922  loss_dice_7: 4.41  loss_ce_8: 3.064  loss_mask_8: 1.075  loss_dice_8: 4.298  loss_mars: 0.8817    time: 5.6160  last_time: 5.1844  data_time: 0.0026  last_data_time: 0.0040   lr: 0.0001  max_mem: 0M
[10/19 17:42:55] d2.utils.events INFO:  eta: 5:03:05  iter: 12319  total_loss: 89.75  loss_ce: 2.938  loss_mask: 1.63  loss_dice: 4.029  loss_ce_0: 2.928  loss_mask_0: 1.45  loss_dice_0: 3.929  loss_ce_1: 3.008  loss_mask_1: 1.261  loss_dice_1: 3.946  loss_ce_2: 2.918  loss_mask_2: 1.765  loss_dice_2: 3.912  loss_ce_3: 3.086  loss_mask_3: 1.418  loss_dice_3: 4.039  loss_ce_4: 3.098  loss_mask_4: 1.515  loss_dice_4: 4.005  loss_ce_5: 3.186  loss_mask_5: 1.39  loss_dice_5: 3.704  loss_ce_6: 3.262  loss_mask_6: 1.469  loss_dice_6: 3.631  loss_ce_7: 3.1  loss_mask_7: 1.357  loss_dice_7: 3.834  loss_ce_8: 3.09  loss_mask_8: 1.367  loss_dice_8: 3.887  loss_mars: 0.7404    time: 5.6161  last_time: 5.2165  data_time: 0.0024  last_data_time: 0.0021   lr: 0.0001  max_mem: 0M
[10/19 17:44:52] d2.utils.events INFO:  eta: 5:01:31  iter: 12339  total_loss: 89.63  loss_ce: 2.846  loss_mask: 0.8077  loss_dice: 4.433  loss_ce_0: 3.251  loss_mask_0: 0.835  loss_dice_0: 4.074  loss_ce_1: 3.07  loss_mask_1: 0.7543  loss_dice_1: 4.171  loss_ce_2: 3.126  loss_mask_2: 0.7498  loss_dice_2: 4.322  loss_ce_3: 3.141  loss_mask_3: 0.7696  loss_dice_3: 4.329  loss_ce_4: 3.314  loss_mask_4: 0.7364  loss_dice_4: 4.341  loss_ce_5: 3.27  loss_mask_5: 0.7326  loss_dice_5: 4.184  loss_ce_6: 3.23  loss_mask_6: 0.756  loss_dice_6: 4.153  loss_ce_7: 2.983  loss_mask_7: 0.7631  loss_dice_7: 4.479  loss_ce_8: 2.934  loss_mask_8: 0.8985  loss_dice_8: 4.48  loss_mars: 0.6366    time: 5.6164  last_time: 8.3197  data_time: 0.0026  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 17:46:34] d2.utils.events INFO:  eta: 4:59:24  iter: 12359  total_loss: 86.71  loss_ce: 2.91  loss_mask: 0.9899  loss_dice: 4.277  loss_ce_0: 3.789  loss_mask_0: 0.9372  loss_dice_0: 4.213  loss_ce_1: 3.258  loss_mask_1: 0.9175  loss_dice_1: 4.169  loss_ce_2: 3.532  loss_mask_2: 0.9012  loss_dice_2: 4.079  loss_ce_3: 3.67  loss_mask_3: 0.9418  loss_dice_3: 4.131  loss_ce_4: 3.628  loss_mask_4: 0.9286  loss_dice_4: 4.138  loss_ce_5: 3.323  loss_mask_5: 0.9102  loss_dice_5: 4.112  loss_ce_6: 3.136  loss_mask_6: 1.066  loss_dice_6: 4.076  loss_ce_7: 3.001  loss_mask_7: 1.044  loss_dice_7: 4.207  loss_ce_8: 3.004  loss_mask_8: 1.011  loss_dice_8: 4.188  loss_mars: 0.8888    time: 5.6155  last_time: 5.7732  data_time: 0.0021  last_data_time: 0.0023   lr: 0.0001  max_mem: 0M
[10/19 17:48:31] d2.utils.events INFO:  eta: 4:58:16  iter: 12379  total_loss: 90.62  loss_ce: 3.658  loss_mask: 0.8825  loss_dice: 4.258  loss_ce_0: 3.355  loss_mask_0: 0.8727  loss_dice_0: 4.194  loss_ce_1: 3.413  loss_mask_1: 0.8522  loss_dice_1: 4.172  loss_ce_2: 3.422  loss_mask_2: 0.8625  loss_dice_2: 4.196  loss_ce_3: 3.524  loss_mask_3: 0.8242  loss_dice_3: 4.154  loss_ce_4: 3.527  loss_mask_4: 0.8745  loss_dice_4: 4.209  loss_ce_5: 3.409  loss_mask_5: 0.9141  loss_dice_5: 4.148  loss_ce_6: 3.541  loss_mask_6: 0.8437  loss_dice_6: 4.203  loss_ce_7: 3.33  loss_mask_7: 0.9049  loss_dice_7: 4.255  loss_ce_8: 3.426  loss_mask_8: 0.9458  loss_dice_8: 4.253  loss_mars: 0.8949    time: 5.6160  last_time: 5.2934  data_time: 0.0032  last_data_time: 0.0079   lr: 0.0001  max_mem: 0M
[10/19 17:50:25] d2.utils.events INFO:  eta: 4:56:01  iter: 12399  total_loss: 94.53  loss_ce: 3.409  loss_mask: 1.09  loss_dice: 4.145  loss_ce_0: 3.487  loss_mask_0: 1.201  loss_dice_0: 3.965  loss_ce_1: 3.796  loss_mask_1: 0.971  loss_dice_1: 4.039  loss_ce_2: 3.948  loss_mask_2: 1.003  loss_dice_2: 4.037  loss_ce_3: 3.962  loss_mask_3: 1.005  loss_dice_3: 3.949  loss_ce_4: 4.023  loss_mask_4: 1.059  loss_dice_4: 3.954  loss_ce_5: 4.241  loss_mask_5: 0.9849  loss_dice_5: 3.979  loss_ce_6: 4.097  loss_mask_6: 0.983  loss_dice_6: 3.919  loss_ce_7: 3.6  loss_mask_7: 1.01  loss_dice_7: 4.103  loss_ce_8: 3.484  loss_mask_8: 1.018  loss_dice_8: 4.151  loss_mars: 0.8676    time: 5.6160  last_time: 5.1150  data_time: 0.0033  last_data_time: 0.0029   lr: 0.0001  max_mem: 0M
[10/19 17:52:18] d2.utils.events INFO:  eta: 4:53:46  iter: 12419  total_loss: 95.34  loss_ce: 3.886  loss_mask: 0.8128  loss_dice: 4.276  loss_ce_0: 4.174  loss_mask_0: 0.6689  loss_dice_0: 4.397  loss_ce_1: 3.684  loss_mask_1: 0.7011  loss_dice_1: 4.394  loss_ce_2: 3.673  loss_mask_2: 0.7183  loss_dice_2: 4.377  loss_ce_3: 3.81  loss_mask_3: 0.8103  loss_dice_3: 4.307  loss_ce_4: 3.9  loss_mask_4: 0.775  loss_dice_4: 4.315  loss_ce_5: 3.964  loss_mask_5: 0.7197  loss_dice_5: 4.452  loss_ce_6: 3.956  loss_mask_6: 0.8014  loss_dice_6: 4.363  loss_ce_7: 3.965  loss_mask_7: 0.7845  loss_dice_7: 4.377  loss_ce_8: 3.845  loss_mask_8: 0.7286  loss_dice_8: 4.318  loss_mars: 0.8205    time: 5.6161  last_time: 8.4482  data_time: 0.0030  last_data_time: 0.0084   lr: 0.0001  max_mem: 0M
[10/19 17:54:07] d2.utils.events INFO:  eta: 4:51:56  iter: 12439  total_loss: 78.41  loss_ce: 2.426  loss_mask: 0.8011  loss_dice: 4.371  loss_ce_0: 2.265  loss_mask_0: 0.6521  loss_dice_0: 4.233  loss_ce_1: 2.216  loss_mask_1: 0.7945  loss_dice_1: 4.245  loss_ce_2: 2.217  loss_mask_2: 0.706  loss_dice_2: 4.197  loss_ce_3: 2.323  loss_mask_3: 0.7581  loss_dice_3: 4.288  loss_ce_4: 2.313  loss_mask_4: 0.6469  loss_dice_4: 4.3  loss_ce_5: 2.424  loss_mask_5: 0.7036  loss_dice_5: 4.195  loss_ce_6: 2.425  loss_mask_6: 0.723  loss_dice_6: 4.111  loss_ce_7: 2.475  loss_mask_7: 0.8196  loss_dice_7: 4.465  loss_ce_8: 2.39  loss_mask_8: 0.8201  loss_dice_8: 4.196  loss_mars: 0.7198    time: 5.6158  last_time: 5.0260  data_time: 0.0028  last_data_time: 0.0031   lr: 0.0001  max_mem: 0M
[10/19 17:55:56] d2.utils.events INFO:  eta: 4:49:59  iter: 12459  total_loss: 89.37  loss_ce: 2.615  loss_mask: 1.572  loss_dice: 4.028  loss_ce_0: 2.964  loss_mask_0: 1.717  loss_dice_0: 4.167  loss_ce_1: 2.545  loss_mask_1: 1.698  loss_dice_1: 3.973  loss_ce_2: 2.625  loss_mask_2: 1.732  loss_dice_2: 4.17  loss_ce_3: 2.765  loss_mask_3: 1.524  loss_dice_3: 4.104  loss_ce_4: 2.572  loss_mask_4: 1.636  loss_dice_4: 4.187  loss_ce_5: 2.756  loss_mask_5: 1.619  loss_dice_5: 4.169  loss_ce_6: 2.666  loss_mask_6: 1.547  loss_dice_6: 4.093  loss_ce_7: 2.859  loss_mask_7: 1.548  loss_dice_7: 4.173  loss_ce_8: 2.74  loss_mask_8: 1.628  loss_dice_8: 4.069  loss_mars: 0.8792    time: 5.6155  last_time: 4.0717  data_time: 0.0023  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 17:57:45] d2.utils.events INFO:  eta: 4:48:16  iter: 12479  total_loss: 88.47  loss_ce: 3.124  loss_mask: 1.696  loss_dice: 3.835  loss_ce_0: 3.387  loss_mask_0: 1.673  loss_dice_0: 4.075  loss_ce_1: 3.095  loss_mask_1: 1.579  loss_dice_1: 4.084  loss_ce_2: 3.276  loss_mask_2: 1.717  loss_dice_2: 3.948  loss_ce_3: 3.121  loss_mask_3: 1.666  loss_dice_3: 4.049  loss_ce_4: 3.24  loss_mask_4: 1.818  loss_dice_4: 4.163  loss_ce_5: 3.244  loss_mask_5: 1.709  loss_dice_5: 4.042  loss_ce_6: 3.168  loss_mask_6: 1.655  loss_dice_6: 3.919  loss_ce_7: 3.141  loss_mask_7: 1.823  loss_dice_7: 3.803  loss_ce_8: 3.071  loss_mask_8: 1.69  loss_dice_8: 3.901  loss_mars: 0.7515    time: 5.6152  last_time: 5.1201  data_time: 0.0033  last_data_time: 0.0020   lr: 0.0001  max_mem: 0M
[10/19 17:59:31] d2.utils.events INFO:  eta: 4:46:03  iter: 12499  total_loss: 92.01  loss_ce: 2.993  loss_mask: 1.571  loss_dice: 4.217  loss_ce_0: 3.332  loss_mask_0: 1.585  loss_dice_0: 4.187  loss_ce_1: 3.29  loss_mask_1: 1.654  loss_dice_1: 4.188  loss_ce_2: 3.468  loss_mask_2: 1.504  loss_dice_2: 4.23  loss_ce_3: 3.095  loss_mask_3: 1.582  loss_dice_3: 4.181  loss_ce_4: 3.018  loss_mask_4: 1.517  loss_dice_4: 4.273  loss_ce_5: 3.195  loss_mask_5: 1.485  loss_dice_5: 4.182  loss_ce_6: 3.169  loss_mask_6: 1.571  loss_dice_6: 4.253  loss_ce_7: 3.076  loss_mask_7: 1.621  loss_dice_7: 4.401  loss_ce_8: 3.067  loss_mask_8: 1.826  loss_dice_8: 4.335  loss_mars: 0.8576    time: 5.6146  last_time: 4.7741  data_time: 0.0021  last_data_time: 0.0017   lr: 0.0001  max_mem: 0M
[10/19 17:59:50] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0012503.pth
[10/19 17:59:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 17:59:50] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 17:59:50] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 17:59:50] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 18:01:19] d2.utils.events INFO:  eta: 4:43:59  iter: 12519  total_loss: 84.34  loss_ce: 2.924  loss_mask: 1.49  loss_dice: 4.283  loss_ce_0: 3.055  loss_mask_0: 1.343  loss_dice_0: 4.014  loss_ce_1: 2.767  loss_mask_1: 1.435  loss_dice_1: 4.028  loss_ce_2: 2.958  loss_mask_2: 1.5  loss_dice_2: 3.695  loss_ce_3: 2.903  loss_mask_3: 1.422  loss_dice_3: 4.012  loss_ce_4: 2.86  loss_mask_4: 1.507  loss_dice_4: 4.055  loss_ce_5: 2.898  loss_mask_5: 1.459  loss_dice_5: 4.157  loss_ce_6: 2.911  loss_mask_6: 1.464  loss_dice_6: 4.291  loss_ce_7: 2.857  loss_mask_7: 1.579  loss_dice_7: 4.302  loss_ce_8: 2.803  loss_mask_8: 1.48  loss_dice_8: 4.262  loss_mars: 0.6444    time: 5.6142  last_time: 6.1511  data_time: 0.0026  last_data_time: 0.0027   lr: 1e-05  max_mem: 0M
[10/19 18:03:06] d2.utils.events INFO:  eta: 4:41:00  iter: 12539  total_loss: 83.43  loss_ce: 2.531  loss_mask: 1.101  loss_dice: 4.287  loss_ce_0: 2.81  loss_mask_0: 1.029  loss_dice_0: 4.214  loss_ce_1: 2.498  loss_mask_1: 1.146  loss_dice_1: 4.243  loss_ce_2: 2.435  loss_mask_2: 1.058  loss_dice_2: 4.267  loss_ce_3: 2.469  loss_mask_3: 0.9582  loss_dice_3: 4.044  loss_ce_4: 2.458  loss_mask_4: 1.121  loss_dice_4: 4.166  loss_ce_5: 2.548  loss_mask_5: 1.11  loss_dice_5: 4.235  loss_ce_6: 2.532  loss_mask_6: 1.164  loss_dice_6: 4.054  loss_ce_7: 2.569  loss_mask_7: 1.298  loss_dice_7: 4.263  loss_ce_8: 2.505  loss_mask_8: 1.262  loss_dice_8: 4.367  loss_mars: 0.7697    time: 5.6138  last_time: 5.0640  data_time: 0.0032  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 18:05:03] d2.utils.events INFO:  eta: 4:38:31  iter: 12559  total_loss: 82.61  loss_ce: 2.753  loss_mask: 1.964  loss_dice: 3.799  loss_ce_0: 3.205  loss_mask_0: 1.951  loss_dice_0: 3.827  loss_ce_1: 2.933  loss_mask_1: 1.816  loss_dice_1: 4.065  loss_ce_2: 2.784  loss_mask_2: 1.709  loss_dice_2: 3.735  loss_ce_3: 2.905  loss_mask_3: 1.908  loss_dice_3: 3.909  loss_ce_4: 2.754  loss_mask_4: 1.668  loss_dice_4: 3.93  loss_ce_5: 2.801  loss_mask_5: 1.631  loss_dice_5: 3.815  loss_ce_6: 2.783  loss_mask_6: 1.674  loss_dice_6: 3.943  loss_ce_7: 2.763  loss_mask_7: 2.108  loss_dice_7: 4.078  loss_ce_8: 2.699  loss_mask_8: 1.934  loss_dice_8: 3.946  loss_mars: 0.3244    time: 5.6141  last_time: 5.9819  data_time: 0.0031  last_data_time: 0.0020   lr: 1e-05  max_mem: 0M
[10/19 18:06:48] d2.utils.events INFO:  eta: 4:37:21  iter: 12579  total_loss: 98.58  loss_ce: 4.315  loss_mask: 0.7029  loss_dice: 4.42  loss_ce_0: 5.478  loss_mask_0: 0.6287  loss_dice_0: 4.347  loss_ce_1: 4.634  loss_mask_1: 0.67  loss_dice_1: 4.396  loss_ce_2: 4.68  loss_mask_2: 0.6902  loss_dice_2: 4.384  loss_ce_3: 4.67  loss_mask_3: 0.6629  loss_dice_3: 4.254  loss_ce_4: 4.495  loss_mask_4: 0.6971  loss_dice_4: 4.293  loss_ce_5: 4.433  loss_mask_5: 0.6727  loss_dice_5: 4.255  loss_ce_6: 4.121  loss_mask_6: 0.7048  loss_dice_6: 4.4  loss_ce_7: 4.227  loss_mask_7: 0.7406  loss_dice_7: 4.462  loss_ce_8: 4.251  loss_mask_8: 0.7152  loss_dice_8: 4.361  loss_mars: 0.7931    time: 5.6135  last_time: 5.0396  data_time: 0.0033  last_data_time: 0.0095   lr: 1e-05  max_mem: 0M
[10/19 18:08:34] d2.utils.events INFO:  eta: 4:34:34  iter: 12599  total_loss: 86.79  loss_ce: 3.152  loss_mask: 1.163  loss_dice: 4.014  loss_ce_0: 3.253  loss_mask_0: 0.985  loss_dice_0: 3.989  loss_ce_1: 2.986  loss_mask_1: 0.8399  loss_dice_1: 4.061  loss_ce_2: 3.23  loss_mask_2: 0.9221  loss_dice_2: 3.998  loss_ce_3: 2.977  loss_mask_3: 1.008  loss_dice_3: 4.078  loss_ce_4: 2.974  loss_mask_4: 1.022  loss_dice_4: 3.827  loss_ce_5: 3.009  loss_mask_5: 1.1  loss_dice_5: 3.881  loss_ce_6: 3.122  loss_mask_6: 1.072  loss_dice_6: 3.84  loss_ce_7: 3.003  loss_mask_7: 1.17  loss_dice_7: 3.969  loss_ce_8: 3.076  loss_mask_8: 1.203  loss_dice_8: 4.147  loss_mars: 0.7938    time: 5.6129  last_time: 5.0225  data_time: 0.0025  last_data_time: 0.0086   lr: 1e-05  max_mem: 0M
[10/19 18:10:24] d2.utils.events INFO:  eta: 4:33:04  iter: 12619  total_loss: 91.04  loss_ce: 3.27  loss_mask: 1.207  loss_dice: 4.155  loss_ce_0: 3.231  loss_mask_0: 1.051  loss_dice_0: 4.157  loss_ce_1: 3.464  loss_mask_1: 1.102  loss_dice_1: 4.215  loss_ce_2: 3.404  loss_mask_2: 1.137  loss_dice_2: 4.163  loss_ce_3: 3.342  loss_mask_3: 1.054  loss_dice_3: 4.169  loss_ce_4: 3.241  loss_mask_4: 1.1  loss_dice_4: 4.072  loss_ce_5: 3.241  loss_mask_5: 1.1  loss_dice_5: 3.885  loss_ce_6: 3.337  loss_mask_6: 1.074  loss_dice_6: 4.096  loss_ce_7: 3.198  loss_mask_7: 1.174  loss_dice_7: 4.048  loss_ce_8: 3.203  loss_mask_8: 1.162  loss_dice_8: 4.143  loss_mars: 0.7985    time: 5.6127  last_time: 6.0065  data_time: 0.0029  last_data_time: 0.0031   lr: 1e-05  max_mem: 0M
[10/19 18:12:18] d2.utils.events INFO:  eta: 4:31:00  iter: 12639  total_loss: 87.68  loss_ce: 2.756  loss_mask: 1.288  loss_dice: 4.046  loss_ce_0: 3.378  loss_mask_0: 1.326  loss_dice_0: 4.34  loss_ce_1: 2.99  loss_mask_1: 1.228  loss_dice_1: 4.185  loss_ce_2: 2.952  loss_mask_2: 1.243  loss_dice_2: 4.101  loss_ce_3: 2.902  loss_mask_3: 1.377  loss_dice_3: 4.101  loss_ce_4: 2.941  loss_mask_4: 1.431  loss_dice_4: 4.133  loss_ce_5: 2.964  loss_mask_5: 1.335  loss_dice_5: 4.032  loss_ce_6: 2.826  loss_mask_6: 1.332  loss_dice_6: 4.238  loss_ce_7: 2.72  loss_mask_7: 1.352  loss_dice_7: 4.289  loss_ce_8: 2.732  loss_mask_8: 1.334  loss_dice_8: 3.963  loss_mars: 0.8588    time: 5.6129  last_time: 5.5486  data_time: 0.0031  last_data_time: 0.0026   lr: 1e-05  max_mem: 0M
[10/19 18:14:01] d2.utils.events INFO:  eta: 4:28:57  iter: 12659  total_loss: 86.35  loss_ce: 2.881  loss_mask: 1.503  loss_dice: 4.065  loss_ce_0: 3.337  loss_mask_0: 1.529  loss_dice_0: 4.101  loss_ce_1: 3.044  loss_mask_1: 1.542  loss_dice_1: 4.055  loss_ce_2: 2.92  loss_mask_2: 1.422  loss_dice_2: 4.097  loss_ce_3: 2.896  loss_mask_3: 1.487  loss_dice_3: 4.038  loss_ce_4: 2.81  loss_mask_4: 1.49  loss_dice_4: 3.916  loss_ce_5: 2.945  loss_mask_5: 1.481  loss_dice_5: 4.018  loss_ce_6: 2.941  loss_mask_6: 1.527  loss_dice_6: 4.018  loss_ce_7: 2.803  loss_mask_7: 1.598  loss_dice_7: 4.041  loss_ce_8: 2.826  loss_mask_8: 1.578  loss_dice_8: 4.09  loss_mars: 0.8454    time: 5.6120  last_time: 5.7341  data_time: 0.0024  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 18:15:50] d2.utils.events INFO:  eta: 4:26:51  iter: 12679  total_loss: 86.01  loss_ce: 3.074  loss_mask: 1.314  loss_dice: 3.984  loss_ce_0: 2.893  loss_mask_0: 1.345  loss_dice_0: 4.152  loss_ce_1: 3.177  loss_mask_1: 1.232  loss_dice_1: 4.18  loss_ce_2: 3.034  loss_mask_2: 1.203  loss_dice_2: 4.056  loss_ce_3: 3.043  loss_mask_3: 1.226  loss_dice_3: 3.969  loss_ce_4: 2.941  loss_mask_4: 1.245  loss_dice_4: 3.942  loss_ce_5: 3.003  loss_mask_5: 1.238  loss_dice_5: 3.972  loss_ce_6: 2.971  loss_mask_6: 1.207  loss_dice_6: 3.884  loss_ce_7: 3.02  loss_mask_7: 1.283  loss_dice_7: 4.045  loss_ce_8: 2.985  loss_mask_8: 1.318  loss_dice_8: 4.15  loss_mars: 0.8838    time: 5.6117  last_time: 5.2108  data_time: 0.0027  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 18:17:40] d2.utils.events INFO:  eta: 4:25:02  iter: 12699  total_loss: 89.76  loss_ce: 3.564  loss_mask: 1.046  loss_dice: 4.352  loss_ce_0: 4.328  loss_mask_0: 0.9494  loss_dice_0: 4.266  loss_ce_1: 3.778  loss_mask_1: 0.8973  loss_dice_1: 4.284  loss_ce_2: 3.868  loss_mask_2: 0.9986  loss_dice_2: 4.332  loss_ce_3: 3.639  loss_mask_3: 0.9812  loss_dice_3: 4.222  loss_ce_4: 3.648  loss_mask_4: 1.02  loss_dice_4: 4.197  loss_ce_5: 3.302  loss_mask_5: 0.9961  loss_dice_5: 4.189  loss_ce_6: 3.39  loss_mask_6: 1.03  loss_dice_6: 4.259  loss_ce_7: 3.539  loss_mask_7: 0.9439  loss_dice_7: 4.245  loss_ce_8: 3.561  loss_mask_8: 1.101  loss_dice_8: 4.259  loss_mars: 0.8646    time: 5.6116  last_time: 5.2976  data_time: 0.0031  last_data_time: 0.0078   lr: 1e-05  max_mem: 0M
[10/19 18:19:28] d2.utils.events INFO:  eta: 4:23:24  iter: 12719  total_loss: 82.7  loss_ce: 2.981  loss_mask: 1.073  loss_dice: 3.976  loss_ce_0: 3.168  loss_mask_0: 1.102  loss_dice_0: 3.978  loss_ce_1: 3.205  loss_mask_1: 0.9502  loss_dice_1: 4.038  loss_ce_2: 3.128  loss_mask_2: 1.075  loss_dice_2: 3.913  loss_ce_3: 3.06  loss_mask_3: 0.9681  loss_dice_3: 4.063  loss_ce_4: 2.998  loss_mask_4: 0.9631  loss_dice_4: 3.984  loss_ce_5: 2.959  loss_mask_5: 0.9398  loss_dice_5: 4.044  loss_ce_6: 2.902  loss_mask_6: 1.007  loss_dice_6: 4.06  loss_ce_7: 2.951  loss_mask_7: 1.131  loss_dice_7: 3.931  loss_ce_8: 2.95  loss_mask_8: 1.186  loss_dice_8: 3.918  loss_mars: 0.7965    time: 5.6112  last_time: 7.8049  data_time: 0.0031  last_data_time: 0.0091   lr: 1e-05  max_mem: 0M
[10/19 18:21:20] d2.utils.events INFO:  eta: 4:21:42  iter: 12739  total_loss: 85.13  loss_ce: 3.044  loss_mask: 0.9415  loss_dice: 4.094  loss_ce_0: 4.383  loss_mask_0: 0.9707  loss_dice_0: 4.044  loss_ce_1: 3.54  loss_mask_1: 0.9723  loss_dice_1: 4.07  loss_ce_2: 3.45  loss_mask_2: 0.99  loss_dice_2: 4.134  loss_ce_3: 3.423  loss_mask_3: 0.9558  loss_dice_3: 4.075  loss_ce_4: 3.37  loss_mask_4: 0.959  loss_dice_4: 4.022  loss_ce_5: 3.244  loss_mask_5: 0.9484  loss_dice_5: 3.919  loss_ce_6: 3.237  loss_mask_6: 0.8925  loss_dice_6: 4.073  loss_ce_7: 3.207  loss_mask_7: 0.9603  loss_dice_7: 4.149  loss_ce_8: 3.165  loss_mask_8: 1.057  loss_dice_8: 4.082  loss_mars: 0.8872    time: 5.6112  last_time: 5.8091  data_time: 0.0030  last_data_time: 0.0020   lr: 1e-05  max_mem: 0M
[10/19 18:23:16] d2.utils.events INFO:  eta: 4:19:53  iter: 12759  total_loss: 86.16  loss_ce: 2.835  loss_mask: 1.265  loss_dice: 4.105  loss_ce_0: 3.118  loss_mask_0: 1.203  loss_dice_0: 4.355  loss_ce_1: 3.032  loss_mask_1: 1.211  loss_dice_1: 4.314  loss_ce_2: 2.981  loss_mask_2: 1.228  loss_dice_2: 4.327  loss_ce_3: 2.971  loss_mask_3: 1.235  loss_dice_3: 4.309  loss_ce_4: 2.863  loss_mask_4: 1.328  loss_dice_4: 4.23  loss_ce_5: 2.847  loss_mask_5: 1.258  loss_dice_5: 4.218  loss_ce_6: 2.678  loss_mask_6: 1.448  loss_dice_6: 4.263  loss_ce_7: 2.867  loss_mask_7: 1.222  loss_dice_7: 4.181  loss_ce_8: 2.868  loss_mask_8: 1.274  loss_dice_8: 4.259  loss_mars: 0.8237    time: 5.6115  last_time: 5.2232  data_time: 0.0028  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 18:25:09] d2.utils.events INFO:  eta: 4:18:32  iter: 12779  total_loss: 87.55  loss_ce: 3.032  loss_mask: 1.801  loss_dice: 3.959  loss_ce_0: 3.169  loss_mask_0: 1.779  loss_dice_0: 4.096  loss_ce_1: 3.128  loss_mask_1: 1.559  loss_dice_1: 4.078  loss_ce_2: 3.257  loss_mask_2: 1.505  loss_dice_2: 4.035  loss_ce_3: 2.933  loss_mask_3: 1.48  loss_dice_3: 4.008  loss_ce_4: 2.999  loss_mask_4: 1.607  loss_dice_4: 4.056  loss_ce_5: 2.922  loss_mask_5: 1.789  loss_dice_5: 3.951  loss_ce_6: 3.069  loss_mask_6: 1.774  loss_dice_6: 3.808  loss_ce_7: 2.93  loss_mask_7: 1.603  loss_dice_7: 3.888  loss_ce_8: 2.926  loss_mask_8: 1.744  loss_dice_8: 3.891  loss_mars: 0.8795    time: 5.6115  last_time: 4.3287  data_time: 0.0032  last_data_time: 0.0023   lr: 1e-05  max_mem: 0M
[10/19 18:27:01] d2.utils.events INFO:  eta: 4:17:39  iter: 12799  total_loss: 84.16  loss_ce: 2.591  loss_mask: 1.694  loss_dice: 3.905  loss_ce_0: 3.075  loss_mask_0: 1.76  loss_dice_0: 3.782  loss_ce_1: 2.892  loss_mask_1: 1.573  loss_dice_1: 3.84  loss_ce_2: 2.703  loss_mask_2: 1.491  loss_dice_2: 3.879  loss_ce_3: 2.642  loss_mask_3: 1.552  loss_dice_3: 3.809  loss_ce_4: 2.677  loss_mask_4: 1.618  loss_dice_4: 3.921  loss_ce_5: 2.701  loss_mask_5: 1.79  loss_dice_5: 3.815  loss_ce_6: 2.642  loss_mask_6: 1.731  loss_dice_6: 3.947  loss_ce_7: 2.508  loss_mask_7: 1.637  loss_dice_7: 3.833  loss_ce_8: 2.436  loss_mask_8: 1.815  loss_dice_8: 3.902  loss_mars: 0.8675    time: 5.6115  last_time: 5.8930  data_time: 0.0038  last_data_time: 0.0079   lr: 1e-05  max_mem: 0M
[10/19 18:28:55] d2.utils.events INFO:  eta: 4:15:21  iter: 12819  total_loss: 83.68  loss_ce: 2.763  loss_mask: 1.513  loss_dice: 4.054  loss_ce_0: 3.246  loss_mask_0: 1.58  loss_dice_0: 3.965  loss_ce_1: 2.993  loss_mask_1: 1.511  loss_dice_1: 4.098  loss_ce_2: 3.037  loss_mask_2: 1.517  loss_dice_2: 4.002  loss_ce_3: 2.921  loss_mask_3: 1.521  loss_dice_3: 3.917  loss_ce_4: 2.996  loss_mask_4: 1.587  loss_dice_4: 3.903  loss_ce_5: 2.82  loss_mask_5: 1.525  loss_dice_5: 4.054  loss_ce_6: 2.776  loss_mask_6: 1.3  loss_dice_6: 4.035  loss_ce_7: 2.817  loss_mask_7: 1.417  loss_dice_7: 4.218  loss_ce_8: 2.779  loss_mask_8: 1.425  loss_dice_8: 4.176  loss_mars: 0.8764    time: 5.6117  last_time: 5.7973  data_time: 0.0037  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 18:30:44] d2.utils.events INFO:  eta: 4:12:31  iter: 12839  total_loss: 81.42  loss_ce: 2.252  loss_mask: 1.599  loss_dice: 4.249  loss_ce_0: 2.357  loss_mask_0: 1.555  loss_dice_0: 3.702  loss_ce_1: 2.52  loss_mask_1: 1.373  loss_dice_1: 3.655  loss_ce_2: 2.362  loss_mask_2: 1.529  loss_dice_2: 4.02  loss_ce_3: 2.445  loss_mask_3: 1.52  loss_dice_3: 4.132  loss_ce_4: 2.424  loss_mask_4: 1.477  loss_dice_4: 4.056  loss_ce_5: 2.363  loss_mask_5: 1.404  loss_dice_5: 3.808  loss_ce_6: 2.353  loss_mask_6: 1.465  loss_dice_6: 4.128  loss_ce_7: 2.134  loss_mask_7: 1.502  loss_dice_7: 3.912  loss_ce_8: 2.151  loss_mask_8: 1.645  loss_dice_8: 4.163  loss_mars: 0.8576    time: 5.6114  last_time: 5.8359  data_time: 0.0023  last_data_time: 0.0023   lr: 1e-05  max_mem: 0M
[10/19 18:32:33] d2.utils.events INFO:  eta: 4:10:52  iter: 12859  total_loss: 93.43  loss_ce: 3.572  loss_mask: 1.423  loss_dice: 4.342  loss_ce_0: 3.6  loss_mask_0: 1.298  loss_dice_0: 4.277  loss_ce_1: 3.712  loss_mask_1: 1.198  loss_dice_1: 4.204  loss_ce_2: 3.839  loss_mask_2: 1.305  loss_dice_2: 4.18  loss_ce_3: 3.649  loss_mask_3: 1.379  loss_dice_3: 4.219  loss_ce_4: 3.642  loss_mask_4: 1.375  loss_dice_4: 4.124  loss_ce_5: 3.526  loss_mask_5: 1.271  loss_dice_5: 4.174  loss_ce_6: 3.428  loss_mask_6: 1.327  loss_dice_6: 4.207  loss_ce_7: 3.425  loss_mask_7: 1.366  loss_dice_7: 4.251  loss_ce_8: 3.511  loss_mask_8: 1.349  loss_dice_8: 4.262  loss_mars: 0.8853    time: 5.6111  last_time: 8.1499  data_time: 0.0023  last_data_time: 0.0033   lr: 1e-05  max_mem: 0M
[10/19 18:34:26] d2.utils.events INFO:  eta: 4:09:09  iter: 12879  total_loss: 82.81  loss_ce: 2.772  loss_mask: 1.043  loss_dice: 4.348  loss_ce_0: 3.393  loss_mask_0: 1.112  loss_dice_0: 4.239  loss_ce_1: 3.118  loss_mask_1: 1.037  loss_dice_1: 4.395  loss_ce_2: 2.995  loss_mask_2: 1.043  loss_dice_2: 4.282  loss_ce_3: 2.887  loss_mask_3: 1.099  loss_dice_3: 4.164  loss_ce_4: 2.816  loss_mask_4: 1.072  loss_dice_4: 4.272  loss_ce_5: 2.829  loss_mask_5: 1.059  loss_dice_5: 4.313  loss_ce_6: 2.91  loss_mask_6: 1.145  loss_dice_6: 4.374  loss_ce_7: 2.828  loss_mask_7: 1.105  loss_dice_7: 4.305  loss_ce_8: 2.909  loss_mask_8: 1.034  loss_dice_8: 4.37  loss_mars: 0.8672    time: 5.6112  last_time: 5.8432  data_time: 0.0025  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 18:36:21] d2.utils.events INFO:  eta: 4:07:24  iter: 12899  total_loss: 89.15  loss_ce: 3.175  loss_mask: 1.166  loss_dice: 4.114  loss_ce_0: 3.611  loss_mask_0: 1.171  loss_dice_0: 4.1  loss_ce_1: 3.62  loss_mask_1: 1.164  loss_dice_1: 3.933  loss_ce_2: 3.536  loss_mask_2: 1.12  loss_dice_2: 4.071  loss_ce_3: 3.397  loss_mask_3: 1.184  loss_dice_3: 3.952  loss_ce_4: 3.349  loss_mask_4: 1.202  loss_dice_4: 3.907  loss_ce_5: 3.387  loss_mask_5: 1.211  loss_dice_5: 3.989  loss_ce_6: 3.188  loss_mask_6: 1.208  loss_dice_6: 3.91  loss_ce_7: 3.165  loss_mask_7: 1.256  loss_dice_7: 3.951  loss_ce_8: 3.171  loss_mask_8: 1.093  loss_dice_8: 3.957  loss_mars: 0.8743    time: 5.6114  last_time: 5.8049  data_time: 0.0023  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 18:38:04] d2.utils.events INFO:  eta: 4:05:16  iter: 12919  total_loss: 85.58  loss_ce: 2.877  loss_mask: 1.311  loss_dice: 3.811  loss_ce_0: 2.932  loss_mask_0: 1.319  loss_dice_0: 3.813  loss_ce_1: 3.014  loss_mask_1: 1.336  loss_dice_1: 3.793  loss_ce_2: 3.087  loss_mask_2: 1.279  loss_dice_2: 3.749  loss_ce_3: 2.929  loss_mask_3: 1.267  loss_dice_3: 3.702  loss_ce_4: 2.851  loss_mask_4: 1.326  loss_dice_4: 3.69  loss_ce_5: 2.853  loss_mask_5: 1.278  loss_dice_5: 3.676  loss_ce_6: 2.847  loss_mask_6: 1.347  loss_dice_6: 3.75  loss_ce_7: 2.854  loss_mask_7: 1.421  loss_dice_7: 3.735  loss_ce_8: 2.799  loss_mask_8: 1.388  loss_dice_8: 3.807  loss_mars: 0.8144    time: 5.6106  last_time: 5.1946  data_time: 0.0025  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 18:39:58] d2.utils.events INFO:  eta: 4:03:46  iter: 12939  total_loss: 91.19  loss_ce: 3.79  loss_mask: 0.6623  loss_dice: 4.499  loss_ce_0: 4.623  loss_mask_0: 0.6587  loss_dice_0: 4.329  loss_ce_1: 3.759  loss_mask_1: 0.6179  loss_dice_1: 4.358  loss_ce_2: 4.03  loss_mask_2: 0.6454  loss_dice_2: 4.341  loss_ce_3: 3.902  loss_mask_3: 0.6691  loss_dice_3: 4.369  loss_ce_4: 3.805  loss_mask_4: 0.6854  loss_dice_4: 4.42  loss_ce_5: 3.882  loss_mask_5: 0.7051  loss_dice_5: 4.46  loss_ce_6: 3.754  loss_mask_6: 0.6735  loss_dice_6: 4.435  loss_ce_7: 3.832  loss_mask_7: 0.7328  loss_dice_7: 4.433  loss_ce_8: 3.773  loss_mask_8: 0.6393  loss_dice_8: 4.499  loss_mars: 0.8327    time: 5.6107  last_time: 5.7047  data_time: 0.0032  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 18:41:48] d2.utils.events INFO:  eta: 4:01:57  iter: 12959  total_loss: 86.49  loss_ce: 2.964  loss_mask: 1.571  loss_dice: 3.591  loss_ce_0: 3.113  loss_mask_0: 1.569  loss_dice_0: 3.69  loss_ce_1: 3.04  loss_mask_1: 1.674  loss_dice_1: 3.751  loss_ce_2: 3.089  loss_mask_2: 1.64  loss_dice_2: 3.794  loss_ce_3: 3.152  loss_mask_3: 1.506  loss_dice_3: 3.635  loss_ce_4: 3.134  loss_mask_4: 1.505  loss_dice_4: 3.709  loss_ce_5: 3.009  loss_mask_5: 1.478  loss_dice_5: 3.579  loss_ce_6: 2.955  loss_mask_6: 1.656  loss_dice_6: 3.542  loss_ce_7: 2.883  loss_mask_7: 1.541  loss_dice_7: 3.542  loss_ce_8: 2.9  loss_mask_8: 1.709  loss_dice_8: 3.715  loss_mars: 0.8969    time: 5.6106  last_time: 5.2788  data_time: 0.0029  last_data_time: 0.0080   lr: 1e-05  max_mem: 0M
[10/19 18:43:46] d2.utils.events INFO:  eta: 3:59:59  iter: 12979  total_loss: 89.64  loss_ce: 3.134  loss_mask: 1.084  loss_dice: 4.305  loss_ce_0: 3.369  loss_mask_0: 0.9917  loss_dice_0: 4.137  loss_ce_1: 3.137  loss_mask_1: 0.9754  loss_dice_1: 4.163  loss_ce_2: 3.227  loss_mask_2: 0.9686  loss_dice_2: 4.239  loss_ce_3: 3.106  loss_mask_3: 1.088  loss_dice_3: 4.18  loss_ce_4: 3.115  loss_mask_4: 1.113  loss_dice_4: 4.16  loss_ce_5: 3.123  loss_mask_5: 0.9813  loss_dice_5: 4.261  loss_ce_6: 3.17  loss_mask_6: 0.943  loss_dice_6: 4.195  loss_ce_7: 3.118  loss_mask_7: 1.032  loss_dice_7: 4.168  loss_ce_8: 3.151  loss_mask_8: 0.9589  loss_dice_8: 4.136  loss_mars: 0.8818    time: 5.6110  last_time: 5.9158  data_time: 0.0023  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 18:45:37] d2.utils.events INFO:  eta: 3:58:10  iter: 12999  total_loss: 83.43  loss_ce: 2.795  loss_mask: 1.411  loss_dice: 3.828  loss_ce_0: 2.909  loss_mask_0: 1.556  loss_dice_0: 3.977  loss_ce_1: 3.094  loss_mask_1: 1.231  loss_dice_1: 3.891  loss_ce_2: 2.993  loss_mask_2: 1.261  loss_dice_2: 3.824  loss_ce_3: 2.822  loss_mask_3: 1.484  loss_dice_3: 3.816  loss_ce_4: 2.792  loss_mask_4: 1.394  loss_dice_4: 3.663  loss_ce_5: 2.814  loss_mask_5: 1.38  loss_dice_5: 3.769  loss_ce_6: 2.849  loss_mask_6: 1.456  loss_dice_6: 3.796  loss_ce_7: 2.759  loss_mask_7: 1.353  loss_dice_7: 3.803  loss_ce_8: 2.778  loss_mask_8: 1.444  loss_dice_8: 3.819  loss_mars: 0.8073    time: 5.6109  last_time: 5.8316  data_time: 0.0027  last_data_time: 0.0081   lr: 1e-05  max_mem: 0M
[10/19 18:47:24] d2.utils.events INFO:  eta: 3:56:18  iter: 13019  total_loss: 95.31  loss_ce: 3.601  loss_mask: 0.993  loss_dice: 4.214  loss_ce_0: 3.957  loss_mask_0: 0.8685  loss_dice_0: 4.239  loss_ce_1: 3.902  loss_mask_1: 0.8692  loss_dice_1: 4.227  loss_ce_2: 3.818  loss_mask_2: 0.8858  loss_dice_2: 4.079  loss_ce_3: 3.741  loss_mask_3: 0.9498  loss_dice_3: 4.107  loss_ce_4: 3.723  loss_mask_4: 1.001  loss_dice_4: 4.036  loss_ce_5: 3.642  loss_mask_5: 0.9359  loss_dice_5: 4.115  loss_ce_6: 3.579  loss_mask_6: 0.9455  loss_dice_6: 4.144  loss_ce_7: 3.656  loss_mask_7: 0.9649  loss_dice_7: 4.068  loss_ce_8: 3.62  loss_mask_8: 0.9714  loss_dice_8: 4.175  loss_mars: 0.8842    time: 5.6105  last_time: 5.9388  data_time: 0.0026  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 18:49:17] d2.utils.events INFO:  eta: 3:54:23  iter: 13039  total_loss: 92.41  loss_ce: 3.08  loss_mask: 1.021  loss_dice: 4.121  loss_ce_0: 3.413  loss_mask_0: 1.061  loss_dice_0: 4.123  loss_ce_1: 3.256  loss_mask_1: 0.9968  loss_dice_1: 4.146  loss_ce_2: 3.203  loss_mask_2: 1.062  loss_dice_2: 4.079  loss_ce_3: 3.164  loss_mask_3: 1.07  loss_dice_3: 4.187  loss_ce_4: 3.152  loss_mask_4: 1.072  loss_dice_4: 4.042  loss_ce_5: 3.05  loss_mask_5: 1.006  loss_dice_5: 4.145  loss_ce_6: 3.052  loss_mask_6: 1.008  loss_dice_6: 4.068  loss_ce_7: 2.941  loss_mask_7: 1.085  loss_dice_7: 4.112  loss_ce_8: 2.973  loss_mask_8: 1.048  loss_dice_8: 4.064  loss_mars: 0.8573    time: 5.6106  last_time: 5.1967  data_time: 0.0028  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 18:51:08] d2.utils.events INFO:  eta: 3:52:34  iter: 13059  total_loss: 93.12  loss_ce: 3.462  loss_mask: 1.059  loss_dice: 4.269  loss_ce_0: 3.874  loss_mask_0: 1.068  loss_dice_0: 4.345  loss_ce_1: 3.701  loss_mask_1: 0.9419  loss_dice_1: 4.239  loss_ce_2: 3.667  loss_mask_2: 1.051  loss_dice_2: 4.213  loss_ce_3: 3.606  loss_mask_3: 1.105  loss_dice_3: 4.221  loss_ce_4: 3.761  loss_mask_4: 1.091  loss_dice_4: 4.095  loss_ce_5: 3.565  loss_mask_5: 1.003  loss_dice_5: 4.182  loss_ce_6: 3.577  loss_mask_6: 1.077  loss_dice_6: 4.297  loss_ce_7: 3.561  loss_mask_7: 1.068  loss_dice_7: 4.199  loss_ce_8: 3.532  loss_mask_8: 1.055  loss_dice_8: 4.239  loss_mars: 0.8597    time: 5.6105  last_time: 5.1376  data_time: 0.0025  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 18:52:57] d2.utils.events INFO:  eta: 3:50:24  iter: 13079  total_loss: 90.65  loss_ce: 3.308  loss_mask: 1.015  loss_dice: 4.085  loss_ce_0: 3.789  loss_mask_0: 1.027  loss_dice_0: 4.168  loss_ce_1: 3.562  loss_mask_1: 0.9466  loss_dice_1: 4.188  loss_ce_2: 3.452  loss_mask_2: 0.9612  loss_dice_2: 4.001  loss_ce_3: 3.476  loss_mask_3: 0.9564  loss_dice_3: 4.059  loss_ce_4: 3.591  loss_mask_4: 1.104  loss_dice_4: 4.057  loss_ce_5: 3.433  loss_mask_5: 1.031  loss_dice_5: 4.145  loss_ce_6: 3.439  loss_mask_6: 0.9941  loss_dice_6: 4.027  loss_ce_7: 3.375  loss_mask_7: 0.9444  loss_dice_7: 4.052  loss_ce_8: 3.306  loss_mask_8: 0.9321  loss_dice_8: 4.11  loss_mars: 0.8832    time: 5.6102  last_time: 5.0864  data_time: 0.0029  last_data_time: 0.0083   lr: 1e-05  max_mem: 0M
[10/19 18:54:48] d2.utils.events INFO:  eta: 3:48:49  iter: 13099  total_loss: 93.29  loss_ce: 3.872  loss_mask: 0.8281  loss_dice: 4.191  loss_ce_0: 3.967  loss_mask_0: 0.878  loss_dice_0: 4.169  loss_ce_1: 3.861  loss_mask_1: 0.8096  loss_dice_1: 4.332  loss_ce_2: 3.822  loss_mask_2: 0.8428  loss_dice_2: 4.331  loss_ce_3: 3.826  loss_mask_3: 0.8493  loss_dice_3: 4.166  loss_ce_4: 3.773  loss_mask_4: 0.8563  loss_dice_4: 4.207  loss_ce_5: 3.825  loss_mask_5: 0.8191  loss_dice_5: 4.107  loss_ce_6: 3.795  loss_mask_6: 0.8346  loss_dice_6: 4.219  loss_ce_7: 3.672  loss_mask_7: 0.8919  loss_dice_7: 4.238  loss_ce_8: 3.707  loss_mask_8: 0.8223  loss_dice_8: 4.195  loss_mars: 0.7914    time: 5.6101  last_time: 5.7334  data_time: 0.0032  last_data_time: 0.0020   lr: 1e-05  max_mem: 0M
[10/19 18:56:33] d2.utils.events INFO:  eta: 3:46:47  iter: 13119  total_loss: 81.98  loss_ce: 2.629  loss_mask: 1.391  loss_dice: 3.901  loss_ce_0: 2.822  loss_mask_0: 1.372  loss_dice_0: 3.793  loss_ce_1: 2.611  loss_mask_1: 1.323  loss_dice_1: 4.01  loss_ce_2: 2.677  loss_mask_2: 1.222  loss_dice_2: 3.932  loss_ce_3: 2.725  loss_mask_3: 1.318  loss_dice_3: 3.884  loss_ce_4: 2.667  loss_mask_4: 1.28  loss_dice_4: 3.933  loss_ce_5: 2.637  loss_mask_5: 1.297  loss_dice_5: 3.947  loss_ce_6: 2.669  loss_mask_6: 1.346  loss_dice_6: 3.998  loss_ce_7: 2.67  loss_mask_7: 1.361  loss_dice_7: 4.049  loss_ce_8: 2.544  loss_mask_8: 1.409  loss_dice_8: 3.89  loss_mars: 0.8813    time: 5.6095  last_time: 5.9939  data_time: 0.0023  last_data_time: 0.0032   lr: 1e-05  max_mem: 0M
[10/19 18:58:21] d2.utils.events INFO:  eta: 3:44:56  iter: 13139  total_loss: 85.31  loss_ce: 3.402  loss_mask: 0.7979  loss_dice: 4.165  loss_ce_0: 3.627  loss_mask_0: 0.7756  loss_dice_0: 4.229  loss_ce_1: 3.558  loss_mask_1: 0.7544  loss_dice_1: 4.213  loss_ce_2: 3.539  loss_mask_2: 0.7521  loss_dice_2: 4.182  loss_ce_3: 3.459  loss_mask_3: 0.7883  loss_dice_3: 4.18  loss_ce_4: 3.416  loss_mask_4: 0.7838  loss_dice_4: 4.087  loss_ce_5: 3.51  loss_mask_5: 0.7597  loss_dice_5: 4.14  loss_ce_6: 3.422  loss_mask_6: 0.8632  loss_dice_6: 4.171  loss_ce_7: 3.317  loss_mask_7: 0.7712  loss_dice_7: 4.263  loss_ce_8: 3.265  loss_mask_8: 0.7946  loss_dice_8: 4.175  loss_mars: 0.8484    time: 5.6090  last_time: 5.1155  data_time: 0.0032  last_data_time: 0.0032   lr: 1e-05  max_mem: 0M
[10/19 19:00:11] d2.utils.events INFO:  eta: 3:42:35  iter: 13159  total_loss: 98.72  loss_ce: 3.971  loss_mask: 1.09  loss_dice: 4.245  loss_ce_0: 5.102  loss_mask_0: 1.107  loss_dice_0: 4.271  loss_ce_1: 4.242  loss_mask_1: 1.056  loss_dice_1: 4.217  loss_ce_2: 4.29  loss_mask_2: 1.02  loss_dice_2: 4.369  loss_ce_3: 3.971  loss_mask_3: 1.011  loss_dice_3: 4.236  loss_ce_4: 4.048  loss_mask_4: 1.049  loss_dice_4: 4.273  loss_ce_5: 4.131  loss_mask_5: 0.9933  loss_dice_5: 4.178  loss_ce_6: 4.131  loss_mask_6: 1.039  loss_dice_6: 4.098  loss_ce_7: 4.122  loss_mask_7: 1.058  loss_dice_7: 4.302  loss_ce_8: 4.083  loss_mask_8: 1.14  loss_dice_8: 4.213  loss_mars: 0.828    time: 5.6089  last_time: 5.3134  data_time: 0.0033  last_data_time: 0.0093   lr: 1e-05  max_mem: 0M
[10/19 19:02:02] d2.utils.events INFO:  eta: 3:40:26  iter: 13179  total_loss: 95.96  loss_ce: 3.303  loss_mask: 1.05  loss_dice: 3.952  loss_ce_0: 3.45  loss_mask_0: 0.9539  loss_dice_0: 4.114  loss_ce_1: 3.382  loss_mask_1: 1.059  loss_dice_1: 4.082  loss_ce_2: 3.369  loss_mask_2: 1.036  loss_dice_2: 4.077  loss_ce_3: 3.405  loss_mask_3: 1.064  loss_dice_3: 3.971  loss_ce_4: 3.412  loss_mask_4: 1.253  loss_dice_4: 4.072  loss_ce_5: 3.339  loss_mask_5: 1.183  loss_dice_5: 4.114  loss_ce_6: 3.393  loss_mask_6: 1.075  loss_dice_6: 4.073  loss_ce_7: 3.211  loss_mask_7: 1.019  loss_dice_7: 4.013  loss_ce_8: 3.208  loss_mask_8: 1.106  loss_dice_8: 4.033  loss_mars: 0.8686    time: 5.6088  last_time: 7.4488  data_time: 0.0023  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 19:03:50] d2.utils.events INFO:  eta: 3:38:23  iter: 13199  total_loss: 79.88  loss_ce: 2.405  loss_mask: 1.93  loss_dice: 3.331  loss_ce_0: 2.412  loss_mask_0: 1.543  loss_dice_0: 3.526  loss_ce_1: 2.235  loss_mask_1: 1.883  loss_dice_1: 3.449  loss_ce_2: 2.353  loss_mask_2: 1.891  loss_dice_2: 3.422  loss_ce_3: 2.398  loss_mask_3: 1.867  loss_dice_3: 3.437  loss_ce_4: 2.405  loss_mask_4: 1.962  loss_dice_4: 3.348  loss_ce_5: 2.346  loss_mask_5: 1.806  loss_dice_5: 3.468  loss_ce_6: 2.356  loss_mask_6: 1.917  loss_dice_6: 3.261  loss_ce_7: 2.339  loss_mask_7: 1.771  loss_dice_7: 3.449  loss_ce_8: 2.338  loss_mask_8: 1.621  loss_dice_8: 3.486  loss_mars: 0.8762    time: 5.6084  last_time: 6.4446  data_time: 0.0020  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 19:05:41] d2.utils.events INFO:  eta: 3:37:03  iter: 13219  total_loss: 81.8  loss_ce: 2.5  loss_mask: 1.377  loss_dice: 4.096  loss_ce_0: 2.478  loss_mask_0: 1.246  loss_dice_0: 4.141  loss_ce_1: 2.369  loss_mask_1: 1.253  loss_dice_1: 4.089  loss_ce_2: 2.517  loss_mask_2: 1.333  loss_dice_2: 4.159  loss_ce_3: 2.581  loss_mask_3: 1.417  loss_dice_3: 3.95  loss_ce_4: 2.591  loss_mask_4: 1.463  loss_dice_4: 4.084  loss_ce_5: 2.455  loss_mask_5: 1.506  loss_dice_5: 4.211  loss_ce_6: 2.487  loss_mask_6: 1.401  loss_dice_6: 4.014  loss_ce_7: 2.492  loss_mask_7: 1.492  loss_dice_7: 3.888  loss_ce_8: 2.391  loss_mask_8: 1.306  loss_dice_8: 4.024  loss_mars: 0.7809    time: 5.6083  last_time: 5.9224  data_time: 0.0025  last_data_time: 0.0045   lr: 1e-05  max_mem: 0M
[10/19 19:07:32] d2.utils.events INFO:  eta: 3:35:21  iter: 13239  total_loss: 94.06  loss_ce: 3.466  loss_mask: 1.329  loss_dice: 4.045  loss_ce_0: 4.077  loss_mask_0: 1.232  loss_dice_0: 4.08  loss_ce_1: 3.508  loss_mask_1: 1.187  loss_dice_1: 4.197  loss_ce_2: 3.497  loss_mask_2: 1.185  loss_dice_2: 4.097  loss_ce_3: 3.424  loss_mask_3: 1.226  loss_dice_3: 4.026  loss_ce_4: 3.597  loss_mask_4: 1.246  loss_dice_4: 3.863  loss_ce_5: 3.576  loss_mask_5: 1.278  loss_dice_5: 3.917  loss_ce_6: 3.477  loss_mask_6: 1.194  loss_dice_6: 3.984  loss_ce_7: 3.415  loss_mask_7: 1.319  loss_dice_7: 3.92  loss_ce_8: 3.352  loss_mask_8: 1.221  loss_dice_8: 4.078  loss_mars: 0.8512    time: 5.6082  last_time: 5.7651  data_time: 0.0030  last_data_time: 0.0080   lr: 1e-05  max_mem: 0M
[10/19 19:09:25] d2.utils.events INFO:  eta: 3:33:33  iter: 13259  total_loss: 89.05  loss_ce: 3.377  loss_mask: 1.109  loss_dice: 4.24  loss_ce_0: 3.609  loss_mask_0: 1.069  loss_dice_0: 4.295  loss_ce_1: 3.576  loss_mask_1: 1.062  loss_dice_1: 4.305  loss_ce_2: 3.546  loss_mask_2: 1.021  loss_dice_2: 4.055  loss_ce_3: 3.496  loss_mask_3: 1.177  loss_dice_3: 4.19  loss_ce_4: 3.425  loss_mask_4: 1.171  loss_dice_4: 4.266  loss_ce_5: 3.405  loss_mask_5: 1.105  loss_dice_5: 4.237  loss_ce_6: 3.518  loss_mask_6: 1.19  loss_dice_6: 4.319  loss_ce_7: 3.433  loss_mask_7: 1.073  loss_dice_7: 4.294  loss_ce_8: 3.408  loss_mask_8: 1.104  loss_dice_8: 4.314  loss_mars: 0.9202    time: 5.6083  last_time: 5.8061  data_time: 0.0035  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 19:11:20] d2.utils.events INFO:  eta: 3:32:15  iter: 13279  total_loss: 80.56  loss_ce: 2.596  loss_mask: 1.123  loss_dice: 4.041  loss_ce_0: 2.729  loss_mask_0: 1.114  loss_dice_0: 4.08  loss_ce_1: 2.518  loss_mask_1: 1.205  loss_dice_1: 4.155  loss_ce_2: 2.568  loss_mask_2: 1.069  loss_dice_2: 4.11  loss_ce_3: 2.607  loss_mask_3: 1.075  loss_dice_3: 4.061  loss_ce_4: 2.621  loss_mask_4: 1.27  loss_dice_4: 3.979  loss_ce_5: 2.555  loss_mask_5: 1.195  loss_dice_5: 4.29  loss_ce_6: 2.617  loss_mask_6: 1.33  loss_dice_6: 3.98  loss_ce_7: 2.571  loss_mask_7: 1.03  loss_dice_7: 4.052  loss_ce_8: 2.523  loss_mask_8: 1.033  loss_dice_8: 4.254  loss_mars: 0.8782    time: 5.6086  last_time: 5.2665  data_time: 0.0021  last_data_time: 0.0029   lr: 1e-05  max_mem: 0M
[10/19 19:13:14] d2.utils.events INFO:  eta: 3:29:56  iter: 13299  total_loss: 84.32  loss_ce: 2.724  loss_mask: 1.687  loss_dice: 3.875  loss_ce_0: 3.074  loss_mask_0: 1.645  loss_dice_0: 4.054  loss_ce_1: 2.726  loss_mask_1: 1.567  loss_dice_1: 4.006  loss_ce_2: 2.824  loss_mask_2: 1.521  loss_dice_2: 3.957  loss_ce_3: 2.58  loss_mask_3: 1.378  loss_dice_3: 3.908  loss_ce_4: 2.708  loss_mask_4: 1.571  loss_dice_4: 3.973  loss_ce_5: 2.687  loss_mask_5: 1.768  loss_dice_5: 3.916  loss_ce_6: 2.631  loss_mask_6: 1.509  loss_dice_6: 3.843  loss_ce_7: 2.725  loss_mask_7: 1.67  loss_dice_7: 3.818  loss_ce_8: 2.726  loss_mask_8: 1.519  loss_dice_8: 3.852  loss_mars: 0.7952    time: 5.6087  last_time: 5.8358  data_time: 0.0029  last_data_time: 0.0016   lr: 1e-05  max_mem: 0M
[10/19 19:15:07] d2.utils.events INFO:  eta: 3:28:47  iter: 13319  total_loss: 90.11  loss_ce: 3.378  loss_mask: 1.383  loss_dice: 3.884  loss_ce_0: 3.366  loss_mask_0: 1.398  loss_dice_0: 4.014  loss_ce_1: 3.223  loss_mask_1: 1.36  loss_dice_1: 3.991  loss_ce_2: 3.283  loss_mask_2: 1.571  loss_dice_2: 3.982  loss_ce_3: 3.236  loss_mask_3: 1.374  loss_dice_3: 3.974  loss_ce_4: 3.23  loss_mask_4: 1.595  loss_dice_4: 3.955  loss_ce_5: 3.236  loss_mask_5: 1.529  loss_dice_5: 3.919  loss_ce_6: 3.277  loss_mask_6: 1.391  loss_dice_6: 3.838  loss_ce_7: 3.25  loss_mask_7: 1.421  loss_dice_7: 3.887  loss_ce_8: 3.228  loss_mask_8: 1.618  loss_dice_8: 3.879  loss_mars: 0.8172    time: 5.6088  last_time: 5.1049  data_time: 0.0035  last_data_time: 0.0022   lr: 1e-05  max_mem: 0M
[10/19 19:16:55] d2.utils.events INFO:  eta: 3:25:48  iter: 13339  total_loss: 87.68  loss_ce: 2.946  loss_mask: 1.361  loss_dice: 3.983  loss_ce_0: 2.87  loss_mask_0: 1.257  loss_dice_0: 4.125  loss_ce_1: 2.912  loss_mask_1: 1.403  loss_dice_1: 4.079  loss_ce_2: 2.936  loss_mask_2: 1.281  loss_dice_2: 4.108  loss_ce_3: 2.764  loss_mask_3: 1.508  loss_dice_3: 3.987  loss_ce_4: 2.75  loss_mask_4: 1.572  loss_dice_4: 3.882  loss_ce_5: 2.909  loss_mask_5: 1.353  loss_dice_5: 4.077  loss_ce_6: 2.855  loss_mask_6: 1.418  loss_dice_6: 4.017  loss_ce_7: 2.809  loss_mask_7: 1.388  loss_dice_7: 4.129  loss_ce_8: 2.865  loss_mask_8: 1.459  loss_dice_8: 3.947  loss_mars: 0.8546    time: 5.6084  last_time: 5.8264  data_time: 0.0034  last_data_time: 0.0072   lr: 1e-05  max_mem: 0M
[10/19 19:18:46] d2.utils.events INFO:  eta: 3:24:48  iter: 13359  total_loss: 91.17  loss_ce: 3.907  loss_mask: 1.179  loss_dice: 4.091  loss_ce_0: 3.85  loss_mask_0: 1.285  loss_dice_0: 4.01  loss_ce_1: 3.967  loss_mask_1: 1.121  loss_dice_1: 4.135  loss_ce_2: 3.777  loss_mask_2: 1.143  loss_dice_2: 4.184  loss_ce_3: 3.747  loss_mask_3: 1.13  loss_dice_3: 4.074  loss_ce_4: 3.861  loss_mask_4: 1.066  loss_dice_4: 3.958  loss_ce_5: 3.76  loss_mask_5: 1.187  loss_dice_5: 4.086  loss_ce_6: 3.804  loss_mask_6: 1.015  loss_dice_6: 4.046  loss_ce_7: 3.666  loss_mask_7: 1.165  loss_dice_7: 4.109  loss_ce_8: 3.86  loss_mask_8: 1.165  loss_dice_8: 4.02  loss_mars: 0.857    time: 5.6083  last_time: 5.3489  data_time: 0.0025  last_data_time: 0.0038   lr: 1e-05  max_mem: 0M
[10/19 19:20:34] d2.utils.events INFO:  eta: 3:23:00  iter: 13379  total_loss: 86.94  loss_ce: 2.773  loss_mask: 1.585  loss_dice: 4.065  loss_ce_0: 2.616  loss_mask_0: 1.82  loss_dice_0: 4.049  loss_ce_1: 2.78  loss_mask_1: 1.421  loss_dice_1: 3.874  loss_ce_2: 2.836  loss_mask_2: 1.476  loss_dice_2: 3.979  loss_ce_3: 2.695  loss_mask_3: 1.689  loss_dice_3: 4.217  loss_ce_4: 2.676  loss_mask_4: 1.602  loss_dice_4: 4.134  loss_ce_5: 2.722  loss_mask_5: 1.55  loss_dice_5: 3.912  loss_ce_6: 2.765  loss_mask_6: 1.536  loss_dice_6: 4.11  loss_ce_7: 2.697  loss_mask_7: 1.589  loss_dice_7: 3.809  loss_ce_8: 2.696  loss_mask_8: 1.615  loss_dice_8: 3.931  loss_mars: 0.844    time: 5.6079  last_time: 6.4271  data_time: 0.0029  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 19:22:23] d2.utils.events INFO:  eta: 3:20:35  iter: 13399  total_loss: 88.9  loss_ce: 3.464  loss_mask: 0.9585  loss_dice: 4.295  loss_ce_0: 3.604  loss_mask_0: 1.097  loss_dice_0: 4.089  loss_ce_1: 3.55  loss_mask_1: 1.042  loss_dice_1: 3.866  loss_ce_2: 3.427  loss_mask_2: 1  loss_dice_2: 4.198  loss_ce_3: 3.519  loss_mask_3: 1.018  loss_dice_3: 4.011  loss_ce_4: 3.742  loss_mask_4: 1.016  loss_dice_4: 4.138  loss_ce_5: 3.624  loss_mask_5: 0.9469  loss_dice_5: 4.195  loss_ce_6: 3.581  loss_mask_6: 0.9344  loss_dice_6: 4.209  loss_ce_7: 3.429  loss_mask_7: 1.061  loss_dice_7: 4.168  loss_ce_8: 3.556  loss_mask_8: 1.063  loss_dice_8: 4.16  loss_mars: 0.8481    time: 5.6077  last_time: 6.0585  data_time: 0.0037  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 19:24:14] d2.utils.events INFO:  eta: 3:19:00  iter: 13419  total_loss: 95.32  loss_ce: 4.201  loss_mask: 0.8268  loss_dice: 4.346  loss_ce_0: 3.967  loss_mask_0: 0.8099  loss_dice_0: 4.396  loss_ce_1: 4.086  loss_mask_1: 0.7748  loss_dice_1: 4.32  loss_ce_2: 4.244  loss_mask_2: 0.8074  loss_dice_2: 4.361  loss_ce_3: 4.237  loss_mask_3: 0.8614  loss_dice_3: 4.3  loss_ce_4: 4.25  loss_mask_4: 1.046  loss_dice_4: 4.303  loss_ce_5: 4.092  loss_mask_5: 0.8274  loss_dice_5: 4.411  loss_ce_6: 4.138  loss_mask_6: 0.8309  loss_dice_6: 4.345  loss_ce_7: 4.04  loss_mask_7: 0.84  loss_dice_7: 4.396  loss_ce_8: 4.126  loss_mask_8: 0.8527  loss_dice_8: 4.338  loss_mars: 0.8628    time: 5.6076  last_time: 5.0777  data_time: 0.0032  last_data_time: 0.0028   lr: 1e-05  max_mem: 0M
[10/19 19:26:03] d2.utils.events INFO:  eta: 3:16:48  iter: 13439  total_loss: 91.98  loss_ce: 3.342  loss_mask: 0.9264  loss_dice: 4.228  loss_ce_0: 3.204  loss_mask_0: 0.967  loss_dice_0: 4.131  loss_ce_1: 3.444  loss_mask_1: 0.909  loss_dice_1: 4.132  loss_ce_2: 3.38  loss_mask_2: 0.9143  loss_dice_2: 4.231  loss_ce_3: 3.454  loss_mask_3: 0.8705  loss_dice_3: 4.253  loss_ce_4: 3.42  loss_mask_4: 0.9267  loss_dice_4: 4.206  loss_ce_5: 3.38  loss_mask_5: 1.182  loss_dice_5: 4.319  loss_ce_6: 3.418  loss_mask_6: 0.8586  loss_dice_6: 4.141  loss_ce_7: 3.334  loss_mask_7: 0.9352  loss_dice_7: 4.126  loss_ce_8: 3.311  loss_mask_8: 0.9435  loss_dice_8: 4.157  loss_mars: 0.8926    time: 5.6073  last_time: 4.7389  data_time: 0.0025  last_data_time: 0.0025   lr: 1e-05  max_mem: 0M
[10/19 19:27:51] d2.utils.events INFO:  eta: 3:14:58  iter: 13459  total_loss: 90.19  loss_ce: 3.043  loss_mask: 0.6056  loss_dice: 4.351  loss_ce_0: 3.541  loss_mask_0: 0.7138  loss_dice_0: 4.334  loss_ce_1: 3.089  loss_mask_1: 0.6542  loss_dice_1: 4.382  loss_ce_2: 3.126  loss_mask_2: 0.6561  loss_dice_2: 4.289  loss_ce_3: 3.144  loss_mask_3: 0.6455  loss_dice_3: 4.343  loss_ce_4: 3.092  loss_mask_4: 0.7262  loss_dice_4: 4.2  loss_ce_5: 3.035  loss_mask_5: 0.7484  loss_dice_5: 4.347  loss_ce_6: 3.075  loss_mask_6: 0.7944  loss_dice_6: 4.301  loss_ce_7: 3.001  loss_mask_7: 0.6676  loss_dice_7: 4.321  loss_ce_8: 3.014  loss_mask_8: 0.637  loss_dice_8: 4.345  loss_mars: 0.9004    time: 5.6070  last_time: 5.6306  data_time: 0.0029  last_data_time: 0.0039   lr: 1e-05  max_mem: 0M
[10/19 19:29:41] d2.utils.events INFO:  eta: 3:13:01  iter: 13479  total_loss: 78.78  loss_ce: 2.85  loss_mask: 1.723  loss_dice: 3.743  loss_ce_0: 2.632  loss_mask_0: 1.581  loss_dice_0: 3.934  loss_ce_1: 2.637  loss_mask_1: 1.607  loss_dice_1: 3.936  loss_ce_2: 2.674  loss_mask_2: 1.472  loss_dice_2: 3.879  loss_ce_3: 2.651  loss_mask_3: 1.673  loss_dice_3: 3.922  loss_ce_4: 2.683  loss_mask_4: 1.576  loss_dice_4: 3.674  loss_ce_5: 2.731  loss_mask_5: 1.558  loss_dice_5: 3.851  loss_ce_6: 2.728  loss_mask_6: 1.711  loss_dice_6: 3.74  loss_ce_7: 2.791  loss_mask_7: 1.572  loss_dice_7: 3.775  loss_ce_8: 2.804  loss_mask_8: 1.534  loss_dice_8: 3.85  loss_mars: 9.728e-05    time: 5.6067  last_time: 6.1807  data_time: 0.0026  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 19:31:33] d2.utils.events INFO:  eta: 3:11:35  iter: 13499  total_loss: 80.06  loss_ce: 2.559  loss_mask: 1.187  loss_dice: 3.99  loss_ce_0: 2.596  loss_mask_0: 1.25  loss_dice_0: 3.804  loss_ce_1: 2.608  loss_mask_1: 1.178  loss_dice_1: 4.007  loss_ce_2: 2.643  loss_mask_2: 1.109  loss_dice_2: 3.945  loss_ce_3: 2.653  loss_mask_3: 1.198  loss_dice_3: 3.959  loss_ce_4: 2.647  loss_mask_4: 1.191  loss_dice_4: 3.864  loss_ce_5: 2.602  loss_mask_5: 1.246  loss_dice_5: 3.923  loss_ce_6: 2.579  loss_mask_6: 1.249  loss_dice_6: 3.929  loss_ce_7: 2.562  loss_mask_7: 1.224  loss_dice_7: 3.782  loss_ce_8: 2.535  loss_mask_8: 1.228  loss_dice_8: 3.905  loss_mars: 0.8644    time: 5.6067  last_time: 5.9896  data_time: 0.0024  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 19:33:20] d2.utils.events INFO:  eta: 3:09:58  iter: 13519  total_loss: 87.85  loss_ce: 3.348  loss_mask: 1.513  loss_dice: 4.122  loss_ce_0: 3.459  loss_mask_0: 1.33  loss_dice_0: 3.839  loss_ce_1: 3.242  loss_mask_1: 1.155  loss_dice_1: 3.952  loss_ce_2: 3.416  loss_mask_2: 1.116  loss_dice_2: 4.138  loss_ce_3: 3.486  loss_mask_3: 1.221  loss_dice_3: 4.037  loss_ce_4: 3.425  loss_mask_4: 1.139  loss_dice_4: 3.987  loss_ce_5: 3.374  loss_mask_5: 1.523  loss_dice_5: 4.116  loss_ce_6: 3.381  loss_mask_6: 1.259  loss_dice_6: 4.084  loss_ce_7: 3.255  loss_mask_7: 1.169  loss_dice_7: 4.067  loss_ce_8: 3.342  loss_mask_8: 1.329  loss_dice_8: 4.109  loss_mars: 0.8273    time: 5.6063  last_time: 5.1088  data_time: 0.0028  last_data_time: 0.0024   lr: 1e-05  max_mem: 0M
[10/19 19:35:14] d2.utils.events INFO:  eta: 3:08:51  iter: 13539  total_loss: 92.75  loss_ce: 3.455  loss_mask: 1.424  loss_dice: 4.192  loss_ce_0: 3.967  loss_mask_0: 1.488  loss_dice_0: 4.129  loss_ce_1: 3.434  loss_mask_1: 1.364  loss_dice_1: 4.154  loss_ce_2: 3.577  loss_mask_2: 1.371  loss_dice_2: 4.194  loss_ce_3: 3.499  loss_mask_3: 1.568  loss_dice_3: 4.042  loss_ce_4: 3.629  loss_mask_4: 1.509  loss_dice_4: 4.172  loss_ce_5: 3.488  loss_mask_5: 1.406  loss_dice_5: 4.235  loss_ce_6: 3.481  loss_mask_6: 1.455  loss_dice_6: 4.132  loss_ce_7: 3.422  loss_mask_7: 1.442  loss_dice_7: 4.082  loss_ce_8: 3.383  loss_mask_8: 1.45  loss_dice_8: 4.115  loss_mars: 0.853    time: 5.6065  last_time: 5.0845  data_time: 0.0030  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 19:37:04] d2.utils.events INFO:  eta: 3:06:38  iter: 13559  total_loss: 89.05  loss_ce: 3.31  loss_mask: 1.45  loss_dice: 3.981  loss_ce_0: 3.308  loss_mask_0: 1.311  loss_dice_0: 4.117  loss_ce_1: 3.11  loss_mask_1: 1.378  loss_dice_1: 4.055  loss_ce_2: 3.185  loss_mask_2: 1.461  loss_dice_2: 4.077  loss_ce_3: 3.031  loss_mask_3: 1.498  loss_dice_3: 3.961  loss_ce_4: 3.013  loss_mask_4: 1.377  loss_dice_4: 3.874  loss_ce_5: 3.151  loss_mask_5: 1.409  loss_dice_5: 4.002  loss_ce_6: 3.278  loss_mask_6: 1.426  loss_dice_6: 3.913  loss_ce_7: 3.327  loss_mask_7: 1.463  loss_dice_7: 3.872  loss_ce_8: 3.314  loss_mask_8: 1.477  loss_dice_8: 3.942  loss_mars: 0.8413    time: 5.6063  last_time: 5.9000  data_time: 0.0029  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 19:38:58] d2.utils.events INFO:  eta: 3:05:29  iter: 13579  total_loss: 80.77  loss_ce: 2.713  loss_mask: 1.32  loss_dice: 3.857  loss_ce_0: 2.911  loss_mask_0: 1.284  loss_dice_0: 3.839  loss_ce_1: 2.721  loss_mask_1: 1.323  loss_dice_1: 4.072  loss_ce_2: 2.7  loss_mask_2: 1.254  loss_dice_2: 4.003  loss_ce_3: 2.825  loss_mask_3: 1.385  loss_dice_3: 3.902  loss_ce_4: 2.816  loss_mask_4: 1.349  loss_dice_4: 3.827  loss_ce_5: 2.607  loss_mask_5: 1.35  loss_dice_5: 3.955  loss_ce_6: 2.713  loss_mask_6: 1.467  loss_dice_6: 3.879  loss_ce_7: 2.629  loss_mask_7: 1.482  loss_dice_7: 3.937  loss_ce_8: 2.757  loss_mask_8: 1.403  loss_dice_8: 3.833  loss_mars: 0.8229    time: 5.6065  last_time: 5.8604  data_time: 0.0028  last_data_time: 0.0038   lr: 1e-05  max_mem: 0M
[10/19 19:40:49] d2.utils.events INFO:  eta: 3:03:41  iter: 13599  total_loss: 76.23  loss_ce: 1.825  loss_mask: 1.818  loss_dice: 3.685  loss_ce_0: 2.025  loss_mask_0: 1.937  loss_dice_0: 3.704  loss_ce_1: 1.999  loss_mask_1: 1.886  loss_dice_1: 3.597  loss_ce_2: 2.074  loss_mask_2: 2.027  loss_dice_2: 3.632  loss_ce_3: 1.85  loss_mask_3: 1.897  loss_dice_3: 3.576  loss_ce_4: 1.804  loss_mask_4: 1.865  loss_dice_4: 3.471  loss_ce_5: 1.93  loss_mask_5: 1.92  loss_dice_5: 3.9  loss_ce_6: 1.845  loss_mask_6: 1.848  loss_dice_6: 3.585  loss_ce_7: 1.784  loss_mask_7: 1.844  loss_dice_7: 3.57  loss_ce_8: 1.827  loss_mask_8: 1.989  loss_dice_8: 3.521  loss_mars: 0.8254    time: 5.6064  last_time: 5.1843  data_time: 0.0031  last_data_time: 0.0036   lr: 1e-05  max_mem: 0M
[10/19 19:42:39] d2.utils.events INFO:  eta: 3:01:53  iter: 13619  total_loss: 80.09  loss_ce: 2.618  loss_mask: 1.232  loss_dice: 3.873  loss_ce_0: 2.972  loss_mask_0: 1.223  loss_dice_0: 3.982  loss_ce_1: 2.473  loss_mask_1: 1.153  loss_dice_1: 3.911  loss_ce_2: 2.593  loss_mask_2: 1.107  loss_dice_2: 3.949  loss_ce_3: 2.637  loss_mask_3: 1.252  loss_dice_3: 3.907  loss_ce_4: 2.627  loss_mask_4: 1.376  loss_dice_4: 3.905  loss_ce_5: 2.623  loss_mask_5: 1.368  loss_dice_5: 4.18  loss_ce_6: 2.593  loss_mask_6: 1.13  loss_dice_6: 4.053  loss_ce_7: 2.538  loss_mask_7: 1.317  loss_dice_7: 3.922  loss_ce_8: 2.462  loss_mask_8: 1.142  loss_dice_8: 3.986  loss_mars: 0.6477    time: 5.6062  last_time: 5.8934  data_time: 0.0028  last_data_time: 0.0031   lr: 1e-05  max_mem: 0M
[10/19 19:44:28] d2.utils.events INFO:  eta: 2:59:24  iter: 13639  total_loss: 81.66  loss_ce: 2.382  loss_mask: 1.66  loss_dice: 3.417  loss_ce_0: 2.738  loss_mask_0: 1.683  loss_dice_0: 3.566  loss_ce_1: 2.566  loss_mask_1: 1.403  loss_dice_1: 3.449  loss_ce_2: 2.465  loss_mask_2: 1.568  loss_dice_2: 3.546  loss_ce_3: 2.28  loss_mask_3: 1.543  loss_dice_3: 3.582  loss_ce_4: 2.315  loss_mask_4: 1.694  loss_dice_4: 3.688  loss_ce_5: 2.574  loss_mask_5: 1.641  loss_dice_5: 3.674  loss_ce_6: 2.473  loss_mask_6: 1.492  loss_dice_6: 3.518  loss_ce_7: 2.346  loss_mask_7: 1.575  loss_dice_7: 3.461  loss_ce_8: 2.347  loss_mask_8: 1.649  loss_dice_8: 3.525  loss_mars: 0.8298    time: 5.6059  last_time: 5.0568  data_time: 0.0034  last_data_time: 0.0077   lr: 1e-05  max_mem: 0M
[10/19 19:46:17] d2.utils.events INFO:  eta: 2:58:36  iter: 13659  total_loss: 84.79  loss_ce: 2.887  loss_mask: 1.322  loss_dice: 3.996  loss_ce_0: 2.967  loss_mask_0: 1.674  loss_dice_0: 3.96  loss_ce_1: 2.801  loss_mask_1: 1.4  loss_dice_1: 4.065  loss_ce_2: 2.724  loss_mask_2: 1.304  loss_dice_2: 3.966  loss_ce_3: 2.677  loss_mask_3: 1.475  loss_dice_3: 4.007  loss_ce_4: 2.655  loss_mask_4: 1.56  loss_dice_4: 3.879  loss_ce_5: 2.797  loss_mask_5: 1.5  loss_dice_5: 3.931  loss_ce_6: 2.786  loss_mask_6: 1.545  loss_dice_6: 3.907  loss_ce_7: 2.879  loss_mask_7: 1.564  loss_dice_7: 3.871  loss_ce_8: 2.972  loss_mask_8: 1.527  loss_dice_8: 3.855  loss_mars: 0.878    time: 5.6056  last_time: 7.2793  data_time: 0.0021  last_data_time: 0.0022   lr: 1e-05  max_mem: 0M
[10/19 19:48:08] d2.utils.events INFO:  eta: 2:58:27  iter: 13679  total_loss: 92.31  loss_ce: 3.522  loss_mask: 0.7674  loss_dice: 4.294  loss_ce_0: 4.184  loss_mask_0: 0.8658  loss_dice_0: 4.247  loss_ce_1: 3.469  loss_mask_1: 0.7186  loss_dice_1: 4.261  loss_ce_2: 3.489  loss_mask_2: 0.7782  loss_dice_2: 4.361  loss_ce_3: 3.508  loss_mask_3: 0.7903  loss_dice_3: 4.195  loss_ce_4: 3.513  loss_mask_4: 0.7874  loss_dice_4: 4.29  loss_ce_5: 3.502  loss_mask_5: 0.8133  loss_dice_5: 4.372  loss_ce_6: 3.465  loss_mask_6: 0.8026  loss_dice_6: 4.304  loss_ce_7: 3.449  loss_mask_7: 0.8013  loss_dice_7: 4.386  loss_ce_8: 3.554  loss_mask_8: 0.838  loss_dice_8: 4.311  loss_mars: 0.8641    time: 5.6055  last_time: 5.7284  data_time: 0.0030  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 19:49:59] d2.utils.events INFO:  eta: 2:56:43  iter: 13699  total_loss: 84.18  loss_ce: 2.867  loss_mask: 1.111  loss_dice: 4  loss_ce_0: 2.571  loss_mask_0: 1.032  loss_dice_0: 4.007  loss_ce_1: 2.778  loss_mask_1: 0.9765  loss_dice_1: 4.219  loss_ce_2: 2.862  loss_mask_2: 1.041  loss_dice_2: 4.126  loss_ce_3: 2.692  loss_mask_3: 1.13  loss_dice_3: 4.237  loss_ce_4: 2.598  loss_mask_4: 1.023  loss_dice_4: 4.059  loss_ce_5: 2.652  loss_mask_5: 1.054  loss_dice_5: 3.945  loss_ce_6: 2.772  loss_mask_6: 1.082  loss_dice_6: 4.055  loss_ce_7: 2.642  loss_mask_7: 1.049  loss_dice_7: 4.112  loss_ce_8: 2.69  loss_mask_8: 0.9557  loss_dice_8: 4.029  loss_mars: 0.8336    time: 5.6054  last_time: 5.0044  data_time: 0.0030  last_data_time: 0.0024   lr: 1e-05  max_mem: 0M
[10/19 19:51:48] d2.utils.events INFO:  eta: 2:55:36  iter: 13719  total_loss: 94.22  loss_ce: 3.782  loss_mask: 0.798  loss_dice: 4.47  loss_ce_0: 3.653  loss_mask_0: 0.8263  loss_dice_0: 4.489  loss_ce_1: 3.665  loss_mask_1: 0.7166  loss_dice_1: 4.459  loss_ce_2: 3.642  loss_mask_2: 0.7092  loss_dice_2: 4.403  loss_ce_3: 3.516  loss_mask_3: 0.8945  loss_dice_3: 4.525  loss_ce_4: 3.533  loss_mask_4: 0.8122  loss_dice_4: 4.436  loss_ce_5: 3.712  loss_mask_5: 0.8118  loss_dice_5: 4.411  loss_ce_6: 3.719  loss_mask_6: 0.7635  loss_dice_6: 4.442  loss_ce_7: 3.532  loss_mask_7: 0.7657  loss_dice_7: 4.426  loss_ce_8: 3.602  loss_mask_8: 0.7305  loss_dice_8: 4.521  loss_mars: 0.7672    time: 5.6052  last_time: 5.7287  data_time: 0.0031  last_data_time: 0.0030   lr: 1e-05  max_mem: 0M
[10/19 19:53:38] d2.utils.events INFO:  eta: 2:54:18  iter: 13739  total_loss: 91.07  loss_ce: 3.205  loss_mask: 1.239  loss_dice: 4.22  loss_ce_0: 3.582  loss_mask_0: 1.366  loss_dice_0: 4.256  loss_ce_1: 3.089  loss_mask_1: 1.192  loss_dice_1: 4.133  loss_ce_2: 3.209  loss_mask_2: 1.3  loss_dice_2: 4.182  loss_ce_3: 3.142  loss_mask_3: 1.292  loss_dice_3: 4.276  loss_ce_4: 3.145  loss_mask_4: 1.29  loss_dice_4: 4.198  loss_ce_5: 3.24  loss_mask_5: 1.252  loss_dice_5: 4.09  loss_ce_6: 3.156  loss_mask_6: 1.246  loss_dice_6: 4.229  loss_ce_7: 3.167  loss_mask_7: 1.259  loss_dice_7: 4.199  loss_ce_8: 3.155  loss_mask_8: 1.198  loss_dice_8: 4.133  loss_mars: 0.7955    time: 5.6050  last_time: 6.0871  data_time: 0.0037  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 19:55:31] d2.utils.events INFO:  eta: 2:51:12  iter: 13759  total_loss: 89.58  loss_ce: 3.686  loss_mask: 1.031  loss_dice: 4.107  loss_ce_0: 3.653  loss_mask_0: 1.049  loss_dice_0: 4.058  loss_ce_1: 3.594  loss_mask_1: 1.13  loss_dice_1: 4.064  loss_ce_2: 3.616  loss_mask_2: 0.9726  loss_dice_2: 4.141  loss_ce_3: 3.609  loss_mask_3: 1.187  loss_dice_3: 4.122  loss_ce_4: 3.596  loss_mask_4: 1.116  loss_dice_4: 4.042  loss_ce_5: 3.543  loss_mask_5: 1.026  loss_dice_5: 4.126  loss_ce_6: 3.587  loss_mask_6: 1.004  loss_dice_6: 4.11  loss_ce_7: 3.517  loss_mask_7: 1.067  loss_dice_7: 4.052  loss_ce_8: 3.546  loss_mask_8: 1.125  loss_dice_8: 4.032  loss_mars: 0.8758    time: 5.6051  last_time: 5.6805  data_time: 0.0028  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 19:57:19] d2.utils.events INFO:  eta: 2:49:22  iter: 13779  total_loss: 80.42  loss_ce: 2.44  loss_mask: 1.248  loss_dice: 3.781  loss_ce_0: 2.21  loss_mask_0: 1.33  loss_dice_0: 3.817  loss_ce_1: 2.336  loss_mask_1: 1.122  loss_dice_1: 3.867  loss_ce_2: 2.419  loss_mask_2: 1.306  loss_dice_2: 3.642  loss_ce_3: 2.317  loss_mask_3: 1.296  loss_dice_3: 3.656  loss_ce_4: 2.29  loss_mask_4: 1.255  loss_dice_4: 3.605  loss_ce_5: 2.36  loss_mask_5: 1.27  loss_dice_5: 3.895  loss_ce_6: 2.4  loss_mask_6: 1.219  loss_dice_6: 3.43  loss_ce_7: 2.332  loss_mask_7: 1.233  loss_dice_7: 3.936  loss_ce_8: 2.382  loss_mask_8: 1.302  loss_dice_8: 3.948  loss_mars: 0.8653    time: 5.6047  last_time: 5.0488  data_time: 0.0035  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 19:59:06] d2.utils.events INFO:  eta: 2:47:17  iter: 13799  total_loss: 85.36  loss_ce: 3.061  loss_mask: 1.017  loss_dice: 4.239  loss_ce_0: 3.202  loss_mask_0: 0.9811  loss_dice_0: 4.188  loss_ce_1: 3.144  loss_mask_1: 0.9751  loss_dice_1: 4.105  loss_ce_2: 3.12  loss_mask_2: 1.014  loss_dice_2: 4.136  loss_ce_3: 3.108  loss_mask_3: 1.053  loss_dice_3: 4.041  loss_ce_4: 3.027  loss_mask_4: 1.033  loss_dice_4: 4.121  loss_ce_5: 3.069  loss_mask_5: 1.023  loss_dice_5: 4.26  loss_ce_6: 3.118  loss_mask_6: 0.9984  loss_dice_6: 4.202  loss_ce_7: 2.987  loss_mask_7: 1.091  loss_dice_7: 4.18  loss_ce_8: 2.971  loss_mask_8: 0.9925  loss_dice_8: 4.208  loss_mars: 0.8691    time: 5.6043  last_time: 6.2232  data_time: 0.0027  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 20:00:57] d2.utils.events INFO:  eta: 2:44:40  iter: 13819  total_loss: 76.98  loss_ce: 2.386  loss_mask: 1.522  loss_dice: 3.867  loss_ce_0: 2.462  loss_mask_0: 1.536  loss_dice_0: 3.721  loss_ce_1: 2.269  loss_mask_1: 1.505  loss_dice_1: 3.827  loss_ce_2: 2.303  loss_mask_2: 1.524  loss_dice_2: 3.893  loss_ce_3: 2.295  loss_mask_3: 1.788  loss_dice_3: 3.942  loss_ce_4: 2.261  loss_mask_4: 1.59  loss_dice_4: 3.719  loss_ce_5: 2.316  loss_mask_5: 1.487  loss_dice_5: 3.831  loss_ce_6: 2.376  loss_mask_6: 1.493  loss_dice_6: 3.926  loss_ce_7: 2.298  loss_mask_7: 1.481  loss_dice_7: 3.893  loss_ce_8: 2.277  loss_mask_8: 1.627  loss_dice_8: 3.925  loss_mars: 0.8693    time: 5.6043  last_time: 5.9151  data_time: 0.0026  last_data_time: 0.0025   lr: 1e-05  max_mem: 0M
[10/19 20:02:46] d2.utils.events INFO:  eta: 2:43:18  iter: 13839  total_loss: 84.63  loss_ce: 2.907  loss_mask: 1.296  loss_dice: 3.974  loss_ce_0: 2.808  loss_mask_0: 1.359  loss_dice_0: 4.051  loss_ce_1: 2.905  loss_mask_1: 1.23  loss_dice_1: 4.099  loss_ce_2: 2.892  loss_mask_2: 1.431  loss_dice_2: 4.005  loss_ce_3: 2.863  loss_mask_3: 1.271  loss_dice_3: 4.004  loss_ce_4: 2.901  loss_mask_4: 1.232  loss_dice_4: 4.118  loss_ce_5: 2.905  loss_mask_5: 1.266  loss_dice_5: 4.077  loss_ce_6: 2.942  loss_mask_6: 1.36  loss_dice_6: 3.871  loss_ce_7: 2.79  loss_mask_7: 1.357  loss_dice_7: 4.025  loss_ce_8: 2.794  loss_mask_8: 1.322  loss_dice_8: 4.023  loss_mars: 0.8211    time: 5.6040  last_time: 5.0200  data_time: 0.0031  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 20:04:34] d2.utils.events INFO:  eta: 2:41:01  iter: 13859  total_loss: 80.96  loss_ce: 2.754  loss_mask: 1.301  loss_dice: 4.105  loss_ce_0: 2.99  loss_mask_0: 1.187  loss_dice_0: 4.049  loss_ce_1: 2.762  loss_mask_1: 1.163  loss_dice_1: 3.981  loss_ce_2: 2.841  loss_mask_2: 1.121  loss_dice_2: 4.101  loss_ce_3: 2.628  loss_mask_3: 1.148  loss_dice_3: 3.977  loss_ce_4: 2.727  loss_mask_4: 1.111  loss_dice_4: 4.018  loss_ce_5: 2.714  loss_mask_5: 1.168  loss_dice_5: 3.906  loss_ce_6: 2.792  loss_mask_6: 1.184  loss_dice_6: 4.03  loss_ce_7: 2.81  loss_mask_7: 1.285  loss_dice_7: 3.918  loss_ce_8: 2.734  loss_mask_8: 1.184  loss_dice_8: 3.989  loss_mars: 0.813    time: 5.6037  last_time: 5.1658  data_time: 0.0024  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 20:06:22] d2.utils.events INFO:  eta: 2:38:03  iter: 13879  total_loss: 86.32  loss_ce: 3.351  loss_mask: 1.105  loss_dice: 3.889  loss_ce_0: 3.264  loss_mask_0: 1.123  loss_dice_0: 3.925  loss_ce_1: 3.354  loss_mask_1: 0.9885  loss_dice_1: 4.007  loss_ce_2: 3.337  loss_mask_2: 1.115  loss_dice_2: 3.947  loss_ce_3: 3.224  loss_mask_3: 1.063  loss_dice_3: 4.009  loss_ce_4: 3.171  loss_mask_4: 1.11  loss_dice_4: 3.895  loss_ce_5: 3.326  loss_mask_5: 1.061  loss_dice_5: 3.928  loss_ce_6: 3.34  loss_mask_6: 1.018  loss_dice_6: 3.823  loss_ce_7: 3.234  loss_mask_7: 1.132  loss_dice_7: 3.906  loss_ce_8: 3.265  loss_mask_8: 1.063  loss_dice_8: 3.816  loss_mars: 0.8352    time: 5.6034  last_time: 6.1213  data_time: 0.0028  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 20:08:13] d2.utils.events INFO:  eta: 2:36:08  iter: 13899  total_loss: 89.5  loss_ce: 3.326  loss_mask: 1.077  loss_dice: 4.088  loss_ce_0: 4.59  loss_mask_0: 1.188  loss_dice_0: 4.03  loss_ce_1: 3.621  loss_mask_1: 1.195  loss_dice_1: 4.103  loss_ce_2: 3.579  loss_mask_2: 1.265  loss_dice_2: 4.191  loss_ce_3: 3.645  loss_mask_3: 1.194  loss_dice_3: 4.032  loss_ce_4: 3.66  loss_mask_4: 1.107  loss_dice_4: 4.088  loss_ce_5: 3.468  loss_mask_5: 1.184  loss_dice_5: 4.139  loss_ce_6: 3.367  loss_mask_6: 1.071  loss_dice_6: 4.047  loss_ce_7: 3.369  loss_mask_7: 1.187  loss_dice_7: 4.094  loss_ce_8: 3.277  loss_mask_8: 1.129  loss_dice_8: 4.089  loss_mars: 0.9049    time: 5.6032  last_time: 5.0452  data_time: 0.0030  last_data_time: 0.0030   lr: 1e-05  max_mem: 0M
[10/19 20:10:02] d2.utils.events INFO:  eta: 2:34:26  iter: 13919  total_loss: 80.93  loss_ce: 2.322  loss_mask: 1.021  loss_dice: 3.754  loss_ce_0: 2.653  loss_mask_0: 0.9876  loss_dice_0: 3.993  loss_ce_1: 2.511  loss_mask_1: 1.104  loss_dice_1: 3.874  loss_ce_2: 2.493  loss_mask_2: 0.9641  loss_dice_2: 3.75  loss_ce_3: 2.604  loss_mask_3: 1.112  loss_dice_3: 3.976  loss_ce_4: 2.549  loss_mask_4: 1.002  loss_dice_4: 3.745  loss_ce_5: 2.412  loss_mask_5: 1.002  loss_dice_5: 3.729  loss_ce_6: 2.457  loss_mask_6: 0.9591  loss_dice_6: 3.64  loss_ce_7: 2.336  loss_mask_7: 1.051  loss_dice_7: 3.755  loss_ce_8: 2.294  loss_mask_8: 1.001  loss_dice_8: 3.69  loss_mars: 0.784    time: 5.6030  last_time: 5.0301  data_time: 0.0021  last_data_time: 0.0037   lr: 1e-05  max_mem: 0M
[10/19 20:11:56] d2.utils.events INFO:  eta: 2:32:48  iter: 13939  total_loss: 89.5  loss_ce: 3.376  loss_mask: 1.279  loss_dice: 4.206  loss_ce_0: 3.46  loss_mask_0: 1.209  loss_dice_0: 4.154  loss_ce_1: 3.304  loss_mask_1: 1.008  loss_dice_1: 4.151  loss_ce_2: 3.382  loss_mask_2: 1.094  loss_dice_2: 4.27  loss_ce_3: 3.286  loss_mask_3: 1.172  loss_dice_3: 4.128  loss_ce_4: 3.358  loss_mask_4: 1.148  loss_dice_4: 4.151  loss_ce_5: 3.289  loss_mask_5: 1.217  loss_dice_5: 4.217  loss_ce_6: 3.381  loss_mask_6: 1.171  loss_dice_6: 4.178  loss_ce_7: 3.363  loss_mask_7: 1.175  loss_dice_7: 4.226  loss_ce_8: 3.371  loss_mask_8: 1.164  loss_dice_8: 4.192  loss_mars: 0.7874    time: 5.6032  last_time: 5.9014  data_time: 0.0026  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 20:13:48] d2.utils.events INFO:  eta: 2:30:49  iter: 13959  total_loss: 87.48  loss_ce: 3.24  loss_mask: 0.9166  loss_dice: 4.057  loss_ce_0: 4.094  loss_mask_0: 0.7747  loss_dice_0: 4.131  loss_ce_1: 3.37  loss_mask_1: 0.8032  loss_dice_1: 4.1  loss_ce_2: 3.303  loss_mask_2: 0.7668  loss_dice_2: 4.125  loss_ce_3: 3.316  loss_mask_3: 0.8217  loss_dice_3: 4.039  loss_ce_4: 3.394  loss_mask_4: 0.7997  loss_dice_4: 4.229  loss_ce_5: 3.317  loss_mask_5: 0.8227  loss_dice_5: 4.054  loss_ce_6: 3.239  loss_mask_6: 0.8148  loss_dice_6: 4.208  loss_ce_7: 3.16  loss_mask_7: 0.8491  loss_dice_7: 4.143  loss_ce_8: 3.134  loss_mask_8: 0.8274  loss_dice_8: 4.154  loss_mars: 0.8289    time: 5.6032  last_time: 5.9124  data_time: 0.0040  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 20:15:37] d2.utils.events INFO:  eta: 2:28:38  iter: 13979  total_loss: 91.64  loss_ce: 3.776  loss_mask: 0.7207  loss_dice: 4.457  loss_ce_0: 4.705  loss_mask_0: 0.6816  loss_dice_0: 4.508  loss_ce_1: 3.983  loss_mask_1: 0.6614  loss_dice_1: 4.465  loss_ce_2: 3.89  loss_mask_2: 0.6074  loss_dice_2: 4.458  loss_ce_3: 3.99  loss_mask_3: 0.6821  loss_dice_3: 4.527  loss_ce_4: 4.031  loss_mask_4: 0.701  loss_dice_4: 4.436  loss_ce_5: 3.81  loss_mask_5: 0.6719  loss_dice_5: 4.457  loss_ce_6: 3.749  loss_mask_6: 0.6697  loss_dice_6: 4.508  loss_ce_7: 3.691  loss_mask_7: 0.6968  loss_dice_7: 4.501  loss_ce_8: 3.655  loss_mask_8: 0.6999  loss_dice_8: 4.511  loss_mars: 0.863    time: 5.6029  last_time: 5.6338  data_time: 0.0030  last_data_time: 0.0036   lr: 1e-05  max_mem: 0M
[10/19 20:17:31] d2.utils.events INFO:  eta: 2:26:49  iter: 13999  total_loss: 91.44  loss_ce: 3.848  loss_mask: 1.091  loss_dice: 4.076  loss_ce_0: 4.406  loss_mask_0: 1.056  loss_dice_0: 4.303  loss_ce_1: 3.748  loss_mask_1: 0.9794  loss_dice_1: 4.255  loss_ce_2: 3.711  loss_mask_2: 1.045  loss_dice_2: 4.285  loss_ce_3: 3.695  loss_mask_3: 1.01  loss_dice_3: 4.121  loss_ce_4: 3.736  loss_mask_4: 1.08  loss_dice_4: 4.117  loss_ce_5: 3.732  loss_mask_5: 1.055  loss_dice_5: 4.143  loss_ce_6: 3.791  loss_mask_6: 1.079  loss_dice_6: 4.147  loss_ce_7: 3.833  loss_mask_7: 1.062  loss_dice_7: 4.153  loss_ce_8: 3.783  loss_mask_8: 1.075  loss_dice_8: 4.147  loss_mars: 0.8407    time: 5.6030  last_time: 5.0744  data_time: 0.0036  last_data_time: 0.0028   lr: 1e-05  max_mem: 0M
[10/19 20:19:20] d2.utils.events INFO:  eta: 2:25:01  iter: 14019  total_loss: 83.23  loss_ce: 2.553  loss_mask: 1.153  loss_dice: 3.798  loss_ce_0: 2.589  loss_mask_0: 1.099  loss_dice_0: 3.72  loss_ce_1: 2.691  loss_mask_1: 1.111  loss_dice_1: 3.99  loss_ce_2: 2.696  loss_mask_2: 1.087  loss_dice_2: 3.832  loss_ce_3: 2.661  loss_mask_3: 1.245  loss_dice_3: 3.795  loss_ce_4: 2.688  loss_mask_4: 1.209  loss_dice_4: 3.921  loss_ce_5: 2.599  loss_mask_5: 1.171  loss_dice_5: 3.656  loss_ce_6: 2.675  loss_mask_6: 1.089  loss_dice_6: 3.741  loss_ce_7: 2.619  loss_mask_7: 1.167  loss_dice_7: 3.684  loss_ce_8: 2.548  loss_mask_8: 1.171  loss_dice_8: 3.864  loss_mars: 0.8394    time: 5.6028  last_time: 3.8579  data_time: 0.0027  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 20:21:01] d2.utils.events INFO:  eta: 2:22:26  iter: 14039  total_loss: 92.21  loss_ce: 3.595  loss_mask: 1.085  loss_dice: 4.254  loss_ce_0: 3.987  loss_mask_0: 1.192  loss_dice_0: 4.437  loss_ce_1: 3.562  loss_mask_1: 1.182  loss_dice_1: 4.382  loss_ce_2: 3.51  loss_mask_2: 1.152  loss_dice_2: 4.374  loss_ce_3: 3.451  loss_mask_3: 1.217  loss_dice_3: 4.319  loss_ce_4: 3.487  loss_mask_4: 1.255  loss_dice_4: 4.278  loss_ce_5: 3.572  loss_mask_5: 1.178  loss_dice_5: 4.307  loss_ce_6: 3.624  loss_mask_6: 1.177  loss_dice_6: 4.277  loss_ce_7: 3.482  loss_mask_7: 1.052  loss_dice_7: 4.245  loss_ce_8: 3.614  loss_mask_8: 1.16  loss_dice_8: 4.245  loss_mars: 0.8093    time: 5.6019  last_time: 5.1105  data_time: 0.0040  last_data_time: 0.0030   lr: 1e-05  max_mem: 0M
[10/19 20:22:50] d2.utils.events INFO:  eta: 2:20:31  iter: 14059  total_loss: 85.42  loss_ce: 3.245  loss_mask: 0.8526  loss_dice: 4.316  loss_ce_0: 4.087  loss_mask_0: 0.8991  loss_dice_0: 4.336  loss_ce_1: 3.7  loss_mask_1: 0.835  loss_dice_1: 4.289  loss_ce_2: 3.642  loss_mask_2: 0.8129  loss_dice_2: 4.333  loss_ce_3: 3.467  loss_mask_3: 0.9491  loss_dice_3: 4.223  loss_ce_4: 3.462  loss_mask_4: 0.8825  loss_dice_4: 4.203  loss_ce_5: 3.432  loss_mask_5: 0.845  loss_dice_5: 4.351  loss_ce_6: 3.308  loss_mask_6: 0.8436  loss_dice_6: 4.354  loss_ce_7: 3.257  loss_mask_7: 0.9226  loss_dice_7: 4.301  loss_ce_8: 3.287  loss_mask_8: 0.9232  loss_dice_8: 4.349  loss_mars: 0.712    time: 5.6017  last_time: 4.7185  data_time: 0.0033  last_data_time: 0.0024   lr: 1e-05  max_mem: 0M
[10/19 20:23:30] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_0014066.pth
[10/19 20:23:31] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 20:23:31] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 20:23:31] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 20:23:31] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/19 20:24:45] d2.utils.events INFO:  eta: 2:18:55  iter: 14079  total_loss: 85  loss_ce: 3.357  loss_mask: 0.9355  loss_dice: 4.165  loss_ce_0: 3.213  loss_mask_0: 0.9443  loss_dice_0: 4.269  loss_ce_1: 3.298  loss_mask_1: 0.8498  loss_dice_1: 4.149  loss_ce_2: 3.229  loss_mask_2: 0.9084  loss_dice_2: 4.171  loss_ce_3: 3.258  loss_mask_3: 0.8323  loss_dice_3: 4.21  loss_ce_4: 3.227  loss_mask_4: 0.9531  loss_dice_4: 4.159  loss_ce_5: 3.248  loss_mask_5: 1.018  loss_dice_5: 4.123  loss_ce_6: 3.254  loss_mask_6: 0.9595  loss_dice_6: 4.157  loss_ce_7: 3.269  loss_mask_7: 0.9484  loss_dice_7: 4.096  loss_ce_8: 3.249  loss_mask_8: 1.049  loss_dice_8: 4.086  loss_mars: 0.8401    time: 5.6019  last_time: 4.9310  data_time: 0.0024  last_data_time: 0.0022   lr: 1e-05  max_mem: 0M
[10/19 20:26:30] d2.utils.events INFO:  eta: 2:17:00  iter: 14099  total_loss: 83.25  loss_ce: 2.956  loss_mask: 0.9259  loss_dice: 3.84  loss_ce_0: 3.149  loss_mask_0: 0.9894  loss_dice_0: 3.928  loss_ce_1: 3.183  loss_mask_1: 0.9742  loss_dice_1: 3.895  loss_ce_2: 3.221  loss_mask_2: 0.9193  loss_dice_2: 3.926  loss_ce_3: 3.241  loss_mask_3: 0.9565  loss_dice_3: 4.03  loss_ce_4: 3.302  loss_mask_4: 0.9958  loss_dice_4: 3.956  loss_ce_5: 3.149  loss_mask_5: 0.9663  loss_dice_5: 3.895  loss_ce_6: 3.11  loss_mask_6: 0.9063  loss_dice_6: 4.074  loss_ce_7: 2.981  loss_mask_7: 0.9596  loss_dice_7: 3.887  loss_ce_8: 2.907  loss_mask_8: 0.993  loss_dice_8: 3.66  loss_mars: 0.7644    time: 5.6013  last_time: 5.1211  data_time: 0.0031  last_data_time: 0.0022   lr: 1e-05  max_mem: 0M
[10/19 20:28:20] d2.utils.events INFO:  eta: 2:15:12  iter: 14119  total_loss: 83.47  loss_ce: 2.631  loss_mask: 1.364  loss_dice: 3.895  loss_ce_0: 3.178  loss_mask_0: 1.396  loss_dice_0: 4.005  loss_ce_1: 2.669  loss_mask_1: 1.37  loss_dice_1: 3.948  loss_ce_2: 2.695  loss_mask_2: 1.375  loss_dice_2: 3.954  loss_ce_3: 2.726  loss_mask_3: 1.307  loss_dice_3: 3.896  loss_ce_4: 2.794  loss_mask_4: 1.316  loss_dice_4: 3.973  loss_ce_5: 2.722  loss_mask_5: 1.281  loss_dice_5: 3.984  loss_ce_6: 2.739  loss_mask_6: 1.284  loss_dice_6: 3.96  loss_ce_7: 2.677  loss_mask_7: 1.36  loss_dice_7: 3.946  loss_ce_8: 2.624  loss_mask_8: 1.343  loss_dice_8: 3.962  loss_mars: 0.7751    time: 5.6012  last_time: 4.8173  data_time: 0.0030  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 20:30:10] d2.utils.events INFO:  eta: 2:13:32  iter: 14139  total_loss: 79.84  loss_ce: 2.826  loss_mask: 1.135  loss_dice: 4.063  loss_ce_0: 3.124  loss_mask_0: 1.165  loss_dice_0: 4.122  loss_ce_1: 2.984  loss_mask_1: 1.136  loss_dice_1: 4.175  loss_ce_2: 2.964  loss_mask_2: 1.127  loss_dice_2: 4.106  loss_ce_3: 2.885  loss_mask_3: 1.083  loss_dice_3: 4.159  loss_ce_4: 2.937  loss_mask_4: 1.106  loss_dice_4: 4.095  loss_ce_5: 2.803  loss_mask_5: 1.125  loss_dice_5: 4.067  loss_ce_6: 2.847  loss_mask_6: 1.026  loss_dice_6: 4.061  loss_ce_7: 2.756  loss_mask_7: 1.359  loss_dice_7: 3.934  loss_ce_8: 2.75  loss_mask_8: 1.311  loss_dice_8: 4.023  loss_mars: 0.7703    time: 5.6010  last_time: 5.8478  data_time: 0.0031  last_data_time: 0.0019   lr: 1e-05  max_mem: 0M
[10/19 20:32:02] d2.utils.events INFO:  eta: 2:11:49  iter: 14159  total_loss: 84.85  loss_ce: 2.942  loss_mask: 1.074  loss_dice: 4.082  loss_ce_0: 2.947  loss_mask_0: 1.05  loss_dice_0: 3.966  loss_ce_1: 2.893  loss_mask_1: 1.057  loss_dice_1: 4.065  loss_ce_2: 3.083  loss_mask_2: 1.081  loss_dice_2: 4.047  loss_ce_3: 2.869  loss_mask_3: 1.198  loss_dice_3: 4.15  loss_ce_4: 2.819  loss_mask_4: 1.192  loss_dice_4: 4.048  loss_ce_5: 2.96  loss_mask_5: 1.093  loss_dice_5: 3.954  loss_ce_6: 2.997  loss_mask_6: 1.122  loss_dice_6: 3.939  loss_ce_7: 2.93  loss_mask_7: 1.131  loss_dice_7: 4.059  loss_ce_8: 2.961  loss_mask_8: 1.074  loss_dice_8: 3.97  loss_mars: 0.8162    time: 5.6010  last_time: 5.0277  data_time: 0.0034  last_data_time: 0.0079   lr: 1e-05  max_mem: 0M
[10/19 20:33:50] d2.utils.events INFO:  eta: 2:10:07  iter: 14179  total_loss: 90.45  loss_ce: 3.251  loss_mask: 1.153  loss_dice: 4.324  loss_ce_0: 4.365  loss_mask_0: 1.14  loss_dice_0: 4.351  loss_ce_1: 3.447  loss_mask_1: 0.9682  loss_dice_1: 4.385  loss_ce_2: 3.463  loss_mask_2: 1.056  loss_dice_2: 4.382  loss_ce_3: 3.195  loss_mask_3: 1.082  loss_dice_3: 4.378  loss_ce_4: 3.198  loss_mask_4: 1.083  loss_dice_4: 4.341  loss_ce_5: 3.255  loss_mask_5: 1.096  loss_dice_5: 4.316  loss_ce_6: 3.283  loss_mask_6: 1.085  loss_dice_6: 4.343  loss_ce_7: 3.275  loss_mask_7: 1.283  loss_dice_7: 4.312  loss_ce_8: 3.246  loss_mask_8: 1.224  loss_dice_8: 4.341  loss_mars: 0.8778    time: 5.6007  last_time: 4.9261  data_time: 0.0036  last_data_time: 0.0081   lr: 1e-05  max_mem: 0M
[10/19 20:35:37] d2.utils.events INFO:  eta: 2:08:05  iter: 14199  total_loss: 90.57  loss_ce: 3.631  loss_mask: 1.063  loss_dice: 3.946  loss_ce_0: 4.392  loss_mask_0: 1.096  loss_dice_0: 3.953  loss_ce_1: 3.854  loss_mask_1: 0.9767  loss_dice_1: 4.094  loss_ce_2: 3.73  loss_mask_2: 1.131  loss_dice_2: 4.06  loss_ce_3: 3.598  loss_mask_3: 1.092  loss_dice_3: 4.098  loss_ce_4: 3.562  loss_mask_4: 1.175  loss_dice_4: 4.051  loss_ce_5: 3.657  loss_mask_5: 1.04  loss_dice_5: 4.026  loss_ce_6: 3.533  loss_mask_6: 1.096  loss_dice_6: 4.125  loss_ce_7: 3.571  loss_mask_7: 1.154  loss_dice_7: 3.981  loss_ce_8: 3.641  loss_mask_8: 1.191  loss_dice_8: 4.054  loss_mars: 0.7219    time: 5.6003  last_time: 5.9101  data_time: 0.0032  last_data_time: 0.0023   lr: 1e-05  max_mem: 0M
[10/19 20:37:29] d2.utils.events INFO:  eta: 2:06:16  iter: 14219  total_loss: 89.98  loss_ce: 3.425  loss_mask: 0.6567  loss_dice: 4.206  loss_ce_0: 3.778  loss_mask_0: 0.6401  loss_dice_0: 4.132  loss_ce_1: 3.375  loss_mask_1: 0.6542  loss_dice_1: 4.182  loss_ce_2: 3.437  loss_mask_2: 0.6425  loss_dice_2: 4.225  loss_ce_3: 3.428  loss_mask_3: 0.7088  loss_dice_3: 4.291  loss_ce_4: 3.343  loss_mask_4: 0.66  loss_dice_4: 4.149  loss_ce_5: 3.332  loss_mask_5: 0.7069  loss_dice_5: 4.146  loss_ce_6: 3.366  loss_mask_6: 0.6662  loss_dice_6: 4.176  loss_ce_7: 3.359  loss_mask_7: 0.6787  loss_dice_7: 4.255  loss_ce_8: 3.294  loss_mask_8: 0.7258  loss_dice_8: 4.255  loss_mars: 0.84    time: 5.6003  last_time: 5.2648  data_time: 0.0029  last_data_time: 0.0023   lr: 1e-05  max_mem: 0M
[10/19 20:39:17] d2.utils.events INFO:  eta: 2:04:22  iter: 14239  total_loss: 88.35  loss_ce: 3.085  loss_mask: 1.304  loss_dice: 4.141  loss_ce_0: 3.684  loss_mask_0: 1.229  loss_dice_0: 4.317  loss_ce_1: 3.037  loss_mask_1: 1.198  loss_dice_1: 4.289  loss_ce_2: 2.971  loss_mask_2: 1.212  loss_dice_2: 4.243  loss_ce_3: 2.948  loss_mask_3: 1.196  loss_dice_3: 4.145  loss_ce_4: 2.98  loss_mask_4: 1.237  loss_dice_4: 4.134  loss_ce_5: 2.999  loss_mask_5: 1.307  loss_dice_5: 3.912  loss_ce_6: 3.04  loss_mask_6: 1.256  loss_dice_6: 4.146  loss_ce_7: 3.063  loss_mask_7: 1.311  loss_dice_7: 4.105  loss_ce_8: 3.072  loss_mask_8: 1.245  loss_dice_8: 4.11  loss_mars: 0.8487    time: 5.6000  last_time: 5.7550  data_time: 0.0028  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 20:41:09] d2.utils.events INFO:  eta: 2:02:42  iter: 14259  total_loss: 88.58  loss_ce: 3.125  loss_mask: 1.521  loss_dice: 3.777  loss_ce_0: 2.772  loss_mask_0: 1.534  loss_dice_0: 3.752  loss_ce_1: 2.979  loss_mask_1: 1.661  loss_dice_1: 4.121  loss_ce_2: 2.937  loss_mask_2: 1.579  loss_dice_2: 3.83  loss_ce_3: 2.928  loss_mask_3: 1.722  loss_dice_3: 3.868  loss_ce_4: 2.885  loss_mask_4: 1.658  loss_dice_4: 3.82  loss_ce_5: 2.963  loss_mask_5: 1.667  loss_dice_5: 3.867  loss_ce_6: 3.025  loss_mask_6: 1.548  loss_dice_6: 3.928  loss_ce_7: 2.983  loss_mask_7: 1.665  loss_dice_7: 3.797  loss_ce_8: 3.028  loss_mask_8: 1.899  loss_dice_8: 3.837  loss_mars: 0.8291    time: 5.6000  last_time: 5.9627  data_time: 0.0035  last_data_time: 0.0042   lr: 1e-05  max_mem: 0M
[10/19 20:42:57] d2.utils.events INFO:  eta: 2:00:41  iter: 14279  total_loss: 90.1  loss_ce: 3.709  loss_mask: 0.8784  loss_dice: 4.14  loss_ce_0: 4.032  loss_mask_0: 0.8659  loss_dice_0: 4.025  loss_ce_1: 3.758  loss_mask_1: 0.8552  loss_dice_1: 4.044  loss_ce_2: 3.741  loss_mask_2: 0.8312  loss_dice_2: 4.066  loss_ce_3: 3.639  loss_mask_3: 0.9375  loss_dice_3: 4.113  loss_ce_4: 3.645  loss_mask_4: 1.043  loss_dice_4: 4.02  loss_ce_5: 3.688  loss_mask_5: 0.9616  loss_dice_5: 4.01  loss_ce_6: 3.815  loss_mask_6: 0.9919  loss_dice_6: 4.035  loss_ce_7: 3.682  loss_mask_7: 1.011  loss_dice_7: 4.03  loss_ce_8: 3.696  loss_mask_8: 0.8837  loss_dice_8: 4.098  loss_mars: 0.8932    time: 5.5996  last_time: 6.4031  data_time: 0.0029  last_data_time: 0.0020   lr: 1e-05  max_mem: 0M
[10/19 20:44:48] d2.utils.events INFO:  eta: 1:58:49  iter: 14299  total_loss: 91.17  loss_ce: 3.561  loss_mask: 1.144  loss_dice: 3.421  loss_ce_0: 3.399  loss_mask_0: 1.138  loss_dice_0: 3.84  loss_ce_1: 3.473  loss_mask_1: 1.093  loss_dice_1: 3.741  loss_ce_2: 3.451  loss_mask_2: 1.052  loss_dice_2: 4.059  loss_ce_3: 3.338  loss_mask_3: 1.092  loss_dice_3: 3.875  loss_ce_4: 3.381  loss_mask_4: 1.235  loss_dice_4: 3.515  loss_ce_5: 3.431  loss_mask_5: 1.194  loss_dice_5: 3.583  loss_ce_6: 3.549  loss_mask_6: 1.134  loss_dice_6: 3.555  loss_ce_7: 3.45  loss_mask_7: 1.093  loss_dice_7: 3.582  loss_ce_8: 3.447  loss_mask_8: 1.198  loss_dice_8: 3.807  loss_mars: 0.8494    time: 5.5996  last_time: 5.5457  data_time: 0.0031  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 20:46:39] d2.utils.events INFO:  eta: 1:56:30  iter: 14319  total_loss: 77.81  loss_ce: 2.792  loss_mask: 1.007  loss_dice: 3.847  loss_ce_0: 2.996  loss_mask_0: 0.9791  loss_dice_0: 3.952  loss_ce_1: 2.821  loss_mask_1: 1.043  loss_dice_1: 3.816  loss_ce_2: 2.819  loss_mask_2: 1.032  loss_dice_2: 3.856  loss_ce_3: 2.868  loss_mask_3: 0.9718  loss_dice_3: 3.884  loss_ce_4: 2.838  loss_mask_4: 1.084  loss_dice_4: 3.889  loss_ce_5: 2.829  loss_mask_5: 1.082  loss_dice_5: 3.673  loss_ce_6: 2.923  loss_mask_6: 1.055  loss_dice_6: 3.862  loss_ce_7: 2.766  loss_mask_7: 1.063  loss_dice_7: 3.917  loss_ce_8: 2.797  loss_mask_8: 1.045  loss_dice_8: 3.978  loss_mars: 0.8798    time: 5.5995  last_time: 5.8150  data_time: 0.0022  last_data_time: 0.0023   lr: 1e-05  max_mem: 0M
[10/19 20:48:28] d2.utils.events INFO:  eta: 1:55:14  iter: 14339  total_loss: 88.13  loss_ce: 2.988  loss_mask: 1.684  loss_dice: 4.049  loss_ce_0: 3.257  loss_mask_0: 1.615  loss_dice_0: 4.121  loss_ce_1: 2.936  loss_mask_1: 1.617  loss_dice_1: 4.123  loss_ce_2: 2.946  loss_mask_2: 1.647  loss_dice_2: 3.974  loss_ce_3: 2.966  loss_mask_3: 1.632  loss_dice_3: 4.129  loss_ce_4: 2.911  loss_mask_4: 1.678  loss_dice_4: 4.126  loss_ce_5: 2.948  loss_mask_5: 1.576  loss_dice_5: 4.089  loss_ce_6: 3.035  loss_mask_6: 1.621  loss_dice_6: 4.053  loss_ce_7: 2.874  loss_mask_7: 1.701  loss_dice_7: 4.029  loss_ce_8: 2.868  loss_mask_8: 1.654  loss_dice_8: 4.02  loss_mars: 0.8663    time: 5.5993  last_time: 6.1953  data_time: 0.0030  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 20:50:20] d2.utils.events INFO:  eta: 1:53:31  iter: 14359  total_loss: 82.03  loss_ce: 3.144  loss_mask: 1.21  loss_dice: 3.794  loss_ce_0: 3.801  loss_mask_0: 1.236  loss_dice_0: 3.893  loss_ce_1: 3.026  loss_mask_1: 1.279  loss_dice_1: 3.842  loss_ce_2: 3.088  loss_mask_2: 1.307  loss_dice_2: 3.866  loss_ce_3: 3.132  loss_mask_3: 1.277  loss_dice_3: 3.836  loss_ce_4: 3.193  loss_mask_4: 1.25  loss_dice_4: 3.789  loss_ce_5: 3.251  loss_mask_5: 1.182  loss_dice_5: 3.823  loss_ce_6: 3.207  loss_mask_6: 1.333  loss_dice_6: 3.826  loss_ce_7: 3.089  loss_mask_7: 1.307  loss_dice_7: 3.86  loss_ce_8: 3.197  loss_mask_8: 1.222  loss_dice_8: 3.826  loss_mars: 0.9179    time: 5.5993  last_time: 5.2977  data_time: 0.0036  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 20:52:18] d2.utils.events INFO:  eta: 1:51:45  iter: 14379  total_loss: 89.69  loss_ce: 3.597  loss_mask: 0.7618  loss_dice: 4.204  loss_ce_0: 4.265  loss_mask_0: 0.71  loss_dice_0: 4.261  loss_ce_1: 3.518  loss_mask_1: 0.7576  loss_dice_1: 4.305  loss_ce_2: 3.596  loss_mask_2: 0.6926  loss_dice_2: 4.156  loss_ce_3: 3.562  loss_mask_3: 0.7669  loss_dice_3: 4.171  loss_ce_4: 3.816  loss_mask_4: 0.7568  loss_dice_4: 4.16  loss_ce_5: 3.609  loss_mask_5: 0.7862  loss_dice_5: 4.066  loss_ce_6: 3.51  loss_mask_6: 0.7737  loss_dice_6: 4.123  loss_ce_7: 3.44  loss_mask_7: 0.8107  loss_dice_7: 4.182  loss_ce_8: 3.458  loss_mask_8: 0.7786  loss_dice_8: 4.236  loss_mars: 0.8605    time: 5.5997  last_time: 6.2516  data_time: 0.0037  last_data_time: 0.0048   lr: 1e-05  max_mem: 0M
[10/19 20:54:12] d2.utils.events INFO:  eta: 1:50:19  iter: 14399  total_loss: 84.74  loss_ce: 3.059  loss_mask: 1.31  loss_dice: 3.974  loss_ce_0: 3.524  loss_mask_0: 1.256  loss_dice_0: 4.043  loss_ce_1: 2.866  loss_mask_1: 1.115  loss_dice_1: 4.097  loss_ce_2: 2.741  loss_mask_2: 1.191  loss_dice_2: 3.987  loss_ce_3: 2.756  loss_mask_3: 1.287  loss_dice_3: 4.105  loss_ce_4: 2.879  loss_mask_4: 1.182  loss_dice_4: 4.094  loss_ce_5: 2.717  loss_mask_5: 1.254  loss_dice_5: 3.923  loss_ce_6: 2.813  loss_mask_6: 1.19  loss_dice_6: 3.98  loss_ce_7: 2.892  loss_mask_7: 1.259  loss_dice_7: 3.912  loss_ce_8: 3.06  loss_mask_8: 1.202  loss_dice_8: 3.962  loss_mars: 0.8908    time: 5.5999  last_time: 5.0733  data_time: 0.0026  last_data_time: 0.0025   lr: 1e-05  max_mem: 0M
[10/19 20:56:03] d2.utils.events INFO:  eta: 1:48:34  iter: 14419  total_loss: 83.69  loss_ce: 3.162  loss_mask: 1.481  loss_dice: 3.678  loss_ce_0: 3.522  loss_mask_0: 1.639  loss_dice_0: 3.74  loss_ce_1: 3.117  loss_mask_1: 1.511  loss_dice_1: 3.733  loss_ce_2: 3.024  loss_mask_2: 1.464  loss_dice_2: 3.842  loss_ce_3: 3.04  loss_mask_3: 1.555  loss_dice_3: 3.75  loss_ce_4: 3.133  loss_mask_4: 1.555  loss_dice_4: 3.685  loss_ce_5: 3.101  loss_mask_5: 1.584  loss_dice_5: 3.78  loss_ce_6: 3.057  loss_mask_6: 1.54  loss_dice_6: 3.696  loss_ce_7: 3.027  loss_mask_7: 1.499  loss_dice_7: 3.562  loss_ce_8: 3.087  loss_mask_8: 1.436  loss_dice_8: 3.565  loss_mars: 0.8967    time: 5.5998  last_time: 5.4740  data_time: 0.0034  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 20:57:58] d2.utils.events INFO:  eta: 1:47:11  iter: 14439  total_loss: 81.7  loss_ce: 3.11  loss_mask: 1.393  loss_dice: 3.893  loss_ce_0: 3.904  loss_mask_0: 1.376  loss_dice_0: 3.715  loss_ce_1: 3.269  loss_mask_1: 1.209  loss_dice_1: 3.644  loss_ce_2: 3.287  loss_mask_2: 1.31  loss_dice_2: 3.748  loss_ce_3: 3.175  loss_mask_3: 1.323  loss_dice_3: 3.788  loss_ce_4: 3.127  loss_mask_4: 1.359  loss_dice_4: 3.665  loss_ce_5: 3.188  loss_mask_5: 1.414  loss_dice_5: 3.744  loss_ce_6: 3.158  loss_mask_6: 1.355  loss_dice_6: 3.653  loss_ce_7: 3.05  loss_mask_7: 1.366  loss_dice_7: 3.629  loss_ce_8: 3.082  loss_mask_8: 1.419  loss_dice_8: 3.7  loss_mars: 0.8676    time: 5.6000  last_time: 6.0930  data_time: 0.0034  last_data_time: 0.0018   lr: 1e-05  max_mem: 0M
[10/19 20:59:47] d2.utils.events INFO:  eta: 1:45:22  iter: 14459  total_loss: 90.63  loss_ce: 3.626  loss_mask: 1.003  loss_dice: 4.394  loss_ce_0: 4.531  loss_mask_0: 0.8792  loss_dice_0: 4.245  loss_ce_1: 3.73  loss_mask_1: 0.8979  loss_dice_1: 4.434  loss_ce_2: 3.593  loss_mask_2: 0.9108  loss_dice_2: 4.4  loss_ce_3: 3.453  loss_mask_3: 0.9198  loss_dice_3: 4.462  loss_ce_4: 3.52  loss_mask_4: 0.9995  loss_dice_4: 4.411  loss_ce_5: 3.674  loss_mask_5: 0.9461  loss_dice_5: 4.356  loss_ce_6: 3.548  loss_mask_6: 0.9316  loss_dice_6: 4.341  loss_ce_7: 3.643  loss_mask_7: 0.9542  loss_dice_7: 4.416  loss_ce_8: 3.612  loss_mask_8: 0.9601  loss_dice_8: 4.409  loss_mars: 0.8036    time: 5.5998  last_time: 5.3862  data_time: 0.0028  last_data_time: 0.0021   lr: 1e-05  max_mem: 0M
[10/19 21:01:39] d2.utils.events INFO:  eta: 1:43:34  iter: 14479  total_loss: 84.06  loss_ce: 2.807  loss_mask: 1.442  loss_dice: 3.839  loss_ce_0: 2.984  loss_mask_0: 1.569  loss_dice_0: 3.985  loss_ce_1: 2.715  loss_mask_1: 1.422  loss_dice_1: 3.872  loss_ce_2: 2.832  loss_mask_2: 1.495  loss_dice_2: 3.9  loss_ce_3: 2.732  loss_mask_3: 1.455  loss_dice_3: 3.769  loss_ce_4: 2.835  loss_mask_4: 1.423  loss_dice_4: 3.773  loss_ce_5: 2.696  loss_mask_5: 1.586  loss_dice_5: 3.696  loss_ce_6: 2.797  loss_mask_6: 1.502  loss_dice_6: 3.761  loss_ce_7: 2.778  loss_mask_7: 1.462  loss_dice_7: 3.913  loss_ce_8: 2.778  loss_mask_8: 1.544  loss_dice_8: 3.869  loss_mars: 0.6865    time: 5.5998  last_time: 4.9843  data_time: 0.0026  last_data_time: 0.0022   lr: 1e-05  max_mem: 0M
[10/19 21:03:29] d2.utils.events INFO:  eta: 1:41:44  iter: 14499  total_loss: 84.38  loss_ce: 2.833  loss_mask: 1.295  loss_dice: 3.785  loss_ce_0: 2.918  loss_mask_0: 1.221  loss_dice_0: 3.935  loss_ce_1: 2.663  loss_mask_1: 1.373  loss_dice_1: 3.687  loss_ce_2: 2.804  loss_mask_2: 1.262  loss_dice_2: 3.879  loss_ce_3: 2.765  loss_mask_3: 1.309  loss_dice_3: 3.798  loss_ce_4: 2.768  loss_mask_4: 1.358  loss_dice_4: 3.797  loss_ce_5: 2.74  loss_mask_5: 1.276  loss_dice_5: 3.631  loss_ce_6: 2.775  loss_mask_6: 1.404  loss_dice_6: 3.762  loss_ce_7: 2.669  loss_mask_7: 1.242  loss_dice_7: 3.828  loss_ce_8: 2.761  loss_mask_8: 1.275  loss_dice_8: 3.699  loss_mars: 0.8432    time: 5.5996  last_time: 5.6657  data_time: 0.0037  last_data_time: 0.0017   lr: 1e-05  max_mem: 0M
[10/19 21:05:22] d2.utils.events INFO:  eta: 1:40:12  iter: 14519  total_loss: 87.99  loss_ce: 3.767  loss_mask: 0.9755  loss_dice: 3.883  loss_ce_0: 3.892  loss_mask_0: 1.133  loss_dice_0: 4.071  loss_ce_1: 3.78  loss_mask_1: 1.045  loss_dice_1: 3.943  loss_ce_2: 3.609  loss_mask_2: 1.075  loss_dice_2: 3.735  loss_ce_3: 3.625  loss_mask_3: 1.109  loss_dice_3: 3.892  loss_ce_4: 3.711  loss_mask_4: 1.187  loss_dice_4: 3.907  loss_ce_5: 3.684  loss_mask_5: 1.083  loss_dice_5: 3.957  loss_ce_6: 3.735  loss_mask_6: 1.05  loss_dice_6: 3.89  loss_ce_7: 3.713  loss_mask_7: 1.19  loss_dice_7: 3.98  loss_ce_8: 3.794  loss_mask_8: 0.9969  loss_dice_8: 3.903  loss_mars: 0.7219    time: 5.5997  last_time: 6.1281  data_time: 0.0022  last_data_time: 0.0043   lr: 1e-06  max_mem: 0M
[10/19 21:07:16] d2.utils.events INFO:  eta: 1:38:24  iter: 14539  total_loss: 88.24  loss_ce: 3.459  loss_mask: 1.298  loss_dice: 3.939  loss_ce_0: 3.649  loss_mask_0: 1.308  loss_dice_0: 3.958  loss_ce_1: 3.238  loss_mask_1: 1.28  loss_dice_1: 3.972  loss_ce_2: 3.287  loss_mask_2: 1.254  loss_dice_2: 3.921  loss_ce_3: 3.167  loss_mask_3: 1.298  loss_dice_3: 3.998  loss_ce_4: 3.17  loss_mask_4: 1.301  loss_dice_4: 3.935  loss_ce_5: 3.19  loss_mask_5: 1.312  loss_dice_5: 3.857  loss_ce_6: 3.285  loss_mask_6: 1.289  loss_dice_6: 3.913  loss_ce_7: 3.41  loss_mask_7: 1.37  loss_dice_7: 3.902  loss_ce_8: 3.507  loss_mask_8: 1.286  loss_dice_8: 3.882  loss_mars: 0.7716    time: 5.5999  last_time: 6.0004  data_time: 0.0034  last_data_time: 0.0019   lr: 1e-06  max_mem: 0M
[10/19 21:09:05] d2.utils.events INFO:  eta: 1:36:26  iter: 14559  total_loss: 84.79  loss_ce: 2.986  loss_mask: 1.516  loss_dice: 3.723  loss_ce_0: 2.901  loss_mask_0: 1.44  loss_dice_0: 3.887  loss_ce_1: 2.891  loss_mask_1: 1.392  loss_dice_1: 3.962  loss_ce_2: 2.969  loss_mask_2: 1.533  loss_dice_2: 3.926  loss_ce_3: 2.847  loss_mask_3: 1.331  loss_dice_3: 3.901  loss_ce_4: 2.837  loss_mask_4: 1.404  loss_dice_4: 3.785  loss_ce_5: 2.867  loss_mask_5: 1.345  loss_dice_5: 3.831  loss_ce_6: 2.987  loss_mask_6: 1.289  loss_dice_6: 3.726  loss_ce_7: 2.964  loss_mask_7: 1.447  loss_dice_7: 3.769  loss_ce_8: 2.937  loss_mask_8: 1.497  loss_dice_8: 3.799  loss_mars: 0.784    time: 5.5996  last_time: 5.7372  data_time: 0.0029  last_data_time: 0.0079   lr: 1e-06  max_mem: 0M
[10/19 21:10:48] d2.utils.events INFO:  eta: 1:34:12  iter: 14579  total_loss: 86.02  loss_ce: 3  loss_mask: 0.9799  loss_dice: 3.903  loss_ce_0: 3.265  loss_mask_0: 0.7943  loss_dice_0: 4.033  loss_ce_1: 2.968  loss_mask_1: 0.9195  loss_dice_1: 4.01  loss_ce_2: 2.95  loss_mask_2: 1.005  loss_dice_2: 3.95  loss_ce_3: 2.837  loss_mask_3: 0.8019  loss_dice_3: 4.154  loss_ce_4: 2.962  loss_mask_4: 0.9558  loss_dice_4: 3.919  loss_ce_5: 3  loss_mask_5: 0.8368  loss_dice_5: 3.859  loss_ce_6: 2.873  loss_mask_6: 0.7736  loss_dice_6: 3.917  loss_ce_7: 2.943  loss_mask_7: 0.8427  loss_dice_7: 3.897  loss_ce_8: 3.004  loss_mask_8: 0.7817  loss_dice_8: 3.836  loss_mars: 0.8443    time: 5.5989  last_time: 4.9830  data_time: 0.0030  last_data_time: 0.0029   lr: 1e-06  max_mem: 0M
[10/19 21:12:38] d2.utils.events INFO:  eta: 1:32:42  iter: 14599  total_loss: 82.13  loss_ce: 2.774  loss_mask: 1.457  loss_dice: 4.016  loss_ce_0: 2.882  loss_mask_0: 1.465  loss_dice_0: 4.028  loss_ce_1: 2.701  loss_mask_1: 1.402  loss_dice_1: 4.074  loss_ce_2: 2.788  loss_mask_2: 1.471  loss_dice_2: 3.97  loss_ce_3: 2.705  loss_mask_3: 1.357  loss_dice_3: 4.021  loss_ce_4: 2.81  loss_mask_4: 1.494  loss_dice_4: 3.938  loss_ce_5: 2.79  loss_mask_5: 1.522  loss_dice_5: 4.003  loss_ce_6: 2.81  loss_mask_6: 1.493  loss_dice_6: 3.991  loss_ce_7: 2.79  loss_mask_7: 1.516  loss_dice_7: 3.934  loss_ce_8: 2.747  loss_mask_8: 1.497  loss_dice_8: 4.078  loss_mars: 0.8266    time: 5.5988  last_time: 8.0128  data_time: 0.0023  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 21:14:30] d2.utils.events INFO:  eta: 1:31:14  iter: 14619  total_loss: 78.7  loss_ce: 2.629  loss_mask: 1.195  loss_dice: 3.751  loss_ce_0: 2.681  loss_mask_0: 1.194  loss_dice_0: 3.844  loss_ce_1: 2.5  loss_mask_1: 1.126  loss_dice_1: 3.911  loss_ce_2: 2.465  loss_mask_2: 1.143  loss_dice_2: 4.126  loss_ce_3: 2.413  loss_mask_3: 1.162  loss_dice_3: 3.816  loss_ce_4: 2.427  loss_mask_4: 1.149  loss_dice_4: 3.915  loss_ce_5: 2.489  loss_mask_5: 1.359  loss_dice_5: 3.895  loss_ce_6: 2.503  loss_mask_6: 1.357  loss_dice_6: 3.698  loss_ce_7: 2.491  loss_mask_7: 1.284  loss_dice_7: 3.686  loss_ce_8: 2.537  loss_mask_8: 1.352  loss_dice_8: 3.73  loss_mars: 0.8439    time: 5.5988  last_time: 5.0745  data_time: 0.0024  last_data_time: 0.0080   lr: 1e-06  max_mem: 0M
[10/19 21:16:20] d2.utils.events INFO:  eta: 1:29:35  iter: 14639  total_loss: 80.81  loss_ce: 2.533  loss_mask: 1.229  loss_dice: 3.511  loss_ce_0: 2.4  loss_mask_0: 1.117  loss_dice_0: 3.843  loss_ce_1: 2.399  loss_mask_1: 1.237  loss_dice_1: 3.864  loss_ce_2: 2.456  loss_mask_2: 1.412  loss_dice_2: 3.838  loss_ce_3: 2.416  loss_mask_3: 1.308  loss_dice_3: 3.605  loss_ce_4: 2.364  loss_mask_4: 1.277  loss_dice_4: 3.607  loss_ce_5: 2.46  loss_mask_5: 1.139  loss_dice_5: 3.713  loss_ce_6: 2.504  loss_mask_6: 1.115  loss_dice_6: 3.725  loss_ce_7: 2.462  loss_mask_7: 1.333  loss_dice_7: 3.617  loss_ce_8: 2.469  loss_mask_8: 1.322  loss_dice_8: 3.582  loss_mars: 0.7797    time: 5.5986  last_time: 6.3440  data_time: 0.0035  last_data_time: 0.0021   lr: 1e-06  max_mem: 0M
[10/19 21:18:10] d2.utils.events INFO:  eta: 1:28:06  iter: 14659  total_loss: 85.72  loss_ce: 3.131  loss_mask: 1.71  loss_dice: 3.619  loss_ce_0: 3.28  loss_mask_0: 1.435  loss_dice_0: 3.686  loss_ce_1: 3.314  loss_mask_1: 1.546  loss_dice_1: 3.811  loss_ce_2: 3.375  loss_mask_2: 1.426  loss_dice_2: 3.726  loss_ce_3: 3.335  loss_mask_3: 1.442  loss_dice_3: 3.79  loss_ce_4: 3.267  loss_mask_4: 1.631  loss_dice_4: 3.688  loss_ce_5: 3.215  loss_mask_5: 1.485  loss_dice_5: 3.683  loss_ce_6: 3.184  loss_mask_6: 1.528  loss_dice_6: 3.633  loss_ce_7: 3.096  loss_mask_7: 1.704  loss_dice_7: 3.585  loss_ce_8: 3.231  loss_mask_8: 1.625  loss_dice_8: 3.683  loss_mars: 0.8235    time: 5.5984  last_time: 5.7669  data_time: 0.0028  last_data_time: 0.0019   lr: 1e-06  max_mem: 0M
[10/19 21:19:59] d2.utils.events INFO:  eta: 1:25:50  iter: 14679  total_loss: 78.72  loss_ce: 2.655  loss_mask: 0.9803  loss_dice: 3.628  loss_ce_0: 2.813  loss_mask_0: 0.99  loss_dice_0: 3.704  loss_ce_1: 2.475  loss_mask_1: 0.9642  loss_dice_1: 3.624  loss_ce_2: 2.53  loss_mask_2: 1.002  loss_dice_2: 3.575  loss_ce_3: 2.583  loss_mask_3: 0.9404  loss_dice_3: 3.702  loss_ce_4: 2.656  loss_mask_4: 0.9719  loss_dice_4: 3.623  loss_ce_5: 2.661  loss_mask_5: 0.9237  loss_dice_5: 3.667  loss_ce_6: 2.65  loss_mask_6: 0.9907  loss_dice_6: 3.583  loss_ce_7: 2.577  loss_mask_7: 1.059  loss_dice_7: 3.515  loss_ce_8: 2.602  loss_mask_8: 0.9909  loss_dice_8: 3.617  loss_mars: 0.8194    time: 5.5982  last_time: 5.0704  data_time: 0.0023  last_data_time: 0.0036   lr: 1e-06  max_mem: 0M
[10/19 21:21:47] d2.utils.events INFO:  eta: 1:24:01  iter: 14699  total_loss: 79.85  loss_ce: 2.505  loss_mask: 1.211  loss_dice: 3.775  loss_ce_0: 2.422  loss_mask_0: 1.08  loss_dice_0: 3.877  loss_ce_1: 2.352  loss_mask_1: 1.17  loss_dice_1: 3.892  loss_ce_2: 2.342  loss_mask_2: 1.057  loss_dice_2: 3.936  loss_ce_3: 2.348  loss_mask_3: 1.067  loss_dice_3: 3.95  loss_ce_4: 2.311  loss_mask_4: 1.101  loss_dice_4: 3.744  loss_ce_5: 2.467  loss_mask_5: 1.211  loss_dice_5: 3.63  loss_ce_6: 2.442  loss_mask_6: 1.113  loss_dice_6: 3.683  loss_ce_7: 2.387  loss_mask_7: 1.025  loss_dice_7: 3.723  loss_ce_8: 2.505  loss_mask_8: 1.279  loss_dice_8: 3.802  loss_mars: 0.8682    time: 5.5979  last_time: 5.7845  data_time: 0.0034  last_data_time: 0.0078   lr: 1e-06  max_mem: 0M
[10/19 21:23:40] d2.utils.events INFO:  eta: 1:22:14  iter: 14719  total_loss: 83.59  loss_ce: 2.754  loss_mask: 0.6321  loss_dice: 4.338  loss_ce_0: 3.476  loss_mask_0: 0.6213  loss_dice_0: 4.33  loss_ce_1: 2.777  loss_mask_1: 0.6521  loss_dice_1: 4.274  loss_ce_2: 2.742  loss_mask_2: 0.7068  loss_dice_2: 4.349  loss_ce_3: 2.595  loss_mask_3: 0.7312  loss_dice_3: 4.347  loss_ce_4: 2.763  loss_mask_4: 0.6951  loss_dice_4: 4.34  loss_ce_5: 2.708  loss_mask_5: 0.6614  loss_dice_5: 4.331  loss_ce_6: 2.665  loss_mask_6: 0.6979  loss_dice_6: 4.358  loss_ce_7: 2.638  loss_mask_7: 0.7184  loss_dice_7: 4.303  loss_ce_8: 2.727  loss_mask_8: 0.7268  loss_dice_8: 4.295  loss_mars: 0.8647    time: 5.5980  last_time: 4.9676  data_time: 0.0035  last_data_time: 0.0020   lr: 1e-06  max_mem: 0M
[10/19 21:25:28] d2.utils.events INFO:  eta: 1:20:19  iter: 14739  total_loss: 89.03  loss_ce: 3.534  loss_mask: 1.466  loss_dice: 4.115  loss_ce_0: 3.931  loss_mask_0: 1.51  loss_dice_0: 4.082  loss_ce_1: 3.595  loss_mask_1: 1.42  loss_dice_1: 4.149  loss_ce_2: 3.588  loss_mask_2: 1.43  loss_dice_2: 4.13  loss_ce_3: 3.441  loss_mask_3: 1.336  loss_dice_3: 4.13  loss_ce_4: 3.461  loss_mask_4: 1.494  loss_dice_4: 4.171  loss_ce_5: 3.481  loss_mask_5: 1.529  loss_dice_5: 4.116  loss_ce_6: 3.505  loss_mask_6: 1.506  loss_dice_6: 4.152  loss_ce_7: 3.468  loss_mask_7: 1.507  loss_dice_7: 4.142  loss_ce_8: 3.461  loss_mask_8: 1.45  loss_dice_8: 4.148  loss_mars: 0.7815    time: 5.5977  last_time: 7.4442  data_time: 0.0037  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 21:27:15] d2.utils.events INFO:  eta: 1:18:30  iter: 14759  total_loss: 85.39  loss_ce: 2.938  loss_mask: 1.477  loss_dice: 3.719  loss_ce_0: 2.776  loss_mask_0: 1.487  loss_dice_0: 3.957  loss_ce_1: 2.912  loss_mask_1: 1.489  loss_dice_1: 3.917  loss_ce_2: 2.899  loss_mask_2: 1.444  loss_dice_2: 3.924  loss_ce_3: 2.824  loss_mask_3: 1.488  loss_dice_3: 3.841  loss_ce_4: 2.875  loss_mask_4: 1.486  loss_dice_4: 3.738  loss_ce_5: 2.828  loss_mask_5: 1.57  loss_dice_5: 3.861  loss_ce_6: 2.875  loss_mask_6: 1.554  loss_dice_6: 3.662  loss_ce_7: 2.798  loss_mask_7: 1.602  loss_dice_7: 3.743  loss_ce_8: 2.817  loss_mask_8: 1.418  loss_dice_8: 3.737  loss_mars: 0.7534    time: 5.5973  last_time: 3.9825  data_time: 0.0030  last_data_time: 0.0019   lr: 1e-06  max_mem: 0M
[10/19 21:29:07] d2.utils.events INFO:  eta: 1:16:41  iter: 14779  total_loss: 85.38  loss_ce: 2.886  loss_mask: 1.33  loss_dice: 3.753  loss_ce_0: 2.7  loss_mask_0: 1.357  loss_dice_0: 3.954  loss_ce_1: 2.732  loss_mask_1: 1.375  loss_dice_1: 3.931  loss_ce_2: 2.774  loss_mask_2: 1.468  loss_dice_2: 3.929  loss_ce_3: 2.782  loss_mask_3: 1.469  loss_dice_3: 3.828  loss_ce_4: 2.749  loss_mask_4: 1.567  loss_dice_4: 3.831  loss_ce_5: 2.766  loss_mask_5: 1.567  loss_dice_5: 3.84  loss_ce_6: 2.83  loss_mask_6: 1.571  loss_dice_6: 3.644  loss_ce_7: 2.806  loss_mask_7: 1.468  loss_dice_7: 3.755  loss_ce_8: 2.87  loss_mask_8: 1.619  loss_dice_8: 3.779  loss_mars: 0.8475    time: 5.5974  last_time: 5.3959  data_time: 0.0035  last_data_time: 0.0021   lr: 1e-06  max_mem: 0M
[10/19 21:30:58] d2.utils.events INFO:  eta: 1:14:49  iter: 14799  total_loss: 74.87  loss_ce: 2.39  loss_mask: 1.001  loss_dice: 3.8  loss_ce_0: 2.348  loss_mask_0: 1.04  loss_dice_0: 3.769  loss_ce_1: 2.25  loss_mask_1: 0.9648  loss_dice_1: 3.765  loss_ce_2: 2.172  loss_mask_2: 0.9816  loss_dice_2: 3.885  loss_ce_3: 2.063  loss_mask_3: 0.9856  loss_dice_3: 4.024  loss_ce_4: 2.152  loss_mask_4: 0.9083  loss_dice_4: 3.811  loss_ce_5: 2.256  loss_mask_5: 0.9544  loss_dice_5: 3.87  loss_ce_6: 2.124  loss_mask_6: 0.8728  loss_dice_6: 3.875  loss_ce_7: 2.221  loss_mask_7: 0.9711  loss_dice_7: 3.804  loss_ce_8: 2.084  loss_mask_8: 0.9327  loss_dice_8: 3.8  loss_mars: 0.8251    time: 5.5973  last_time: 5.6446  data_time: 0.0029  last_data_time: 0.0022   lr: 1e-06  max_mem: 0M
[10/19 21:32:43] d2.utils.events INFO:  eta: 1:12:48  iter: 14819  total_loss: 81.22  loss_ce: 2.563  loss_mask: 1.809  loss_dice: 3.897  loss_ce_0: 2.525  loss_mask_0: 1.711  loss_dice_0: 3.985  loss_ce_1: 2.51  loss_mask_1: 1.703  loss_dice_1: 3.98  loss_ce_2: 2.467  loss_mask_2: 1.936  loss_dice_2: 3.882  loss_ce_3: 2.392  loss_mask_3: 1.862  loss_dice_3: 3.784  loss_ce_4: 2.328  loss_mask_4: 1.691  loss_dice_4: 3.877  loss_ce_5: 2.425  loss_mask_5: 1.738  loss_dice_5: 3.869  loss_ce_6: 2.431  loss_mask_6: 1.661  loss_dice_6: 3.829  loss_ce_7: 2.423  loss_mask_7: 1.813  loss_dice_7: 3.886  loss_ce_8: 2.466  loss_mask_8: 1.784  loss_dice_8: 3.791  loss_mars: 0.8474    time: 5.5967  last_time: 3.6798  data_time: 0.0026  last_data_time: 0.0024   lr: 1e-06  max_mem: 0M
[10/19 21:34:45] d2.utils.events INFO:  eta: 1:11:00  iter: 14839  total_loss: 82.44  loss_ce: 2.977  loss_mask: 1.054  loss_dice: 4.215  loss_ce_0: 3.464  loss_mask_0: 0.8776  loss_dice_0: 4.322  loss_ce_1: 2.959  loss_mask_1: 0.9454  loss_dice_1: 4.26  loss_ce_2: 2.938  loss_mask_2: 0.9613  loss_dice_2: 4.198  loss_ce_3: 2.92  loss_mask_3: 0.9654  loss_dice_3: 4.165  loss_ce_4: 3.056  loss_mask_4: 0.9368  loss_dice_4: 4.221  loss_ce_5: 3.006  loss_mask_5: 0.9442  loss_dice_5: 4.25  loss_ce_6: 3.026  loss_mask_6: 0.9125  loss_dice_6: 4.214  loss_ce_7: 2.989  loss_mask_7: 0.9591  loss_dice_7: 4.163  loss_ce_8: 3.015  loss_mask_8: 0.9713  loss_dice_8: 4.24  loss_mars: 0.8612    time: 5.5975  last_time: 5.3151  data_time: 0.0029  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 21:36:43] d2.utils.events INFO:  eta: 1:09:27  iter: 14859  total_loss: 82.81  loss_ce: 2.67  loss_mask: 1.022  loss_dice: 3.668  loss_ce_0: 2.612  loss_mask_0: 0.9777  loss_dice_0: 3.721  loss_ce_1: 2.703  loss_mask_1: 0.9171  loss_dice_1: 3.708  loss_ce_2: 2.787  loss_mask_2: 1.019  loss_dice_2: 3.687  loss_ce_3: 2.603  loss_mask_3: 1.048  loss_dice_3: 3.71  loss_ce_4: 2.559  loss_mask_4: 0.9939  loss_dice_4: 3.614  loss_ce_5: 2.571  loss_mask_5: 1.021  loss_dice_5: 3.509  loss_ce_6: 2.661  loss_mask_6: 0.9707  loss_dice_6: 3.614  loss_ce_7: 2.614  loss_mask_7: 1.005  loss_dice_7: 3.611  loss_ce_8: 2.623  loss_mask_8: 1.12  loss_dice_8: 3.501  loss_mars: 0.8388    time: 5.5980  last_time: 5.3772  data_time: 0.0024  last_data_time: 0.0025   lr: 1e-06  max_mem: 0M
[10/19 21:38:34] d2.utils.events INFO:  eta: 1:07:45  iter: 14879  total_loss: 84.04  loss_ce: 2.906  loss_mask: 0.7103  loss_dice: 4.281  loss_ce_0: 4.117  loss_mask_0: 0.7156  loss_dice_0: 4.224  loss_ce_1: 3.102  loss_mask_1: 0.7151  loss_dice_1: 4.248  loss_ce_2: 3.236  loss_mask_2: 0.6896  loss_dice_2: 4.289  loss_ce_3: 3.087  loss_mask_3: 0.7039  loss_dice_3: 4.327  loss_ce_4: 3.324  loss_mask_4: 0.7214  loss_dice_4: 4.24  loss_ce_5: 3.266  loss_mask_5: 0.6892  loss_dice_5: 4.196  loss_ce_6: 3.128  loss_mask_6: 0.7079  loss_dice_6: 4.243  loss_ce_7: 2.86  loss_mask_7: 0.6997  loss_dice_7: 4.222  loss_ce_8: 2.879  loss_mask_8: 0.7132  loss_dice_8: 4.168  loss_mars: 0.8055    time: 5.5979  last_time: 4.9089  data_time: 0.0032  last_data_time: 0.0028   lr: 1e-06  max_mem: 0M
[10/19 21:40:36] d2.utils.events INFO:  eta: 1:06:11  iter: 14899  total_loss: 85.5  loss_ce: 2.761  loss_mask: 1.439  loss_dice: 3.911  loss_ce_0: 3.123  loss_mask_0: 1.467  loss_dice_0: 3.948  loss_ce_1: 2.913  loss_mask_1: 1.407  loss_dice_1: 3.961  loss_ce_2: 2.921  loss_mask_2: 1.394  loss_dice_2: 4.019  loss_ce_3: 2.881  loss_mask_3: 1.586  loss_dice_3: 3.928  loss_ce_4: 2.786  loss_mask_4: 1.591  loss_dice_4: 3.959  loss_ce_5: 2.85  loss_mask_5: 1.409  loss_dice_5: 3.882  loss_ce_6: 2.83  loss_mask_6: 1.598  loss_dice_6: 3.967  loss_ce_7: 2.824  loss_mask_7: 1.609  loss_dice_7: 3.897  loss_ce_8: 2.702  loss_mask_8: 1.452  loss_dice_8: 3.927  loss_mars: 0.8607    time: 5.5986  last_time: 8.5797  data_time: 0.0027  last_data_time: 0.0029   lr: 1e-06  max_mem: 0M
[10/19 21:42:27] d2.utils.events INFO:  eta: 1:04:39  iter: 14919  total_loss: 94.21  loss_ce: 3.858  loss_mask: 1.164  loss_dice: 4.25  loss_ce_0: 4.28  loss_mask_0: 1.193  loss_dice_0: 4.178  loss_ce_1: 3.719  loss_mask_1: 1.102  loss_dice_1: 4.146  loss_ce_2: 3.83  loss_mask_2: 1.144  loss_dice_2: 4.254  loss_ce_3: 3.684  loss_mask_3: 1.131  loss_dice_3: 4.151  loss_ce_4: 3.736  loss_mask_4: 1.116  loss_dice_4: 4.159  loss_ce_5: 3.873  loss_mask_5: 1.149  loss_dice_5: 4.248  loss_ce_6: 3.737  loss_mask_6: 1.146  loss_dice_6: 4.289  loss_ce_7: 3.798  loss_mask_7: 1.131  loss_dice_7: 4.286  loss_ce_8: 3.772  loss_mask_8: 1.185  loss_dice_8: 4.213  loss_mars: 0.8311    time: 5.5986  last_time: 9.1241  data_time: 0.0036  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 21:44:16] d2.utils.events INFO:  eta: 1:02:23  iter: 14939  total_loss: 86.38  loss_ce: 3.591  loss_mask: 0.8906  loss_dice: 4.369  loss_ce_0: 4.04  loss_mask_0: 0.7624  loss_dice_0: 4.349  loss_ce_1: 3.567  loss_mask_1: 0.8445  loss_dice_1: 4.436  loss_ce_2: 3.616  loss_mask_2: 0.7837  loss_dice_2: 4.305  loss_ce_3: 3.41  loss_mask_3: 0.8521  loss_dice_3: 4.406  loss_ce_4: 3.593  loss_mask_4: 0.808  loss_dice_4: 4.447  loss_ce_5: 3.601  loss_mask_5: 0.7716  loss_dice_5: 4.409  loss_ce_6: 3.667  loss_mask_6: 0.8096  loss_dice_6: 4.356  loss_ce_7: 3.633  loss_mask_7: 0.7845  loss_dice_7: 4.325  loss_ce_8: 3.638  loss_mask_8: 0.8298  loss_dice_8: 4.373  loss_mars: 0.849    time: 5.5983  last_time: 5.0410  data_time: 0.0029  last_data_time: 0.0021   lr: 1e-06  max_mem: 0M
[10/19 21:46:06] d2.utils.events INFO:  eta: 1:00:26  iter: 14959  total_loss: 90.16  loss_ce: 4.123  loss_mask: 0.9288  loss_dice: 4.364  loss_ce_0: 4.772  loss_mask_0: 0.9237  loss_dice_0: 4.414  loss_ce_1: 4.097  loss_mask_1: 0.9209  loss_dice_1: 4.349  loss_ce_2: 4.007  loss_mask_2: 0.9439  loss_dice_2: 4.322  loss_ce_3: 3.901  loss_mask_3: 0.7909  loss_dice_3: 4.378  loss_ce_4: 4.081  loss_mask_4: 0.9047  loss_dice_4: 4.376  loss_ce_5: 4.063  loss_mask_5: 0.9091  loss_dice_5: 4.435  loss_ce_6: 4.055  loss_mask_6: 0.8751  loss_dice_6: 4.378  loss_ce_7: 3.989  loss_mask_7: 0.9562  loss_dice_7: 4.364  loss_ce_8: 4.059  loss_mask_8: 0.9134  loss_dice_8: 4.337  loss_mars: 0.8588    time: 5.5981  last_time: 5.5654  data_time: 0.0031  last_data_time: 0.0016   lr: 1e-06  max_mem: 0M
[10/19 21:47:51] d2.utils.events INFO:  eta: 0:58:31  iter: 14979  total_loss: 86.49  loss_ce: 3.034  loss_mask: 1.292  loss_dice: 4.109  loss_ce_0: 3.419  loss_mask_0: 1.232  loss_dice_0: 4.082  loss_ce_1: 3.087  loss_mask_1: 1.26  loss_dice_1: 4.113  loss_ce_2: 3.05  loss_mask_2: 1.104  loss_dice_2: 4.197  loss_ce_3: 3.029  loss_mask_3: 1.069  loss_dice_3: 4.059  loss_ce_4: 3.007  loss_mask_4: 1.209  loss_dice_4: 4.1  loss_ce_5: 3.117  loss_mask_5: 1.16  loss_dice_5: 4.091  loss_ce_6: 3.144  loss_mask_6: 1.067  loss_dice_6: 4.014  loss_ce_7: 3.002  loss_mask_7: 1.273  loss_dice_7: 4.032  loss_ce_8: 3.16  loss_mask_8: 1.176  loss_dice_8: 4.157  loss_mars: 0.807    time: 5.5976  last_time: 5.0391  data_time: 0.0028  last_data_time: 0.0030   lr: 1e-06  max_mem: 0M
[10/19 21:49:42] d2.utils.events INFO:  eta: 0:56:42  iter: 14999  total_loss: 93.94  loss_ce: 3.857  loss_mask: 0.9598  loss_dice: 4.337  loss_ce_0: 4.668  loss_mask_0: 0.8746  loss_dice_0: 4.37  loss_ce_1: 3.908  loss_mask_1: 0.7912  loss_dice_1: 4.223  loss_ce_2: 3.945  loss_mask_2: 0.8597  loss_dice_2: 4.293  loss_ce_3: 3.928  loss_mask_3: 0.8882  loss_dice_3: 4.365  loss_ce_4: 3.946  loss_mask_4: 0.7708  loss_dice_4: 4.211  loss_ce_5: 3.814  loss_mask_5: 0.8479  loss_dice_5: 4.265  loss_ce_6: 3.898  loss_mask_6: 0.8895  loss_dice_6: 4.251  loss_ce_7: 3.73  loss_mask_7: 0.8544  loss_dice_7: 4.266  loss_ce_8: 3.757  loss_mask_8: 0.9743  loss_dice_8: 4.206  loss_mars: 0.8561    time: 5.5976  last_time: 5.7710  data_time: 0.0030  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 21:51:33] d2.utils.events INFO:  eta: 0:55:06  iter: 15019  total_loss: 82.66  loss_ce: 2.691  loss_mask: 1.409  loss_dice: 4.008  loss_ce_0: 2.43  loss_mask_0: 1.385  loss_dice_0: 4.089  loss_ce_1: 2.551  loss_mask_1: 1.321  loss_dice_1: 4.081  loss_ce_2: 2.649  loss_mask_2: 1.388  loss_dice_2: 4.042  loss_ce_3: 2.524  loss_mask_3: 1.572  loss_dice_3: 4.125  loss_ce_4: 2.637  loss_mask_4: 1.46  loss_dice_4: 3.988  loss_ce_5: 2.583  loss_mask_5: 1.552  loss_dice_5: 3.962  loss_ce_6: 2.599  loss_mask_6: 1.486  loss_dice_6: 3.985  loss_ce_7: 2.535  loss_mask_7: 1.493  loss_dice_7: 4.009  loss_ce_8: 2.6  loss_mask_8: 1.535  loss_dice_8: 3.943  loss_mars: 0.8288    time: 5.5975  last_time: 5.6621  data_time: 0.0023  last_data_time: 0.0026   lr: 1e-06  max_mem: 0M
[10/19 21:53:17] d2.utils.events INFO:  eta: 0:53:29  iter: 15039  total_loss: 81.82  loss_ce: 2.7  loss_mask: 1.388  loss_dice: 3.994  loss_ce_0: 2.81  loss_mask_0: 1.332  loss_dice_0: 4.079  loss_ce_1: 2.731  loss_mask_1: 1.351  loss_dice_1: 4.061  loss_ce_2: 2.802  loss_mask_2: 1.406  loss_dice_2: 3.913  loss_ce_3: 2.772  loss_mask_3: 1.335  loss_dice_3: 4.027  loss_ce_4: 2.749  loss_mask_4: 1.339  loss_dice_4: 4.031  loss_ce_5: 2.739  loss_mask_5: 1.488  loss_dice_5: 3.868  loss_ce_6: 2.711  loss_mask_6: 1.445  loss_dice_6: 3.895  loss_ce_7: 2.594  loss_mask_7: 1.234  loss_dice_7: 4.006  loss_ce_8: 2.636  loss_mask_8: 1.385  loss_dice_8: 3.963  loss_mars: 0.8785    time: 5.5969  last_time: 4.7525  data_time: 0.0037  last_data_time: 0.0017   lr: 1e-06  max_mem: 0M
[10/19 21:55:04] d2.utils.events INFO:  eta: 0:51:48  iter: 15059  total_loss: 87.92  loss_ce: 3.775  loss_mask: 1.027  loss_dice: 4.002  loss_ce_0: 4.369  loss_mask_0: 0.9614  loss_dice_0: 4.146  loss_ce_1: 3.771  loss_mask_1: 0.9361  loss_dice_1: 4.164  loss_ce_2: 3.792  loss_mask_2: 0.9308  loss_dice_2: 4.196  loss_ce_3: 3.685  loss_mask_3: 0.8439  loss_dice_3: 4.234  loss_ce_4: 3.661  loss_mask_4: 0.9778  loss_dice_4: 4.069  loss_ce_5: 3.797  loss_mask_5: 0.928  loss_dice_5: 4.092  loss_ce_6: 3.742  loss_mask_6: 0.9408  loss_dice_6: 4.217  loss_ce_7: 3.886  loss_mask_7: 1.005  loss_dice_7: 4.095  loss_ce_8: 3.894  loss_mask_8: 0.9589  loss_dice_8: 4.141  loss_mars: 0.803    time: 5.5965  last_time: 5.6124  data_time: 0.0029  last_data_time: 0.0023   lr: 1e-06  max_mem: 0M
[10/19 21:56:47] d2.utils.events INFO:  eta: 0:49:31  iter: 15079  total_loss: 85.54  loss_ce: 3.359  loss_mask: 1.112  loss_dice: 3.991  loss_ce_0: 3.916  loss_mask_0: 1.103  loss_dice_0: 4.081  loss_ce_1: 3.243  loss_mask_1: 1.048  loss_dice_1: 4.01  loss_ce_2: 3.561  loss_mask_2: 1.109  loss_dice_2: 3.906  loss_ce_3: 3.448  loss_mask_3: 1.26  loss_dice_3: 4.148  loss_ce_4: 3.521  loss_mask_4: 1.189  loss_dice_4: 4.021  loss_ce_5: 3.435  loss_mask_5: 1.142  loss_dice_5: 3.927  loss_ce_6: 3.424  loss_mask_6: 1.187  loss_dice_6: 3.934  loss_ce_7: 3.389  loss_mask_7: 1.242  loss_dice_7: 3.892  loss_ce_8: 3.301  loss_mask_8: 1.206  loss_dice_8: 4.077  loss_mars: 0.7095    time: 5.5959  last_time: 5.0073  data_time: 0.0029  last_data_time: 0.0029   lr: 1e-06  max_mem: 0M
[10/19 21:58:35] d2.utils.events INFO:  eta: 0:47:48  iter: 15099  total_loss: 85.48  loss_ce: 3.111  loss_mask: 1.096  loss_dice: 4.079  loss_ce_0: 3.105  loss_mask_0: 1.096  loss_dice_0: 4.04  loss_ce_1: 3.166  loss_mask_1: 1.053  loss_dice_1: 4.002  loss_ce_2: 3.096  loss_mask_2: 1.205  loss_dice_2: 4.133  loss_ce_3: 2.974  loss_mask_3: 1.162  loss_dice_3: 3.943  loss_ce_4: 3.027  loss_mask_4: 1.169  loss_dice_4: 4.015  loss_ce_5: 3.145  loss_mask_5: 1.133  loss_dice_5: 3.889  loss_ce_6: 3.073  loss_mask_6: 1.152  loss_dice_6: 3.996  loss_ce_7: 3.188  loss_mask_7: 1.192  loss_dice_7: 4.074  loss_ce_8: 3.179  loss_mask_8: 1.159  loss_dice_8: 3.895  loss_mars: 0.8525    time: 5.5956  last_time: 6.5253  data_time: 0.0031  last_data_time: 0.0019   lr: 1e-06  max_mem: 0M
[10/19 22:00:24] d2.utils.events INFO:  eta: 0:46:05  iter: 15119  total_loss: 92.19  loss_ce: 3.393  loss_mask: 0.9374  loss_dice: 4.138  loss_ce_0: 4.22  loss_mask_0: 0.9699  loss_dice_0: 4.249  loss_ce_1: 3.671  loss_mask_1: 0.9392  loss_dice_1: 4.027  loss_ce_2: 3.661  loss_mask_2: 0.9712  loss_dice_2: 4.313  loss_ce_3: 3.589  loss_mask_3: 0.9774  loss_dice_3: 4.204  loss_ce_4: 3.733  loss_mask_4: 0.9578  loss_dice_4: 4.158  loss_ce_5: 3.426  loss_mask_5: 0.9815  loss_dice_5: 4.186  loss_ce_6: 3.438  loss_mask_6: 0.9097  loss_dice_6: 4.232  loss_ce_7: 3.363  loss_mask_7: 0.9814  loss_dice_7: 4.142  loss_ce_8: 3.429  loss_mask_8: 0.9463  loss_dice_8: 4.199  loss_mars: 0.7822    time: 5.5953  last_time: 5.7375  data_time: 0.0026  last_data_time: 0.0022   lr: 1e-06  max_mem: 0M
[10/19 22:02:15] d2.utils.events INFO:  eta: 0:44:13  iter: 15139  total_loss: 82.34  loss_ce: 2.759  loss_mask: 1.226  loss_dice: 3.556  loss_ce_0: 2.778  loss_mask_0: 1.121  loss_dice_0: 3.799  loss_ce_1: 2.689  loss_mask_1: 1.087  loss_dice_1: 3.883  loss_ce_2: 2.703  loss_mask_2: 1.21  loss_dice_2: 3.823  loss_ce_3: 2.545  loss_mask_3: 1.15  loss_dice_3: 3.621  loss_ce_4: 2.567  loss_mask_4: 1.288  loss_dice_4: 3.65  loss_ce_5: 2.548  loss_mask_5: 1.277  loss_dice_5: 3.568  loss_ce_6: 2.599  loss_mask_6: 1.107  loss_dice_6: 3.554  loss_ce_7: 2.604  loss_mask_7: 1.219  loss_dice_7: 3.61  loss_ce_8: 2.667  loss_mask_8: 1.269  loss_dice_8: 3.548  loss_mars: 0.7768    time: 5.5953  last_time: 5.4559  data_time: 0.0022  last_data_time: 0.0026   lr: 1e-06  max_mem: 0M
[10/19 22:04:04] d2.utils.events INFO:  eta: 0:42:23  iter: 15159  total_loss: 84.52  loss_ce: 2.754  loss_mask: 1.454  loss_dice: 3.674  loss_ce_0: 2.966  loss_mask_0: 1.148  loss_dice_0: 3.852  loss_ce_1: 2.673  loss_mask_1: 1.085  loss_dice_1: 3.85  loss_ce_2: 2.646  loss_mask_2: 1.122  loss_dice_2: 3.884  loss_ce_3: 2.44  loss_mask_3: 1.257  loss_dice_3: 4.013  loss_ce_4: 2.524  loss_mask_4: 1.384  loss_dice_4: 3.765  loss_ce_5: 2.608  loss_mask_5: 1.275  loss_dice_5: 3.64  loss_ce_6: 2.608  loss_mask_6: 1.334  loss_dice_6: 3.716  loss_ce_7: 2.649  loss_mask_7: 1.279  loss_dice_7: 3.593  loss_ce_8: 2.637  loss_mask_8: 1.19  loss_dice_8: 3.619  loss_mars: 0.8391    time: 5.5951  last_time: 5.1023  data_time: 0.0033  last_data_time: 0.0096   lr: 1e-06  max_mem: 0M
[10/19 22:06:01] d2.utils.events INFO:  eta: 0:40:47  iter: 15179  total_loss: 92.19  loss_ce: 3.958  loss_mask: 0.9006  loss_dice: 4.352  loss_ce_0: 5.245  loss_mask_0: 0.7493  loss_dice_0: 4.292  loss_ce_1: 4.124  loss_mask_1: 0.7541  loss_dice_1: 4.39  loss_ce_2: 4.045  loss_mask_2: 0.76  loss_dice_2: 4.294  loss_ce_3: 4.028  loss_mask_3: 0.7708  loss_dice_3: 4.447  loss_ce_4: 4.211  loss_mask_4: 0.8319  loss_dice_4: 4.363  loss_ce_5: 4.097  loss_mask_5: 0.7769  loss_dice_5: 4.437  loss_ce_6: 3.966  loss_mask_6: 0.7809  loss_dice_6: 4.301  loss_ce_7: 3.859  loss_mask_7: 0.8162  loss_dice_7: 4.381  loss_ce_8: 3.924  loss_mask_8: 0.8485  loss_dice_8: 4.371  loss_mars: 0.91    time: 5.5955  last_time: 5.0399  data_time: 0.0035  last_data_time: 0.0030   lr: 1e-06  max_mem: 0M
[10/19 22:07:50] d2.utils.events INFO:  eta: 0:39:13  iter: 15199  total_loss: 91.81  loss_ce: 3.74  loss_mask: 0.9413  loss_dice: 4.377  loss_ce_0: 3.763  loss_mask_0: 0.859  loss_dice_0: 4.459  loss_ce_1: 3.537  loss_mask_1: 0.8193  loss_dice_1: 4.297  loss_ce_2: 3.675  loss_mask_2: 0.8198  loss_dice_2: 4.338  loss_ce_3: 3.737  loss_mask_3: 0.7947  loss_dice_3: 4.404  loss_ce_4: 3.763  loss_mask_4: 0.8954  loss_dice_4: 4.305  loss_ce_5: 3.727  loss_mask_5: 0.8625  loss_dice_5: 4.284  loss_ce_6: 3.839  loss_mask_6: 0.8886  loss_dice_6: 4.313  loss_ce_7: 3.763  loss_mask_7: 0.9623  loss_dice_7: 4.357  loss_ce_8: 3.781  loss_mask_8: 0.9002  loss_dice_8: 4.388  loss_mars: 0.8002    time: 5.5953  last_time: 5.8616  data_time: 0.0030  last_data_time: 0.0021   lr: 1e-06  max_mem: 0M
[10/19 22:09:37] d2.utils.events INFO:  eta: 0:37:22  iter: 15219  total_loss: 75.91  loss_ce: 2.138  loss_mask: 1.262  loss_dice: 3.509  loss_ce_0: 2.353  loss_mask_0: 0.9957  loss_dice_0: 3.677  loss_ce_1: 2.102  loss_mask_1: 0.9949  loss_dice_1: 3.598  loss_ce_2: 2.141  loss_mask_2: 1.036  loss_dice_2: 3.564  loss_ce_3: 2.072  loss_mask_3: 1.141  loss_dice_3: 3.605  loss_ce_4: 2.063  loss_mask_4: 1.363  loss_dice_4: 3.587  loss_ce_5: 2.153  loss_mask_5: 0.96  loss_dice_5: 3.41  loss_ce_6: 2.19  loss_mask_6: 0.9925  loss_dice_6: 3.335  loss_ce_7: 2.082  loss_mask_7: 0.9819  loss_dice_7: 3.435  loss_ce_8: 2.098  loss_mask_8: 1.165  loss_dice_8: 3.485  loss_mars: 0.8553    time: 5.5949  last_time: 7.2926  data_time: 0.0028  last_data_time: 0.0024   lr: 1e-06  max_mem: 0M
[10/19 22:11:27] d2.utils.events INFO:  eta: 0:35:39  iter: 15239  total_loss: 81.71  loss_ce: 2.865  loss_mask: 1.068  loss_dice: 4.005  loss_ce_0: 3.277  loss_mask_0: 1.122  loss_dice_0: 3.971  loss_ce_1: 2.791  loss_mask_1: 1.039  loss_dice_1: 4.072  loss_ce_2: 2.815  loss_mask_2: 1.095  loss_dice_2: 4.033  loss_ce_3: 2.601  loss_mask_3: 1.018  loss_dice_3: 4.09  loss_ce_4: 2.626  loss_mask_4: 1.11  loss_dice_4: 3.837  loss_ce_5: 2.683  loss_mask_5: 1.073  loss_dice_5: 4.14  loss_ce_6: 2.794  loss_mask_6: 1.011  loss_dice_6: 3.843  loss_ce_7: 2.677  loss_mask_7: 1.078  loss_dice_7: 4.027  loss_ce_8: 2.838  loss_mask_8: 1.099  loss_dice_8: 3.967  loss_mars: 0.8506    time: 5.5947  last_time: 5.5727  data_time: 0.0035  last_data_time: 0.0027   lr: 1e-06  max_mem: 0M
[10/19 22:13:14] d2.utils.events INFO:  eta: 0:33:43  iter: 15259  total_loss: 90.11  loss_ce: 3.39  loss_mask: 1.224  loss_dice: 4.001  loss_ce_0: 3.653  loss_mask_0: 1.235  loss_dice_0: 4.167  loss_ce_1: 3.306  loss_mask_1: 1.173  loss_dice_1: 4.064  loss_ce_2: 3.252  loss_mask_2: 1.159  loss_dice_2: 4.072  loss_ce_3: 3.288  loss_mask_3: 1.216  loss_dice_3: 4.051  loss_ce_4: 3.365  loss_mask_4: 1.297  loss_dice_4: 4.049  loss_ce_5: 3.409  loss_mask_5: 1.246  loss_dice_5: 4.055  loss_ce_6: 3.374  loss_mask_6: 1.175  loss_dice_6: 4.045  loss_ce_7: 3.358  loss_mask_7: 1.168  loss_dice_7: 4.09  loss_ce_8: 3.322  loss_mask_8: 1.209  loss_dice_8: 4.016  loss_mars: 0.8269    time: 5.5944  last_time: 4.8639  data_time: 0.0024  last_data_time: 0.0021   lr: 1e-06  max_mem: 0M
[10/19 22:15:05] d2.utils.events INFO:  eta: 0:31:57  iter: 15279  total_loss: 91.3  loss_ce: 3.527  loss_mask: 1.123  loss_dice: 4.171  loss_ce_0: 3.809  loss_mask_0: 1.121  loss_dice_0: 4.2  loss_ce_1: 3.417  loss_mask_1: 1.056  loss_dice_1: 4.251  loss_ce_2: 3.61  loss_mask_2: 1.143  loss_dice_2: 4.27  loss_ce_3: 3.237  loss_mask_3: 1.217  loss_dice_3: 4.193  loss_ce_4: 3.323  loss_mask_4: 1.108  loss_dice_4: 4.192  loss_ce_5: 3.488  loss_mask_5: 1.18  loss_dice_5: 4.057  loss_ce_6: 3.555  loss_mask_6: 1.235  loss_dice_6: 4.213  loss_ce_7: 3.568  loss_mask_7: 1.103  loss_dice_7: 4.18  loss_ce_8: 3.523  loss_mask_8: 1.198  loss_dice_8: 4.206  loss_mars: 0.848    time: 5.5943  last_time: 5.2237  data_time: 0.0027  last_data_time: 0.0028   lr: 1e-06  max_mem: 0M
[10/19 22:16:55] d2.utils.events INFO:  eta: 0:30:05  iter: 15299  total_loss: 86.55  loss_ce: 2.936  loss_mask: 1.106  loss_dice: 3.828  loss_ce_0: 2.636  loss_mask_0: 1.126  loss_dice_0: 4.036  loss_ce_1: 2.942  loss_mask_1: 1.044  loss_dice_1: 3.986  loss_ce_2: 2.878  loss_mask_2: 1.16  loss_dice_2: 4.116  loss_ce_3: 2.892  loss_mask_3: 1.078  loss_dice_3: 4.051  loss_ce_4: 2.79  loss_mask_4: 1.134  loss_dice_4: 4.008  loss_ce_5: 2.861  loss_mask_5: 1.061  loss_dice_5: 4.004  loss_ce_6: 2.851  loss_mask_6: 1.052  loss_dice_6: 3.987  loss_ce_7: 2.749  loss_mask_7: 1.104  loss_dice_7: 3.962  loss_ce_8: 2.776  loss_mask_8: 1.111  loss_dice_8: 3.876  loss_mars: 0.8502    time: 5.5942  last_time: 4.9372  data_time: 0.0024  last_data_time: 0.0024   lr: 1e-06  max_mem: 0M
[10/19 22:18:42] d2.utils.events INFO:  eta: 0:28:09  iter: 15319  total_loss: 94.22  loss_ce: 3.998  loss_mask: 1.107  loss_dice: 4.165  loss_ce_0: 4.409  loss_mask_0: 1.031  loss_dice_0: 4.286  loss_ce_1: 3.988  loss_mask_1: 1.124  loss_dice_1: 4.284  loss_ce_2: 3.99  loss_mask_2: 1.078  loss_dice_2: 4.278  loss_ce_3: 3.962  loss_mask_3: 1.053  loss_dice_3: 4.177  loss_ce_4: 3.98  loss_mask_4: 1.087  loss_dice_4: 4.174  loss_ce_5: 3.963  loss_mask_5: 1.11  loss_dice_5: 4.083  loss_ce_6: 3.96  loss_mask_6: 1.121  loss_dice_6: 4.178  loss_ce_7: 3.823  loss_mask_7: 1.086  loss_dice_7: 4.181  loss_ce_8: 3.884  loss_mask_8: 1.082  loss_dice_8: 4.156  loss_mars: 0.7202    time: 5.5938  last_time: 5.1526  data_time: 0.0039  last_data_time: 0.0017   lr: 1e-06  max_mem: 0M
[10/19 22:20:36] d2.utils.events INFO:  eta: 0:26:22  iter: 15339  total_loss: 83.44  loss_ce: 3.081  loss_mask: 1.543  loss_dice: 3.791  loss_ce_0: 3.089  loss_mask_0: 1.556  loss_dice_0: 4.002  loss_ce_1: 2.916  loss_mask_1: 1.557  loss_dice_1: 3.72  loss_ce_2: 2.971  loss_mask_2: 1.457  loss_dice_2: 3.79  loss_ce_3: 2.912  loss_mask_3: 1.483  loss_dice_3: 3.935  loss_ce_4: 2.949  loss_mask_4: 1.535  loss_dice_4: 3.866  loss_ce_5: 3.018  loss_mask_5: 1.453  loss_dice_5: 3.833  loss_ce_6: 3.067  loss_mask_6: 1.531  loss_dice_6: 3.78  loss_ce_7: 3.054  loss_mask_7: 1.492  loss_dice_7: 3.8  loss_ce_8: 3.033  loss_mask_8: 1.628  loss_dice_8: 3.762  loss_mars: 0.87    time: 5.5940  last_time: 5.8442  data_time: 0.0036  last_data_time: 0.0026   lr: 1e-06  max_mem: 0M
[10/19 22:22:24] d2.utils.events INFO:  eta: 0:24:26  iter: 15359  total_loss: 87.96  loss_ce: 3.091  loss_mask: 1.685  loss_dice: 3.569  loss_ce_0: 2.999  loss_mask_0: 1.861  loss_dice_0: 3.801  loss_ce_1: 3.126  loss_mask_1: 1.637  loss_dice_1: 3.762  loss_ce_2: 3.092  loss_mask_2: 1.667  loss_dice_2: 3.527  loss_ce_3: 2.997  loss_mask_3: 1.754  loss_dice_3: 3.763  loss_ce_4: 2.948  loss_mask_4: 1.786  loss_dice_4: 3.659  loss_ce_5: 2.995  loss_mask_5: 1.762  loss_dice_5: 3.691  loss_ce_6: 3.078  loss_mask_6: 1.653  loss_dice_6: 3.663  loss_ce_7: 3.049  loss_mask_7: 1.778  loss_dice_7: 3.69  loss_ce_8: 2.996  loss_mask_8: 1.648  loss_dice_8: 3.636  loss_mars: 0.8333    time: 5.5937  last_time: 4.9986  data_time: 0.0036  last_data_time: 0.0017   lr: 1e-06  max_mem: 0M
[10/19 22:24:14] d2.utils.events INFO:  eta: 0:22:28  iter: 15379  total_loss: 81.57  loss_ce: 3.122  loss_mask: 0.8156  loss_dice: 3.853  loss_ce_0: 3.292  loss_mask_0: 0.9077  loss_dice_0: 3.725  loss_ce_1: 3.057  loss_mask_1: 0.8189  loss_dice_1: 3.787  loss_ce_2: 3  loss_mask_2: 0.7946  loss_dice_2: 3.692  loss_ce_3: 3.068  loss_mask_3: 0.8228  loss_dice_3: 3.86  loss_ce_4: 3.128  loss_mask_4: 0.7697  loss_dice_4: 3.776  loss_ce_5: 3.062  loss_mask_5: 0.7803  loss_dice_5: 3.677  loss_ce_6: 3.124  loss_mask_6: 0.824  loss_dice_6: 3.621  loss_ce_7: 3.088  loss_mask_7: 0.8922  loss_dice_7: 3.805  loss_ce_8: 3.135  loss_mask_8: 0.8694  loss_dice_8: 3.591  loss_mars: 0.8565    time: 5.5936  last_time: 6.1649  data_time: 0.0027  last_data_time: 0.0019   lr: 1e-06  max_mem: 0M
[10/19 22:26:07] d2.utils.events INFO:  eta: 0:20:38  iter: 15399  total_loss: 83.01  loss_ce: 3.271  loss_mask: 1.252  loss_dice: 3.828  loss_ce_0: 3.312  loss_mask_0: 1.253  loss_dice_0: 3.889  loss_ce_1: 3.423  loss_mask_1: 1.133  loss_dice_1: 4.041  loss_ce_2: 3.482  loss_mask_2: 1.249  loss_dice_2: 4.053  loss_ce_3: 3.459  loss_mask_3: 1.116  loss_dice_3: 3.947  loss_ce_4: 3.389  loss_mask_4: 1.267  loss_dice_4: 3.845  loss_ce_5: 3.315  loss_mask_5: 1.157  loss_dice_5: 3.909  loss_ce_6: 3.448  loss_mask_6: 1.194  loss_dice_6: 3.82  loss_ce_7: 3.305  loss_mask_7: 1.141  loss_dice_7: 3.82  loss_ce_8: 3.225  loss_mask_8: 1.274  loss_dice_8: 3.944  loss_mars: 0.8887    time: 5.5937  last_time: 5.1840  data_time: 0.0026  last_data_time: 0.0030   lr: 1e-06  max_mem: 0M
[10/19 22:27:53] d2.utils.events INFO:  eta: 0:18:45  iter: 15419  total_loss: 83.71  loss_ce: 2.756  loss_mask: 1.568  loss_dice: 3.989  loss_ce_0: 2.819  loss_mask_0: 1.473  loss_dice_0: 4.025  loss_ce_1: 2.569  loss_mask_1: 1.477  loss_dice_1: 3.875  loss_ce_2: 2.706  loss_mask_2: 1.521  loss_dice_2: 3.819  loss_ce_3: 2.64  loss_mask_3: 1.451  loss_dice_3: 3.975  loss_ce_4: 2.822  loss_mask_4: 1.441  loss_dice_4: 4.074  loss_ce_5: 2.722  loss_mask_5: 1.572  loss_dice_5: 3.886  loss_ce_6: 2.78  loss_mask_6: 1.5  loss_dice_6: 3.849  loss_ce_7: 2.782  loss_mask_7: 1.545  loss_dice_7: 4.033  loss_ce_8: 2.679  loss_mask_8: 1.48  loss_dice_8: 3.875  loss_mars: 0.8211    time: 5.5932  last_time: 5.7636  data_time: 0.0035  last_data_time: 0.0029   lr: 1e-06  max_mem: 0M
[10/19 22:29:40] d2.utils.events INFO:  eta: 0:16:50  iter: 15439  total_loss: 89.13  loss_ce: 3.231  loss_mask: 1.485  loss_dice: 3.774  loss_ce_0: 3.98  loss_mask_0: 1.376  loss_dice_0: 3.906  loss_ce_1: 3.393  loss_mask_1: 1.365  loss_dice_1: 3.901  loss_ce_2: 3.342  loss_mask_2: 1.381  loss_dice_2: 3.94  loss_ce_3: 3.382  loss_mask_3: 1.408  loss_dice_3: 3.85  loss_ce_4: 3.485  loss_mask_4: 1.425  loss_dice_4: 3.842  loss_ce_5: 3.379  loss_mask_5: 1.569  loss_dice_5: 3.804  loss_ce_6: 3.38  loss_mask_6: 1.45  loss_dice_6: 3.827  loss_ce_7: 3.282  loss_mask_7: 1.625  loss_dice_7: 3.875  loss_ce_8: 3.227  loss_mask_8: 1.438  loss_dice_8: 3.882  loss_mars: 0.8899    time: 5.5928  last_time: 7.5049  data_time: 0.0029  last_data_time: 0.0023   lr: 1e-06  max_mem: 0M
[10/19 22:31:27] d2.utils.events INFO:  eta: 0:15:03  iter: 15459  total_loss: 85.5  loss_ce: 2.565  loss_mask: 1.002  loss_dice: 4.003  loss_ce_0: 2.511  loss_mask_0: 0.8873  loss_dice_0: 3.928  loss_ce_1: 2.499  loss_mask_1: 1.034  loss_dice_1: 3.844  loss_ce_2: 2.493  loss_mask_2: 1.02  loss_dice_2: 3.905  loss_ce_3: 2.41  loss_mask_3: 0.9362  loss_dice_3: 3.95  loss_ce_4: 2.409  loss_mask_4: 0.977  loss_dice_4: 3.895  loss_ce_5: 2.425  loss_mask_5: 0.9804  loss_dice_5: 3.875  loss_ce_6: 2.503  loss_mask_6: 0.959  loss_dice_6: 3.852  loss_ce_7: 2.472  loss_mask_7: 1.057  loss_dice_7: 3.886  loss_ce_8: 2.495  loss_mask_8: 1.049  loss_dice_8: 3.872  loss_mars: 0.8515    time: 5.5925  last_time: 5.6609  data_time: 0.0039  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 22:33:15] d2.utils.events INFO:  eta: 0:13:11  iter: 15479  total_loss: 89.96  loss_ce: 3.501  loss_mask: 0.9476  loss_dice: 4.245  loss_ce_0: 3.743  loss_mask_0: 0.8736  loss_dice_0: 4.281  loss_ce_1: 3.423  loss_mask_1: 0.8766  loss_dice_1: 4.191  loss_ce_2: 3.388  loss_mask_2: 0.8565  loss_dice_2: 4.235  loss_ce_3: 3.446  loss_mask_3: 0.9024  loss_dice_3: 4.237  loss_ce_4: 3.422  loss_mask_4: 0.9693  loss_dice_4: 4.215  loss_ce_5: 3.516  loss_mask_5: 0.8994  loss_dice_5: 4.217  loss_ce_6: 3.511  loss_mask_6: 0.87  loss_dice_6: 4.186  loss_ce_7: 3.393  loss_mask_7: 0.9721  loss_dice_7: 4.211  loss_ce_8: 3.42  loss_mask_8: 0.9015  loss_dice_8: 4.239  loss_mars: 0.8631    time: 5.5922  last_time: 5.9809  data_time: 0.0028  last_data_time: 0.0021   lr: 1e-06  max_mem: 0M
[10/19 22:35:04] d2.utils.events INFO:  eta: 0:11:21  iter: 15499  total_loss: 93.44  loss_ce: 3.635  loss_mask: 1.319  loss_dice: 4.023  loss_ce_0: 3.735  loss_mask_0: 1.361  loss_dice_0: 4.168  loss_ce_1: 3.392  loss_mask_1: 1.351  loss_dice_1: 4.036  loss_ce_2: 3.412  loss_mask_2: 1.333  loss_dice_2: 4.084  loss_ce_3: 3.386  loss_mask_3: 1.28  loss_dice_3: 4.169  loss_ce_4: 3.366  loss_mask_4: 1.331  loss_dice_4: 4.098  loss_ce_5: 3.415  loss_mask_5: 1.293  loss_dice_5: 4.092  loss_ce_6: 3.556  loss_mask_6: 1.399  loss_dice_6: 4.15  loss_ce_7: 3.596  loss_mask_7: 1.322  loss_dice_7: 4.066  loss_ce_8: 3.6  loss_mask_8: 1.339  loss_dice_8: 3.986  loss_mars: 0.8088    time: 5.5920  last_time: 5.7933  data_time: 0.0025  last_data_time: 0.0023   lr: 1e-06  max_mem: 0M
[10/19 22:36:53] d2.utils.events INFO:  eta: 0:09:28  iter: 15519  total_loss: 83.6  loss_ce: 3.032  loss_mask: 1.133  loss_dice: 4.1  loss_ce_0: 3.718  loss_mask_0: 1.061  loss_dice_0: 4.294  loss_ce_1: 2.958  loss_mask_1: 1.051  loss_dice_1: 4.185  loss_ce_2: 2.915  loss_mask_2: 1.008  loss_dice_2: 4.171  loss_ce_3: 2.853  loss_mask_3: 1.066  loss_dice_3: 4.2  loss_ce_4: 2.871  loss_mask_4: 1.07  loss_dice_4: 4.083  loss_ce_5: 2.983  loss_mask_5: 1.069  loss_dice_5: 4.084  loss_ce_6: 2.944  loss_mask_6: 1.083  loss_dice_6: 4.042  loss_ce_7: 2.934  loss_mask_7: 1.035  loss_dice_7: 4.098  loss_ce_8: 3.008  loss_mask_8: 1.06  loss_dice_8: 4.113  loss_mars: 0.8871    time: 5.5918  last_time: 5.0706  data_time: 0.0030  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 22:38:43] d2.utils.events INFO:  eta: 0:07:40  iter: 15539  total_loss: 87.18  loss_ce: 2.945  loss_mask: 1.25  loss_dice: 3.875  loss_ce_0: 2.982  loss_mask_0: 1.179  loss_dice_0: 4.001  loss_ce_1: 2.949  loss_mask_1: 1.236  loss_dice_1: 3.965  loss_ce_2: 2.975  loss_mask_2: 1.219  loss_dice_2: 3.982  loss_ce_3: 2.861  loss_mask_3: 1.174  loss_dice_3: 3.934  loss_ce_4: 2.914  loss_mask_4: 1.24  loss_dice_4: 3.884  loss_ce_5: 2.835  loss_mask_5: 1.193  loss_dice_5: 3.871  loss_ce_6: 2.917  loss_mask_6: 1.173  loss_dice_6: 3.926  loss_ce_7: 2.904  loss_mask_7: 1.164  loss_dice_7: 3.872  loss_ce_8: 2.878  loss_mask_8: 1.248  loss_dice_8: 3.84  loss_mars: 0.8464    time: 5.5917  last_time: 3.8993  data_time: 0.0031  last_data_time: 0.0019   lr: 1e-06  max_mem: 0M
[10/19 22:40:34] d2.utils.events INFO:  eta: 0:05:52  iter: 15559  total_loss: 90.69  loss_ce: 3.674  loss_mask: 1.184  loss_dice: 4.181  loss_ce_0: 4.164  loss_mask_0: 1.138  loss_dice_0: 4.241  loss_ce_1: 3.694  loss_mask_1: 1.101  loss_dice_1: 4.112  loss_ce_2: 3.724  loss_mask_2: 1.167  loss_dice_2: 4.139  loss_ce_3: 3.595  loss_mask_3: 1.156  loss_dice_3: 4.25  loss_ce_4: 3.658  loss_mask_4: 1.124  loss_dice_4: 4.216  loss_ce_5: 3.515  loss_mask_5: 1.189  loss_dice_5: 4.125  loss_ce_6: 3.594  loss_mask_6: 1.162  loss_dice_6: 4.176  loss_ce_7: 3.59  loss_mask_7: 1.241  loss_dice_7: 4.112  loss_ce_8: 3.591  loss_mask_8: 1.164  loss_dice_8: 4.159  loss_mars: 0.8294    time: 5.5917  last_time: 5.2346  data_time: 0.0030  last_data_time: 0.0018   lr: 1e-06  max_mem: 0M
[10/19 22:42:26] d2.utils.events INFO:  eta: 0:04:04  iter: 15579  total_loss: 83.17  loss_ce: 3.244  loss_mask: 1.017  loss_dice: 4.044  loss_ce_0: 3.334  loss_mask_0: 1.024  loss_dice_0: 4.147  loss_ce_1: 3.091  loss_mask_1: 1.068  loss_dice_1: 4.161  loss_ce_2: 3.109  loss_mask_2: 1.002  loss_dice_2: 4.094  loss_ce_3: 3.015  loss_mask_3: 1.006  loss_dice_3: 4.206  loss_ce_4: 2.927  loss_mask_4: 0.9847  loss_dice_4: 4.137  loss_ce_5: 3.057  loss_mask_5: 1.047  loss_dice_5: 4.059  loss_ce_6: 3.063  loss_mask_6: 1.057  loss_dice_6: 4.138  loss_ce_7: 3.116  loss_mask_7: 1.016  loss_dice_7: 4.065  loss_ce_8: 3.126  loss_mask_8: 1.015  loss_dice_8: 4.063  loss_mars: 0.7365    time: 5.5916  last_time: 6.5376  data_time: 0.0030  last_data_time: 0.0029   lr: 1e-06  max_mem: 0M
[10/19 22:44:13] d2.utils.events INFO:  eta: 0:02:14  iter: 15599  total_loss: 82.74  loss_ce: 3.139  loss_mask: 1.208  loss_dice: 4.129  loss_ce_0: 3.086  loss_mask_0: 1.289  loss_dice_0: 4.192  loss_ce_1: 2.848  loss_mask_1: 1.194  loss_dice_1: 4.265  loss_ce_2: 2.945  loss_mask_2: 1.206  loss_dice_2: 4.166  loss_ce_3: 2.778  loss_mask_3: 1.273  loss_dice_3: 4.251  loss_ce_4: 2.782  loss_mask_4: 1.31  loss_dice_4: 4.153  loss_ce_5: 2.772  loss_mask_5: 1.292  loss_dice_5: 4.118  loss_ce_6: 2.883  loss_mask_6: 1.298  loss_dice_6: 4.085  loss_ce_7: 2.849  loss_mask_7: 1.346  loss_dice_7: 4.2  loss_ce_8: 2.854  loss_mask_8: 1.445  loss_dice_8: 4.086  loss_mars: 0.7844    time: 5.5913  last_time: 5.0014  data_time: 0.0028  last_data_time: 0.0070   lr: 1e-06  max_mem: 0M
[10/19 22:46:03] d2.utils.events INFO:  eta: 0:00:26  iter: 15619  total_loss: 85.24  loss_ce: 3.375  loss_mask: 0.8459  loss_dice: 4.001  loss_ce_0: 3.696  loss_mask_0: 0.8035  loss_dice_0: 3.972  loss_ce_1: 3.215  loss_mask_1: 0.8237  loss_dice_1: 4.105  loss_ce_2: 3.206  loss_mask_2: 0.7771  loss_dice_2: 4.035  loss_ce_3: 3.255  loss_mask_3: 0.8084  loss_dice_3: 4.207  loss_ce_4: 3.262  loss_mask_4: 0.8007  loss_dice_4: 4.114  loss_ce_5: 3.36  loss_mask_5: 0.8387  loss_dice_5: 4.101  loss_ce_6: 3.344  loss_mask_6: 0.8123  loss_dice_6: 4.116  loss_ce_7: 3.281  loss_mask_7: 0.8081  loss_dice_7: 3.946  loss_ce_8: 3.349  loss_mask_8: 0.8215  loss_dice_8: 3.98  loss_mars: 0.8381    time: 5.5911  last_time: 5.2640  data_time: 0.0039  last_data_time: 0.0024   lr: 1e-06  max_mem: 0M
[10/19 22:46:32] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_mars_fixed_5k_50ep/model_final.pth
[10/19 22:46:32] d2.utils.events INFO:  eta: 0:00:00  iter: 15624  total_loss: 88.88  loss_ce: 3.375  loss_mask: 1.017  loss_dice: 4.171  loss_ce_0: 3.696  loss_mask_0: 0.9285  loss_dice_0: 4.285  loss_ce_1: 3.215  loss_mask_1: 0.9034  loss_dice_1: 4.279  loss_ce_2: 3.183  loss_mask_2: 0.947  loss_dice_2: 4.301  loss_ce_3: 3.234  loss_mask_3: 0.9444  loss_dice_3: 4.348  loss_ce_4: 3.262  loss_mask_4: 0.9584  loss_dice_4: 4.271  loss_ce_5: 3.36  loss_mask_5: 0.9385  loss_dice_5: 4.244  loss_ce_6: 3.275  loss_mask_6: 0.965  loss_dice_6: 4.228  loss_ce_7: 3.228  loss_mask_7: 1.048  loss_dice_7: 4.207  loss_ce_8: 3.329  loss_mask_8: 1.045  loss_dice_8: 4.158  loss_mars: 0.856    time: 5.5912  last_time: 5.2274  data_time: 0.0040  last_data_time: 0.0017   lr: 1e-06  max_mem: 0M
[10/19 22:46:32] d2.engine.hooks INFO: Overall training speed: 14060 iterations in 21:50:12 (5.5912 s / it)
[10/19 22:46:32] d2.engine.hooks INFO: Total training time: 21:50:35 (0:00:23 on hooks)
[10/19 22:46:32] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 22:46:32] d2.data.common INFO: Serializing 1000 elements to byte tensors and concatenating them all ...
[10/19 22:46:32] d2.data.common INFO: Serialized dataset takes 3.76 MiB
[10/19 22:46:32] d2.engine.defaults WARNING: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[10/23 12:55:13] detectron2 INFO: Rank of current process: 0. World size: 1
[10/23 12:55:13] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0]
numpy                   1.22.4
detectron2              0.6 @/home/vaishali/miniconda3/envs/mask2former39/lib/python3.9/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.1+cu111 @/home/vaishali/miniconda3/envs/mask2former39/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA RTX A6000 (arch=8.6)
Driver version          535.230.02
CUDA_HOME               /usr/local/cuda-11.8
Pillow                  8.3.2
torchvision             0.10.1+cu111 @/home/vaishali/miniconda3/envs/mask2former39/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.5.3
----------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[10/23 12:55:13] detectron2 INFO: Command line arguments: Namespace(config_file='/home/vaishali/projects/Mask2Former/configs/coco/mars_5k_50ep_corrected.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[10/23 12:55:13] detectron2 INFO: Contents of args.config_file=/home/vaishali/projects/Mask2Former/configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/23 12:55:13] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/23 12:55:13] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/23 12:55:21] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 7.01 seconds.
[10/23 12:55:21] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/23 12:55:27] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/23 12:55:28] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/23 12:55:28] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/23 12:55:28] d2.data.build INFO: Using training sampler TrainingSampler
[10/23 12:55:28] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/23 12:55:28] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/23 12:57:49] detectron2 INFO: Rank of current process: 0. World size: 1
[10/23 12:57:50] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0]
numpy                   1.22.4
detectron2              0.6 @/home/vaishali/miniconda3/envs/mask2former39/lib/python3.9/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.1+cu111 @/home/vaishali/miniconda3/envs/mask2former39/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA RTX A6000 (arch=8.6)
Driver version          535.230.02
CUDA_HOME               /usr/local/cuda-11.8
Pillow                  8.3.2
torchvision             0.10.1+cu111 @/home/vaishali/miniconda3/envs/mask2former39/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.5.3
----------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[10/23 12:57:50] detectron2 INFO: Command line arguments: Namespace(config_file='/home/vaishali/projects/Mask2Former/configs/coco/mars_5k_50ep_corrected.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[10/23 12:57:50] detectron2 INFO: Contents of args.config_file=/home/vaishali/projects/Mask2Former/configs/coco/mars_5k_50ep_corrected.yaml:
# MARS + Mask2Former Training Configuration
# Dataset: 5000 training images, 1000 validation images
# Training: 50 epochs

_BASE_: "instance-segmentation/maskformer2_R50_bs16_50ep.yaml"

# ==================== DATASETS ====================
DATASETS:
  TRAIN: ("coco_2017_train_subset",)
  TEST: ("coco_2017_val_subset",)

# ==================== MODEL ====================
MODEL:
  META_ARCHITECTURE: "MaskFormerWithMARS"
  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  
  # MARS Configuration
  MARS:
    ENABLED: True
    WEIGHT: 1.0                       # Increased from 0.15
    LOSS_TYPE: "cosine"
  
  # Backbone
  BACKBONE:
    FREEZE_AT: 0

# ==================== SOLVER ====================
SOLVER:
  # Batch size and iterations
  IMS_PER_BATCH: 1                  # âœ… Fixed: Was 1, now 2
  GRADIENT_ACCUMULATION_STEPS: 4   # âœ… Fixed: Was 16, now 8
  # Effective batch = 2 * 8 = 16
  
  # Learning rate
  BASE_LR: 0.0001
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_NORM: 0.0
  
  # Schedule
  MAX_ITER: 15625                   # 50 epochs
  STEPS: (12500, 14500)
  GAMMA: 0.1
  WARMUP_ITERS: 250
  WARMUP_FACTOR: 0.001
  
  # Optimization
  OPTIMIZER: "ADAMW"
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "value"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
  
  # Mixed precision
  AMP:
    ENABLED: True
  
  # Checkpointing
  CHECKPOINT_PERIOD: 1563

# ==================== DATALOADER ====================
DATALOADER:
  NUM_WORKERS: 4
  FILTER_EMPTY_ANNOTATIONS: True

# ==================== INPUT ====================
INPUT:
  # âœ… Fixed: Use DEFAULT sizes from base config (don't override!)
  # MIN_SIZE_TRAIN and MAX_SIZE_TRAIN will use base config values
  # This matches the working baseline exactly
  
  # Augmentations
  RANDOM_FLIP: "horizontal"
  FORMAT: "RGB"

# ==================== TEST ====================
TEST:
  EVAL_PERIOD: 1563
  AUG:
    ENABLED: False

# ==================== OUTPUT ====================
OUTPUT_DIR: "./output_mars_fixed_5k_50ep"

# ==================== MISC ====================
SEED: 42
CUDNN_BENCHMARK: True

[10/23 12:57:50] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_subset
  TRAIN:
  - coco_2017_train_subset
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MARS:
    ENABLED: true
    LOSS_TYPE: cosine
    WEIGHT: 1.0
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormerWithMARS
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 80
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl
OUTPUT_DIR: ./output_mars_fixed_5k_50ep
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1563
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  GRADIENT_ACCUMULATION_STEPS: 4
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 15625
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 12500
  - 14500
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1563
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/23 12:57:50] detectron2 INFO: Full config saved to ./output_mars_fixed_5k_50ep/config.yaml
[10/23 12:57:57] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 6.94 seconds.
[10/23 12:57:58] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/23 12:58:03] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/23 12:58:05] d2.data.build INFO: Removed 49 images with no usable annotations. 4951 images left.
[10/23 12:58:05] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 11038        |   bicycle    | 350          |      car      | 1874         |
|  motorcycle   | 375          |   airplane   | 226          |      bus      | 266          |
|     train     | 207          |    truck     | 406          |     boat      | 433          |
| traffic light | 514          | fire hydrant | 71           |   stop sign   | 70           |
| parking meter | 44           |    bench     | 392          |     bird      | 412          |
|      cat      | 204          |     dog      | 268          |     horse     | 315          |
|     sheep     | 403          |     cow      | 221          |   elephant    | 223          |
|     bear      | 59           |    zebra     | 264          |    giraffe    | 257          |
|   backpack    | 383          |   umbrella   | 488          |    handbag    | 569          |
|      tie      | 274          |   suitcase   | 278          |    frisbee    | 127          |
|     skis      | 290          |  snowboard   | 91           |  sports ball  | 256          |
|     kite      | 375          | baseball bat | 115          | baseball gl.. | 163          |
|  skateboard   | 255          |  surfboard   | 259          | tennis racket | 220          |
|    bottle     | 983          |  wine glass  | 304          |      cup      | 829          |
|     fork      | 242          |    knife     | 328          |     spoon     | 255          |
|     bowl      | 620          |    banana    | 335          |     apple     | 241          |
|   sandwich    | 199          |    orange    | 272          |   broccoli    | 343          |
|    carrot     | 360          |   hot dog    | 150          |     pizza     | 239          |
|     donut     | 320          |     cake     | 270          |     chair     | 1480         |
|     couch     | 213          | potted plant | 333          |      bed      | 177          |
| dining table  | 619          |    toilet    | 182          |      tv       | 204          |
|    laptop     | 240          |    mouse     | 97           |    remote     | 235          |
|   keyboard    | 113          |  cell phone  | 270          |   microwave   | 54           |
|     oven      | 135          |   toaster    | 13           |     sink      | 221          |
| refrigerator  | 101          |     book     | 845          |     clock     | 330          |
|     vase      | 252          |   scissors   | 58           |  teddy bear   | 158          |
|  hair drier   | 7            |  toothbrush  | 103          |               |              |
|     total     | 35765        |              |              |               |              |[0m
[10/23 12:58:05] d2.data.build INFO: Using training sampler TrainingSampler
[10/23 12:58:05] d2.data.common INFO: Serializing 4951 elements to byte tensors and concatenating them all ...
[10/23 12:58:05] d2.data.common INFO: Serialized dataset takes 19.13 MiB
[10/23 12:58:05] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[10/23 12:58:05] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[10/23 12:58:05] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
[10/23 12:58:05] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[10/23 12:58:05] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[10/23 12:58:05] d2.engine.train_loop INFO: Starting training from iteration 0
[10/29 12:55:08] detectron2 INFO: Rank of current process: 0. World size: 1
